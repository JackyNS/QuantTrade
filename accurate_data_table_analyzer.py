#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å‡†ç¡®çš„æ•°æ®è¡¨æ ¼åˆ†æå™¨
åŸºäºå®é™…æ–‡ä»¶å†…å®¹å‡†ç¡®åˆ†æAPIæ•°æ®åˆ†å¸ƒ
"""

import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
import json
import warnings
from collections import defaultdict
warnings.filterwarnings('ignore')

class AccurateDataTableAnalyzer:
    """å‡†ç¡®çš„æ•°æ®è¡¨æ ¼åˆ†æå™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        self.base_path = Path("/Users/jackstudio/QuantTrade/data")
        self.results = []
    
    def analyze_batch_data(self, data_type, path_suffix):
        """åˆ†ææ‰¹æ¬¡æ•°æ®"""
        data_path = self.base_path / f"priority_download/market_data/{path_suffix}"
        
        if not data_path.exists():
            return {
                'data_type': data_type,
                'status': 'âŒ ä¸å­˜åœ¨',
                'file_count': 0,
                'time_range': 'æ— æ•°æ®',
                'stock_sample': 0,
                'completeness': 'æ— æ•°æ®',
                'path': str(data_path)
            }
        
        batch_files = list(data_path.glob("*.csv"))
        
        if not batch_files:
            return {
                'data_type': data_type,
                'status': 'âŒ æ— æ–‡ä»¶',
                'file_count': 0,
                'time_range': 'æ— æ•°æ®',
                'stock_sample': 0,
                'completeness': 'æ— æ•°æ®',
                'path': str(data_path)
            }
        
        # åˆ†æç¬¬ä¸€ä¸ªå’Œæœ€åä¸€ä¸ªæ–‡ä»¶è·å–æ—¶é—´èŒƒå›´
        first_file = min(batch_files, key=lambda x: x.name)
        last_file = max(batch_files, key=lambda x: x.name)
        
        time_range = "æ— æ³•ç¡®å®š"
        stock_sample = 0
        
        try:
            # åˆ†æç¬¬ä¸€ä¸ªæ–‡ä»¶
            df_first = pd.read_csv(first_file)
            if 'tradeDate' in df_first.columns:
                df_first['tradeDate'] = pd.to_datetime(df_first['tradeDate'])
                start_date = df_first['tradeDate'].min()
                stock_sample = len(df_first['secID'].unique()) if 'secID' in df_first.columns else 0
            else:
                start_date = None
                
            # åˆ†ææœ€åä¸€ä¸ªæ–‡ä»¶
            df_last = pd.read_csv(last_file)
            if 'tradeDate' in df_last.columns:
                df_last['tradeDate'] = pd.to_datetime(df_last['tradeDate'])
                end_date = df_last['tradeDate'].max()
            else:
                end_date = None
            
            if start_date and end_date:
                time_range = f"{start_date.strftime('%Y-%m-%d')} - {end_date.strftime('%Y-%m-%d')}"
                
                # åˆ¤æ–­å®Œæ•´æ€§
                target_start = pd.to_datetime('2000-01-01')
                target_end = pd.to_datetime('2025-08-31')
                
                if start_date <= target_start + pd.Timedelta(days=10) and end_date >= target_end - pd.Timedelta(days=10):
                    completeness = "âœ… å®Œæ•´"
                elif start_date <= target_start + pd.Timedelta(days=365) and end_date >= target_end - pd.Timedelta(days=90):
                    completeness = "ğŸŸ¡ åŸºæœ¬å®Œæ•´"
                else:
                    completeness = "ğŸ”´ éƒ¨åˆ†ç¼ºå¤±"
            else:
                completeness = "â“ æ— æ³•ç¡®å®š"
                
        except Exception as e:
            time_range = f"è¯»å–é”™è¯¯: {str(e)}"
            completeness = "âŒ æ•°æ®æŸå"
        
        return {
            'data_type': data_type,
            'status': 'âœ… å­˜åœ¨',
            'file_count': len(batch_files),
            'time_range': time_range,
            'stock_sample': stock_sample,
            'completeness': completeness,
            'path': str(data_path)
        }
    
    def analyze_csv_complete_data(self, data_type, path_suffix):
        """åˆ†æcsv_completeæ•°æ®"""
        data_path = self.base_path / f"csv_complete/{path_suffix}"
        
        if not data_path.exists():
            return {
                'data_type': f"{data_type}(ä¸ªè‚¡æ–‡ä»¶)",
                'status': 'âŒ ä¸å­˜åœ¨',
                'file_count': 0,
                'time_range': 'æ— æ•°æ®',
                'stock_sample': 0,
                'completeness': 'æ— æ•°æ®',
                'path': str(data_path)
            }
        
        csv_files = list(data_path.rglob("*.csv"))
        
        if not csv_files:
            return {
                'data_type': f"{data_type}(ä¸ªè‚¡æ–‡ä»¶)",
                'status': 'âŒ æ— æ–‡ä»¶',
                'file_count': 0,
                'time_range': 'æ— æ•°æ®',
                'stock_sample': 0,
                'completeness': 'æ— æ•°æ®',
                'path': str(data_path)
            }
        
        # éšæœºæŠ½æ ·åˆ†æå‡ ä¸ªæ–‡ä»¶è·å–æ—¶é—´èŒƒå›´
        sample_files = csv_files[:3] if len(csv_files) >= 3 else csv_files
        
        all_start_dates = []
        all_end_dates = []
        
        for sample_file in sample_files:
            try:
                df = pd.read_csv(sample_file)
                if 'tradeDate' in df.columns:
                    df['tradeDate'] = pd.to_datetime(df['tradeDate'])
                    all_start_dates.append(df['tradeDate'].min())
                    all_end_dates.append(df['tradeDate'].max())
            except:
                continue
        
        if all_start_dates and all_end_dates:
            overall_start = min(all_start_dates)
            overall_end = max(all_end_dates)
            time_range = f"{overall_start.strftime('%Y-%m-%d')} - {overall_end.strftime('%Y-%m-%d')}"
            
            # åˆ¤æ–­å®Œæ•´æ€§
            target_start = pd.to_datetime('2000-01-01')
            target_end = pd.to_datetime('2025-08-31')
            
            if overall_start <= target_start + pd.Timedelta(days=10) and overall_end >= target_end - pd.Timedelta(days=10):
                completeness = "âœ… å®Œæ•´"
            elif overall_start <= target_start + pd.Timedelta(days=365) and overall_end >= target_end - pd.Timedelta(days=90):
                completeness = "ğŸŸ¡ åŸºæœ¬å®Œæ•´"
            else:
                completeness = "ğŸ”´ éƒ¨åˆ†ç¼ºå¤±"
        else:
            time_range = "æ— æ³•ç¡®å®š"
            completeness = "â“ æ— æ³•ç¡®å®š"
        
        return {
            'data_type': f"{data_type}(ä¸ªè‚¡æ–‡ä»¶)",
            'status': 'âœ… å­˜åœ¨',
            'file_count': len(csv_files),
            'time_range': time_range,
            'stock_sample': len(csv_files),  # ä¸ªè‚¡æ–‡ä»¶æ•°é‡å°±æ˜¯è‚¡ç¥¨æ•°é‡
            'completeness': completeness,
            'path': str(data_path)
        }
    
    def analyze_special_data(self):
        """åˆ†æç‰¹æ®Šæ•°æ®ç±»å‹"""
        special_results = []
        
        # æ£€æŸ¥final_comprehensive_downloadç›®å½•
        final_path = self.base_path / "final_comprehensive_download"
        
        if final_path.exists():
            subdirs = [d for d in final_path.iterdir() if d.is_dir()]
            
            for subdir in subdirs:
                files = list(subdir.rglob("*.csv"))
                
                if files:
                    # å°è¯•åˆ†ææ—¶é—´èŒƒå›´
                    sample_file = files[0]
                    time_range = "æ•°æ®å­˜åœ¨"
                    
                    try:
                        df = pd.read_csv(sample_file)
                        date_cols = [col for col in df.columns if any(keyword in col.lower() for keyword in ['date', 'time', 'æ—¥æœŸ'])]
                        
                        if date_cols:
                            df[date_cols[0]] = pd.to_datetime(df[date_cols[0]])
                            start_date = df[date_cols[0]].min()
                            end_date = df[date_cols[0]].max()
                            time_range = f"{start_date.strftime('%Y-%m-%d')} - {end_date.strftime('%Y-%m-%d')}"
                    except:
                        pass
                    
                    special_results.append({
                        'data_type': f"{subdir.name}æ•°æ®",
                        'status': 'âœ… å­˜åœ¨',
                        'file_count': len(files),
                        'time_range': time_range,
                        'stock_sample': 'ä¸ç¡®å®š',
                        'completeness': 'ğŸŸ¡ éœ€è¦å…·ä½“åˆ†æ',
                        'path': str(subdir)
                    })
        
        return special_results
    
    def generate_complete_table(self):
        """ç”Ÿæˆå®Œæ•´çš„æ•°æ®è¡¨æ ¼"""
        print("ğŸ“Š ç”Ÿæˆå‡†ç¡®çš„æ•°æ®åˆ†æè¡¨æ ¼...")
        print("ğŸ¯ ç›®æ ‡: 2000-01-01 è‡³ 2025-08-31")
        print("=" * 100)
        
        # åˆ†ææ ¸å¿ƒè¡Œæƒ…æ•°æ®
        core_data = [
            ('æ—¥çº¿è¡Œæƒ…æ•°æ®', 'daily'),
            ('å‘¨çº¿è¡Œæƒ…æ•°æ®', 'weekly'), 
            ('æœˆçº¿è¡Œæƒ…æ•°æ®', 'monthly'),
            ('å‰å¤æƒæ—¥çº¿', 'adj_daily'),
            ('å‰å¤æƒå‘¨çº¿', 'adj_weekly'),
            ('å‰å¤æƒæœˆçº¿', 'adj_monthly')
        ]
        
        print("\\nğŸ“ˆ æ ¸å¿ƒè¡Œæƒ…æ•°æ®åˆ†æ:")
        print("-" * 100)
        print(f"{'æ•°æ®ç±»å‹':<20} {'çŠ¶æ€':<10} {'æ–‡ä»¶æ•°':<8} {'æ—¶é—´èŒƒå›´':<30} {'è‚¡ç¥¨æ ·æœ¬':<10} {'å®Œæ•´æ€§':<15}")
        print("-" * 100)
        
        for data_name, path_suffix in core_data:
            result = self.analyze_batch_data(data_name, path_suffix)
            self.results.append(result)
            
            print(f"{result['data_type']:<20} {result['status']:<10} {result['file_count']:<8} {result['time_range']:<30} {result['stock_sample']:<10} {result['completeness']:<15}")
        
        # åˆ†æä¸ªè‚¡æ–‡ä»¶æ•°æ®
        print("\\nğŸ“Š ä¸ªè‚¡æ–‡ä»¶æ•°æ®åˆ†æ:")
        print("-" * 100)
        print(f"{'æ•°æ®ç±»å‹':<20} {'çŠ¶æ€':<10} {'æ–‡ä»¶æ•°':<8} {'æ—¶é—´èŒƒå›´':<30} {'è‚¡ç¥¨æ•°é‡':<10} {'å®Œæ•´æ€§':<15}")
        print("-" * 100)
        
        for data_name, path_suffix in [('æ—¥çº¿', 'daily'), ('å‘¨çº¿', 'weekly'), ('æœˆçº¿', 'monthly')]:
            result = self.analyze_csv_complete_data(data_name, path_suffix)
            self.results.append(result)
            
            print(f"{result['data_type']:<20} {result['status']:<10} {result['file_count']:<8} {result['time_range']:<30} {result['stock_sample']:<10} {result['completeness']:<15}")
        
        # åˆ†æå…¶ä»–æ•°æ®
        special_results = self.analyze_special_data()
        
        if special_results:
            print("\\nğŸ” å…¶ä»–æ•°æ®ç±»å‹:")
            print("-" * 100)
            print(f"{'æ•°æ®ç±»å‹':<20} {'çŠ¶æ€':<10} {'æ–‡ä»¶æ•°':<8} {'æ—¶é—´èŒƒå›´':<30} {'è¯´æ˜':<10} {'å®Œæ•´æ€§':<15}")
            print("-" * 100)
            
            for result in special_results:
                self.results.append(result)
                print(f"{result['data_type']:<20} {result['status']:<10} {result['file_count']:<8} {result['time_range']:<30} {result['stock_sample']:<10} {result['completeness']:<15}")
        
        self.generate_summary()
    
    def generate_summary(self):
        """ç”Ÿæˆæ€»ç»“"""
        print("\\nğŸŠ æ•°æ®åˆ†ææ€»ç»“:")
        print("=" * 80)
        
        # ç»Ÿè®¡å„çŠ¶æ€æ•°é‡
        existing_count = sum(1 for r in self.results if r['status'] == 'âœ… å­˜åœ¨')
        missing_count = sum(1 for r in self.results if r['status'].startswith('âŒ'))
        complete_count = sum(1 for r in self.results if r['completeness'] == 'âœ… å®Œæ•´')
        
        print(f"ğŸ“Š æ•°æ®ç±»å‹æ€»æ•°: {len(self.results)}")
        print(f"âœ… å­˜åœ¨æ•°æ®: {existing_count}")
        print(f"âŒ ç¼ºå¤±æ•°æ®: {missing_count}")
        print(f"ğŸ† å®Œæ•´æ•°æ®: {complete_count}")
        
        # é‡ç‚¹å…³æ³¨çš„æ ¸å¿ƒæ•°æ®
        core_types = ['æ—¥çº¿è¡Œæƒ…æ•°æ®', 'å‘¨çº¿è¡Œæƒ…æ•°æ®', 'æœˆçº¿è¡Œæƒ…æ•°æ®']
        print(f"\\nğŸ¯ æ ¸å¿ƒè¡Œæƒ…æ•°æ®çŠ¶æ€:")
        for result in self.results:
            if result['data_type'] in core_types:
                print(f"   {result['data_type']}: {result['completeness']} ({result['time_range']})")
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        report_file = self.base_path / 'accurate_data_analysis_table.json'
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump({
                'analysis_time': datetime.now().isoformat(),
                'target_range': '2000-01-01 to 2025-08-31',
                'summary': {
                    'total_types': len(self.results),
                    'existing_data': existing_count,
                    'missing_data': missing_count,
                    'complete_data': complete_count
                },
                'detailed_results': self.results
            }, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"\\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_file}")

def main():
    """ä¸»å‡½æ•°"""
    analyzer = AccurateDataTableAnalyzer()
    analyzer.generate_complete_table()

if __name__ == "__main__":
    main()