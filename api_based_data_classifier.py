#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŸºäºAPIåˆ†ç±»çš„æ•°æ®åˆ†æå™¨
æŒ‰ä¼˜çŸ¿APIç±»å‹åˆ†ç±»ï¼Œè¡¨æ ¼å½¢å¼æ˜¾ç¤ºæ•°æ®æ˜ç»†ã€æ—¶é—´èŒƒå›´å’Œç¼ºå¤±æƒ…å†µ
"""

import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
import json
import warnings
from collections import defaultdict
import os
warnings.filterwarnings('ignore')

class APIBasedDataClassifier:
    """åŸºäºAPIåˆ†ç±»çš„æ•°æ®åˆ†æå™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        self.base_path = Path("/Users/jackstudio/QuantTrade/data")
        self.target_start = "2000-01-01"
        self.target_end = "2025-08-31"
        
        # ä¼˜çŸ¿APIæ˜ å°„è¡¨
        self.api_mapping = {
            # è‚¡ç¥¨è¡Œæƒ…æ•°æ®
            'MktEqudGet': {'name': 'æ—¥çº¿è¡Œæƒ…', 'category': 'è‚¡ç¥¨è¡Œæƒ…', 'expected_path': 'priority_download/market_data/daily'},
            'MktEquwGet': {'name': 'å‘¨çº¿è¡Œæƒ…', 'category': 'è‚¡ç¥¨è¡Œæƒ…', 'expected_path': 'priority_download/market_data/weekly'},  
            'MktEqumGet': {'name': 'æœˆçº¿è¡Œæƒ…', 'category': 'è‚¡ç¥¨è¡Œæƒ…', 'expected_path': 'priority_download/market_data/monthly'},
            'MktEqudAdjGet': {'name': 'æ—¥çº¿è¡Œæƒ…(å‰å¤æƒ)', 'category': 'è‚¡ç¥¨è¡Œæƒ…', 'expected_path': 'priority_download/market_data/adj_daily'},
            'MktEquwAdjGet': {'name': 'å‘¨çº¿è¡Œæƒ…(å‰å¤æƒ)', 'category': 'è‚¡ç¥¨è¡Œæƒ…', 'expected_path': 'priority_download/market_data/adj_weekly'},
            'MktEqumAdjGet': {'name': 'æœˆçº¿è¡Œæƒ…(å‰å¤æƒ)', 'category': 'è‚¡ç¥¨è¡Œæƒ…', 'expected_path': 'priority_download/market_data/adj_monthly'},
            
            # è´¢åŠ¡æ•°æ®
            'FdmtIncomestatementsGet': {'name': 'åˆ©æ¶¦è¡¨', 'category': 'è´¢åŠ¡æ•°æ®', 'expected_path': 'final_comprehensive_download/financial'},
            'FdmtBalancesheetsGet': {'name': 'èµ„äº§è´Ÿå€ºè¡¨', 'category': 'è´¢åŠ¡æ•°æ®', 'expected_path': 'final_comprehensive_download/financial'},
            'FdmtCashflowstatementsGet': {'name': 'ç°é‡‘æµé‡è¡¨', 'category': 'è´¢åŠ¡æ•°æ®', 'expected_path': 'final_comprehensive_download/financial'},
            'FdmtEfinancialindicatorGet': {'name': 'è´¢åŠ¡æŒ‡æ ‡', 'category': 'è´¢åŠ¡æ•°æ®', 'expected_path': 'final_comprehensive_download/financial'},
            
            # åŸºç¡€ä¿¡æ¯
            'EquGet': {'name': 'è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯', 'category': 'åŸºç¡€ä¿¡æ¯', 'expected_path': 'final_comprehensive_download/basic_info'},
            'SecIDGet': {'name': 'è¯åˆ¸ä»£ç è¡¨', 'category': 'åŸºç¡€ä¿¡æ¯', 'expected_path': 'final_comprehensive_download/basic_info'},
            
            # å…¬å¸æ²»ç†
            'EquDivGet': {'name': 'åˆ†çº¢é…è‚¡', 'category': 'å…¬å¸æ²»ç†', 'expected_path': 'final_comprehensive_download/governance'},
            'EquRetGet': {'name': 'å›æŠ¥ç‡', 'category': 'å…¬å¸æ²»ç†', 'expected_path': 'final_comprehensive_download/governance'},
            'EquIndustryGet': {'name': 'è¡Œä¸šåˆ†ç±»', 'category': 'å…¬å¸æ²»ç†', 'expected_path': 'final_comprehensive_download/governance'},
            
            # ç‰¹æ®Šäº¤æ˜“
            'MktLimitGet': {'name': 'æ¶¨è·Œåœç»Ÿè®¡', 'category': 'ç‰¹æ®Šäº¤æ˜“', 'expected_path': 'final_comprehensive_download/special_trading'},
            'MktEqudStatsGet': {'name': 'è‚¡ç¥¨ç»Ÿè®¡', 'category': 'ç‰¹æ®Šäº¤æ˜“', 'expected_path': 'final_comprehensive_download/special_trading'},
            'ResconSeccodesGet': {'name': 'é™å”®è‚¡ä»½', 'category': 'ç‰¹æ®Šäº¤æ˜“', 'expected_path': 'final_comprehensive_download/special_trading'},
        }
        
        self.data_inventory = {}
    
    def scan_directory_for_api_data(self, dir_path, api_name):
        """æ‰«æç›®å½•å¯»æ‰¾ç‰¹å®šAPIçš„æ•°æ®"""
        full_path = self.base_path / dir_path
        
        if not full_path.exists():
            return {
                'exists': False,
                'file_count': 0,
                'time_range': None,
                'sample_files': [],
                'status': 'ç›®å½•ä¸å­˜åœ¨'
            }
        
        # æŸ¥æ‰¾å¯èƒ½çš„æ•°æ®æ–‡ä»¶
        patterns_to_check = [
            f"*{api_name.lower()}*",
            f"*{api_name}*", 
            "*.csv",
            "*.parquet"
        ]
        
        found_files = []
        for pattern in patterns_to_check:
            found_files.extend(list(full_path.rglob(pattern)))
        
        if not found_files:
            # æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–CSVæ–‡ä»¶
            csv_files = list(full_path.rglob("*.csv"))
            return {
                'exists': len(csv_files) > 0,
                'file_count': len(csv_files),
                'time_range': self._analyze_time_range(csv_files[:3]) if csv_files else None,
                'sample_files': [f.name for f in csv_files[:3]],
                'status': 'å¯èƒ½åŒ…å«ç›¸å…³æ•°æ®' if csv_files else 'æ— ç›¸å…³æ–‡ä»¶'
            }
        
        # åˆ†ææ‰¾åˆ°çš„æ–‡ä»¶
        time_range = self._analyze_time_range(found_files[:5])
        
        return {
            'exists': True,
            'file_count': len(found_files),
            'time_range': time_range,
            'sample_files': [f.name for f in found_files[:3]],
            'status': 'æ•°æ®å®Œæ•´' if time_range and self._check_completeness(time_range) else 'æ•°æ®éƒ¨åˆ†ç¼ºå¤±'
        }
    
    def _analyze_time_range(self, file_list):
        """åˆ†ææ–‡ä»¶çš„æ—¶é—´èŒƒå›´"""
        if not file_list:
            return None
            
        all_dates = []
        
        for file_path in file_list:
            try:
                df = pd.read_csv(file_path)
                
                # å¯»æ‰¾å¯èƒ½çš„æ—¥æœŸåˆ—
                date_columns = []
                for col in df.columns:
                    if any(keyword in col.lower() for keyword in ['date', 'time', 'æ—¥æœŸ', 'æ—¶é—´']):
                        date_columns.append(col)
                
                # åˆ†ææ—¥æœŸèŒƒå›´
                for date_col in date_columns:
                    try:
                        dates = pd.to_datetime(df[date_col])
                        all_dates.extend([dates.min(), dates.max()])
                        break  # æ‰¾åˆ°ä¸€ä¸ªæœ‰æ•ˆçš„æ—¥æœŸåˆ—å°±å¤Ÿäº†
                    except:
                        continue
                        
            except Exception as e:
                continue
        
        if all_dates:
            return {
                'start': min(all_dates).strftime('%Y-%m-%d'),
                'end': max(all_dates).strftime('%Y-%m-%d'),
                'span_years': (max(all_dates) - min(all_dates)).days // 365
            }
        
        return None
    
    def _check_completeness(self, time_range):
        """æ£€æŸ¥æ—¶é—´å®Œæ•´æ€§"""
        if not time_range:
            return False
        
        start_date = pd.to_datetime(time_range['start'])
        end_date = pd.to_datetime(time_range['end'])
        target_start = pd.to_datetime(self.target_start)
        target_end = pd.to_datetime(self.target_end)
        
        # å…è®¸ä¸€äº›å®¹å·®
        start_ok = start_date <= target_start + pd.Timedelta(days=365)
        end_ok = end_date >= target_end - pd.Timedelta(days=90)
        
        return start_ok and end_ok
    
    def classify_all_data(self):
        """æŒ‰APIåˆ†ç±»æ‰€æœ‰æ•°æ®"""
        print("ğŸ” æŒ‰APIç±»å‹åˆ†ç±»æ•°æ®...")
        print("ğŸ¯ ç›®æ ‡æ—¶é—´èŒƒå›´: 2000-01-01 è‡³ 2025-08-31")
        print("=" * 100)
        
        results = []
        
        for api_name, api_info in self.api_mapping.items():
            print(f"ğŸ“Š æ£€æŸ¥ {api_info['name']} ({api_name})...")
            
            # æ‰«æé¢„æœŸè·¯å¾„
            data_info = self.scan_directory_for_api_data(api_info['expected_path'], api_name)
            
            # å¦‚æœé¢„æœŸè·¯å¾„æ²¡æ‰¾åˆ°ï¼Œå°è¯•å…¶ä»–å¯èƒ½è·¯å¾„
            if not data_info['exists']:
                alternative_paths = [
                    'optimized_data',
                    'raw',
                    'csv_complete',
                    'smart_download',
                    'historical_download'
                ]
                
                for alt_path in alternative_paths:
                    alt_info = self.scan_directory_for_api_data(alt_path, api_name)
                    if alt_info['exists']:
                        data_info = alt_info
                        data_info['actual_path'] = alt_path
                        break
            
            # åˆ†æç¼ºå¤±æƒ…å†µ
            missing_info = self._analyze_missing_data(data_info)
            
            result = {
                'APIåç§°': api_name,
                'æ•°æ®ç±»å‹': api_info['name'],
                'æ•°æ®åˆ†ç±»': api_info['category'],
                'é¢„æœŸè·¯å¾„': api_info['expected_path'],
                'å®é™…è·¯å¾„': data_info.get('actual_path', api_info['expected_path']),
                'æ˜¯å¦å­˜åœ¨': 'âœ…' if data_info['exists'] else 'âŒ',
                'æ–‡ä»¶æ•°é‡': data_info['file_count'],
                'æ—¶é—´èŒƒå›´': f"{data_info['time_range']['start']} - {data_info['time_range']['end']}" if data_info['time_range'] else 'æ— æ•°æ®',
                'æ—¶é—´è·¨åº¦': f"{data_info['time_range']['span_years']}å¹´" if data_info['time_range'] else '0å¹´',
                'å®Œæ•´æ€§': data_info['status'],
                'ç¼ºå¤±åˆ†æ': missing_info,
                'æ ·æœ¬æ–‡ä»¶': '; '.join(data_info['sample_files'][:2]) if data_info['sample_files'] else 'æ— '
            }
            
            results.append(result)
            print(f"   ç»“æœ: {result['æ˜¯å¦å­˜åœ¨']} {result['å®Œæ•´æ€§']}")
        
        self.data_inventory = results
        return results
    
    def _analyze_missing_data(self, data_info):
        """åˆ†æç¼ºå¤±æ•°æ®"""
        if not data_info['exists']:
            return "å®Œå…¨ç¼ºå¤±"
        
        if not data_info['time_range']:
            return "æ— æ³•ç¡®å®šæ—¶é—´èŒƒå›´"
        
        start_date = pd.to_datetime(data_info['time_range']['start'])
        end_date = pd.to_datetime(data_info['time_range']['end'])
        target_start = pd.to_datetime(self.target_start)
        target_end = pd.to_datetime(self.target_end)
        
        missing_parts = []
        
        if start_date > target_start + pd.Timedelta(days=365):
            missing_parts.append(f"ç¼ºå¤±æ—©æœŸæ•°æ®({self.target_start} - {start_date.strftime('%Y-%m-%d')})")
        
        if end_date < target_end - pd.Timedelta(days=90):
            missing_parts.append(f"ç¼ºå¤±è¿‘æœŸæ•°æ®({end_date.strftime('%Y-%m-%d')} - {self.target_end})")
        
        return '; '.join(missing_parts) if missing_parts else "æ—¶é—´èŒƒå›´å®Œæ•´"
    
    def generate_summary_table(self):
        """ç”Ÿæˆæ±‡æ€»è¡¨æ ¼"""
        if not self.data_inventory:
            return
        
        df = pd.DataFrame(self.data_inventory)
        
        print("\\nğŸ“‹ æ•°æ®æ±‡æ€»è¡¨æ ¼:")
        print("=" * 120)
        
        # æŒ‰åˆ†ç±»æ•´ç†
        categories = df['æ•°æ®åˆ†ç±»'].unique()
        
        for category in categories:
            print(f"\\nğŸ“Š {category}:")
            print("-" * 80)
            
            cat_df = df[df['æ•°æ®åˆ†ç±»'] == category]
            
            # åˆ›å»ºç®€åŒ–è¡¨æ ¼
            display_columns = ['æ•°æ®ç±»å‹', 'æ˜¯å¦å­˜åœ¨', 'æ–‡ä»¶æ•°é‡', 'æ—¶é—´èŒƒå›´', 'å®Œæ•´æ€§', 'ç¼ºå¤±åˆ†æ']
            display_df = cat_df[display_columns].copy()
            
            # æ ¼å¼åŒ–æ˜¾ç¤º
            for _, row in display_df.iterrows():
                print(f"  {row['æ•°æ®ç±»å‹']:<15} {row['æ˜¯å¦å­˜åœ¨']:<4} {row['æ–‡ä»¶æ•°é‡']:<8} {row['æ—¶é—´èŒƒå›´']:<25} {row['å®Œæ•´æ€§']:<15} {row['ç¼ºå¤±åˆ†æ']}")
        
        return df
    
    def generate_detailed_report(self):
        """ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š"""
        results = self.classify_all_data()
        summary_df = self.generate_summary_table()
        
        # ç»Ÿè®¡æ±‡æ€»
        total_apis = len(results)
        existing_apis = sum(1 for r in results if r['æ˜¯å¦å­˜åœ¨'] == 'âœ…')
        missing_apis = total_apis - existing_apis
        
        # æŒ‰åˆ†ç±»ç»Ÿè®¡
        category_stats = {}
        for result in results:
            category = result['æ•°æ®åˆ†ç±»']
            if category not in category_stats:
                category_stats[category] = {'total': 0, 'existing': 0}
            category_stats[category]['total'] += 1
            if result['æ˜¯å¦å­˜åœ¨'] == 'âœ…':
                category_stats[category]['existing'] += 1
        
        print("\\nğŸŠ æ•°æ®åˆ†ææ±‡æ€»:")
        print("=" * 60)
        print(f"ğŸ“Š APIæ€»æ•°: {total_apis}")
        print(f"âœ… æœ‰æ•°æ®: {existing_apis} ({existing_apis/total_apis*100:.1f}%)")
        print(f"âŒ ç¼ºå¤±: {missing_apis} ({missing_apis/total_apis*100:.1f}%)")
        
        print(f"\\nğŸ“‹ åˆ†ç±»ç»Ÿè®¡:")
        for category, stats in category_stats.items():
            coverage = stats['existing'] / stats['total'] * 100
            print(f"  {category}: {stats['existing']}/{stats['total']} ({coverage:.1f}%)")
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        report = {
            'analysis_time': datetime.now().isoformat(),
            'target_time_range': f"{self.target_start} - {self.target_end}",
            'summary_stats': {
                'total_apis': total_apis,
                'existing_apis': existing_apis,
                'missing_apis': missing_apis,
                'coverage_rate': f"{existing_apis/total_apis*100:.1f}%"
            },
            'category_stats': category_stats,
            'detailed_inventory': results
        }
        
        report_file = self.base_path / 'api_based_data_classification_report.json'
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2, default=str)
        
        # ä¿å­˜CSVæ ¼å¼
        if summary_df is not None:
            csv_file = self.base_path / 'api_data_inventory.csv'
            summary_df.to_csv(csv_file, index=False, encoding='utf-8-sig')
            print(f"\\nğŸ“„ è¯¦ç»†æŠ¥å‘Š: {report_file}")
            print(f"ğŸ“Š CSVè¡¨æ ¼: {csv_file}")

def main():
    """ä¸»å‡½æ•°"""
    classifier = APIBasedDataClassifier()
    classifier.generate_detailed_report()

if __name__ == "__main__":
    main()