#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®éªŒè¯å™¨ - ç°ä»£åŒ–çš„æ•°æ®æ¨¡å—éªŒè¯å·¥å…·
=====================================

åŠŸèƒ½:
1. æ•°æ®æ¨¡å—å®Œæ•´æ€§éªŒè¯
2. æ•°æ®è´¨é‡æ£€æŸ¥
3. æ•°æ®æºè¿æ¥éªŒè¯
4. æ•°æ®æ ¼å¼éªŒè¯
5. æ•°æ®ç®¡é“æµ‹è¯•

ä½¿ç”¨æ–¹æ³•:
python tools/testing/data_validator.py [æ¨¡å¼]

æ¨¡å¼é€‰é¡¹:
- integrity: æ•°æ®å®Œæ•´æ€§éªŒè¯
- quality: æ•°æ®è´¨é‡æ£€æŸ¥
- sources: æ•°æ®æºéªŒè¯
- pipeline: æ•°æ®ç®¡é“æµ‹è¯•
- all: è¿è¡Œæ‰€æœ‰éªŒè¯ (é»˜è®¤)
"""

import os
import sys
import json
import argparse
from pathlib import Path
from datetime import datetime
import pandas as pd
import numpy as np

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

class DataValidator:
    """æ•°æ®éªŒè¯å™¨"""
    
    def __init__(self):
        self.validation_results = {
            'timestamp': datetime.now().isoformat(),
            'validation_status': 'unknown',
            'tests_passed': 0,
            'tests_failed': 0,
            'test_results': {},
            'data_metrics': {}
        }
    
    def validate_data_modules(self):
        """éªŒè¯æ•°æ®æ¨¡å—"""
        print("ğŸ“Š éªŒè¯æ•°æ®æ¨¡å—...")
        
        test_result = {
            'status': 'unknown',
            'modules_tested': 0,
            'modules_loaded': 0,
            'errors': []
        }
        
        data_modules = [
            ('DataManager', 'core.data.data_manager', 'æ•°æ®ç®¡ç†å™¨'),
            ('EnhancedDataManager', 'core.data.enhanced_data_manager', 'å¢å¼ºæ•°æ®ç®¡ç†å™¨'),
            ('UqerAdapter', 'core.data.adapters.uqer_adapter', 'ä¼˜çŸ¿é€‚é…å™¨'),
            ('DataProcessor', 'core.data.processors.data_processor', 'æ•°æ®å¤„ç†å™¨')
        ]
        
        for class_name, module_path, description in data_modules:
            test_result['modules_tested'] += 1
            
            try:
                module = __import__(module_path, fromlist=[class_name])
                data_class = getattr(module, class_name)
                
                # å°è¯•å®ä¾‹åŒ–
                if class_name in ['DataManager', 'EnhancedDataManager']:
                    instance = data_class()
                elif class_name == 'UqerAdapter':
                    # UqerAdapterå¯èƒ½éœ€è¦token
                    try:
                        instance = data_class()
                    except:
                        instance = data_class(token="test_token")
                else:
                    instance = data_class()
                
                test_result['modules_loaded'] += 1
                print(f"  âœ… {class_name}: {description} - åŠ è½½æˆåŠŸ")
                
            except ImportError:
                test_result['errors'].append(f"{class_name}: æ¨¡å—æœªæ‰¾åˆ°")
                print(f"  âš ï¸ {class_name}: æ¨¡å—æœªå®ç°")
            except Exception as e:
                test_result['errors'].append(f"{class_name}: {str(e)}")
                print(f"  âŒ {class_name}: åˆå§‹åŒ–å¤±è´¥ - {e}")
        
        test_result['status'] = 'success' if test_result['modules_loaded'] > 0 else 'failed'
        
        self.validation_results['test_results']['data_modules'] = test_result
        
        if test_result['status'] == 'success':
            self.validation_results['tests_passed'] += 1
        else:
            self.validation_results['tests_failed'] += 1
        
        return test_result['status'] == 'success'
    
    def validate_data_directories(self):
        """éªŒè¯æ•°æ®ç›®å½•ç»“æ„"""
        print("ğŸ“ éªŒè¯æ•°æ®ç›®å½•ç»“æ„...")
        
        test_result = {
            'status': 'unknown',
            'directories_checked': 0,
            'directories_found': 0,
            'directory_details': {},
            'total_size_mb': 0
        }
        
        expected_directories = [
            ('data', 'ä¸»æ•°æ®ç›®å½•'),
            ('data/optimized', 'ä¼˜åŒ–æ•°æ®'),
            ('data/raw', 'åŸå§‹æ•°æ®'),
            ('data/processed', 'å¤„ç†åæ•°æ®'),
            ('data/cache', 'ç¼“å­˜æ•°æ®'),
            ('outputs', 'è¾“å‡ºç›®å½•'),
            ('logs', 'æ—¥å¿—ç›®å½•')
        ]
        
        for dir_path, description in expected_directories:
            test_result['directories_checked'] += 1
            full_path = project_root / dir_path
            
            dir_info = {
                'exists': full_path.exists(),
                'description': description,
                'size_mb': 0,
                'file_count': 0
            }
            
            if full_path.exists():
                test_result['directories_found'] += 1
                
                # è®¡ç®—ç›®å½•å¤§å°å’Œæ–‡ä»¶æ•°é‡
                total_size = 0
                file_count = 0
                
                try:
                    for file_path in full_path.rglob("*"):
                        if file_path.is_file():
                            file_count += 1
                            total_size += file_path.stat().st_size
                    
                    dir_info['size_mb'] = total_size / (1024 * 1024)
                    dir_info['file_count'] = file_count
                    test_result['total_size_mb'] += dir_info['size_mb']
                    
                    print(f"  âœ… {dir_path}: {file_count}ä¸ªæ–‡ä»¶, {dir_info['size_mb']:.1f}MB")
                    
                except Exception as e:
                    print(f"  âš ï¸ {dir_path}: æ— æ³•ç»Ÿè®¡ - {e}")
            else:
                print(f"  âš ï¸ {dir_path}: ç›®å½•ä¸å­˜åœ¨")
            
            test_result['directory_details'][dir_path] = dir_info
        
        test_result['status'] = 'success' if test_result['directories_found'] >= 3 else 'warning'
        
        self.validation_results['test_results']['data_directories'] = test_result
        
        if test_result['status'] == 'success':
            self.validation_results['tests_passed'] += 1
        else:
            self.validation_results['tests_failed'] += 1
        
        return test_result['status'] == 'success'
    
    def validate_data_quality(self):
        """éªŒè¯æ•°æ®è´¨é‡"""
        print("ğŸ” éªŒè¯æ•°æ®è´¨é‡...")
        
        test_result = {
            'status': 'unknown',
            'files_checked': 0,
            'quality_passed': 0,
            'quality_metrics': {},
            'errors': []
        }
        
        # æ£€æŸ¥ä¼˜åŒ–æ•°æ®ç›®å½•
        optimized_dir = project_root / 'data' / 'optimized'
        
        if optimized_dir.exists():
            # æ£€æŸ¥ä¸åŒç±»å‹çš„æ•°æ®æ–‡ä»¶
            data_types = ['daily', 'weekly', 'monthly']
            
            for data_type in data_types:
                type_dir = optimized_dir / data_type
                
                if type_dir.exists():
                    parquet_files = list(type_dir.rglob("*.parquet"))
                    csv_files = list(type_dir.rglob("*.csv"))
                    
                    type_metrics = {
                        'parquet_files': len(parquet_files),
                        'csv_files': len(csv_files),
                        'total_files': len(parquet_files) + len(csv_files)
                    }
                    
                    test_result['files_checked'] += type_metrics['total_files']
                    
                    # æ£€æŸ¥æ•°æ®è´¨é‡ï¼ˆæŠ½æ ·ï¼‰
                    if parquet_files:
                        try:
                            sample_file = parquet_files[0]
                            df = pd.read_parquet(sample_file)
                            
                            # åŸºæœ¬è´¨é‡æ£€æŸ¥
                            quality_checks = {
                                'non_empty': len(df) > 0,
                                'no_all_null_columns': not df.isnull().all().any(),
                                'reasonable_size': len(df) < 1000000  # å•æ–‡ä»¶ä¸è¶…è¿‡100ä¸‡è¡Œ
                            }
                            
                            if all(quality_checks.values()):
                                test_result['quality_passed'] += 1
                                print(f"  âœ… {data_type}: æ•°æ®è´¨é‡è‰¯å¥½ ({len(df)}è¡Œ)")
                            else:
                                failed_checks = [k for k, v in quality_checks.items() if not v]
                                test_result['errors'].append(f"{data_type}: è´¨é‡æ£€æŸ¥å¤±è´¥ - {failed_checks}")
                                print(f"  âŒ {data_type}: è´¨é‡æ£€æŸ¥å¤±è´¥")
                            
                            type_metrics['sample_rows'] = len(df)
                            type_metrics['sample_columns'] = len(df.columns)
                            
                        except Exception as e:
                            test_result['errors'].append(f"{data_type}: è¯»å–å¤±è´¥ - {str(e)}")
                            print(f"  âŒ {data_type}: æ— æ³•è¯»å–æ ·æœ¬æ–‡ä»¶ - {e}")
                    else:
                        print(f"  âš ï¸ {data_type}: æ²¡æœ‰Parquetæ–‡ä»¶")
                    
                    test_result['quality_metrics'][data_type] = type_metrics
                    
                else:
                    print(f"  âš ï¸ {data_type}: ç›®å½•ä¸å­˜åœ¨")
        else:
            test_result['errors'].append("ä¼˜åŒ–æ•°æ®ç›®å½•ä¸å­˜åœ¨")
            print(f"  âŒ ä¼˜åŒ–æ•°æ®ç›®å½•ä¸å­˜åœ¨")
        
        test_result['status'] = 'success' if test_result['quality_passed'] > 0 else 'failed'
        
        self.validation_results['test_results']['data_quality'] = test_result
        
        if test_result['status'] == 'success':
            self.validation_results['tests_passed'] += 1
        else:
            self.validation_results['tests_failed'] += 1
        
        return test_result['status'] == 'success'
    
    def validate_data_sources(self):
        """éªŒè¯æ•°æ®æºè¿æ¥"""
        print("ğŸŒ éªŒè¯æ•°æ®æºè¿æ¥...")
        
        test_result = {
            'status': 'unknown',
            'sources_tested': 0,
            'sources_available': 0,
            'source_details': {},
            'errors': []
        }
        
        # æµ‹è¯•ä¼˜çŸ¿è¿æ¥
        test_result['sources_tested'] += 1
        uqer_status = {
            'available': False,
            'error': None
        }
        
        try:
            import uqer
            # å°è¯•åˆ›å»ºå®¢æˆ·ç«¯ï¼ˆä½¿ç”¨æµ‹è¯•tokenï¼‰
            test_token = "test_token"
            client = uqer.Client(token=test_token)
            uqer_status['available'] = True
            test_result['sources_available'] += 1
            print(f"  âœ… ä¼˜çŸ¿API: åº“å¯ç”¨ï¼Œå¯åˆ›å»ºå®¢æˆ·ç«¯")
            
        except ImportError:
            uqer_status['error'] = "uqeråŒ…æœªå®‰è£…"
            print(f"  âŒ ä¼˜çŸ¿API: uqeråŒ…æœªå®‰è£…")
        except Exception as e:
            uqer_status['error'] = str(e)
            print(f"  âš ï¸ ä¼˜çŸ¿API: {str(e)}")
        
        test_result['source_details']['uqer'] = uqer_status
        
        # æµ‹è¯•å…¶ä»–å¯èƒ½çš„æ•°æ®æº
        optional_sources = [
            ('tushare', 'Tushareé‡‘èæ•°æ®'),
            ('yfinance', 'Yahoo Finance'),
            ('akshare', 'AKShareæ•°æ®')
        ]
        
        for source_name, description in optional_sources:
            test_result['sources_tested'] += 1
            source_status = {
                'available': False,
                'description': description,
                'error': None
            }
            
            try:
                __import__(source_name)
                source_status['available'] = True
                test_result['sources_available'] += 1
                print(f"  âœ… {source_name}: å¯ç”¨")
            except ImportError:
                source_status['error'] = f"{source_name}åŒ…æœªå®‰è£…"
                print(f"  âš ï¸ {source_name}: æœªå®‰è£… (å¯é€‰)")
            
            test_result['source_details'][source_name] = source_status
        
        test_result['status'] = 'success' if test_result['sources_available'] > 0 else 'warning'
        
        self.validation_results['test_results']['data_sources'] = test_result
        
        if test_result['status'] in ['success', 'warning']:
            self.validation_results['tests_passed'] += 1
        else:
            self.validation_results['tests_failed'] += 1
        
        return test_result['status'] in ['success', 'warning']
    
    def test_data_pipeline(self):
        """æµ‹è¯•æ•°æ®ç®¡é“"""
        print("ğŸ”„ æµ‹è¯•æ•°æ®ç®¡é“...")
        
        test_result = {
            'status': 'unknown',
            'pipeline_steps': 0,
            'pipeline_passed': 0,
            'errors': []
        }
        
        # æµ‹è¯•æ•°æ®åŠ è½½
        test_result['pipeline_steps'] += 1
        try:
            from core.data.data_manager import DataManager
            dm = DataManager()
            test_result['pipeline_passed'] += 1
            print(f"  âœ… æ•°æ®åŠ è½½: æ•°æ®ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            test_result['errors'].append(f"æ•°æ®åŠ è½½å¤±è´¥: {str(e)}")
            print(f"  âŒ æ•°æ®åŠ è½½: {str(e)}")
        
        # æµ‹è¯•æ•°æ®å¤„ç†
        test_result['pipeline_steps'] += 1
        try:
            # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®æµ‹è¯•å¤„ç†
            test_data = pd.DataFrame({
                'date': pd.date_range('2024-01-01', periods=10),
                'close': np.random.randn(10) * 10 + 100,
                'volume': np.random.randint(1000, 10000, 10)
            })
            
            # åŸºæœ¬æ•°æ®å¤„ç†æµ‹è¯•
            processed_data = test_data.copy()
            processed_data['sma_5'] = processed_data['close'].rolling(5).mean()
            
            if not processed_data['sma_5'].isna().all():
                test_result['pipeline_passed'] += 1
                print(f"  âœ… æ•°æ®å¤„ç†: åŸºæœ¬å¤„ç†åŠŸèƒ½æ­£å¸¸")
            else:
                test_result['errors'].append("æ•°æ®å¤„ç†ç»“æœå¼‚å¸¸")
                print(f"  âŒ æ•°æ®å¤„ç†: ç»“æœå¼‚å¸¸")
                
        except Exception as e:
            test_result['errors'].append(f"æ•°æ®å¤„ç†å¤±è´¥: {str(e)}")
            print(f"  âŒ æ•°æ®å¤„ç†: {str(e)}")
        
        test_result['status'] = 'success' if test_result['pipeline_passed'] >= test_result['pipeline_steps'] * 0.5 else 'failed'
        
        self.validation_results['test_results']['data_pipeline'] = test_result
        
        if test_result['status'] == 'success':
            self.validation_results['tests_passed'] += 1
        else:
            self.validation_results['tests_failed'] += 1
        
        return test_result['status'] == 'success'
    
    def generate_validation_report(self, save_to_file=True):
        """ç”ŸæˆéªŒè¯æŠ¥å‘Š"""
        print("\nğŸ“‹ ç”Ÿæˆæ•°æ®éªŒè¯æŠ¥å‘Š...")
        
        # è®¡ç®—æ€»ä½“çŠ¶æ€
        total_tests = self.validation_results['tests_passed'] + self.validation_results['tests_failed']
        success_rate = self.validation_results['tests_passed'] / total_tests if total_tests > 0 else 0
        
        if success_rate >= 0.9:
            self.validation_results['validation_status'] = 'excellent'
        elif success_rate >= 0.7:
            self.validation_results['validation_status'] = 'good'
        elif success_rate >= 0.5:
            self.validation_results['validation_status'] = 'fair'
        else:
            self.validation_results['validation_status'] = 'poor'
        
        if save_to_file:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            json_file = f"outputs/reports/data_validation_{timestamp}.json"
            
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            Path(json_file).parent.mkdir(parents=True, exist_ok=True)
            
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(self.validation_results, f, indent=2, ensure_ascii=False)
            
            print(f"ğŸ“Š æŠ¥å‘Šå·²ä¿å­˜: {json_file}")
        
        return self.validation_results
    
    def run_integrity_validation(self):
        """è¿è¡Œå®Œæ•´æ€§éªŒè¯"""
        print("ğŸš€ å¼€å§‹æ•°æ®å®Œæ•´æ€§éªŒè¯...\n")
        
        self.validate_data_modules()
        self.validate_data_directories()
        
        self.generate_validation_report()
        
        return self.validation_results['validation_status'] in ['excellent', 'good']
    
    def run_quality_validation(self):
        """è¿è¡Œè´¨é‡éªŒè¯"""
        print("ğŸš€ å¼€å§‹æ•°æ®è´¨é‡éªŒè¯...\n")
        
        self.validate_data_directories()
        self.validate_data_quality()
        
        self.generate_validation_report()
        
        return self.validation_results['validation_status'] in ['excellent', 'good']
    
    def run_sources_validation(self):
        """è¿è¡Œæ•°æ®æºéªŒè¯"""
        print("ğŸš€ å¼€å§‹æ•°æ®æºéªŒè¯...\n")
        
        self.validate_data_sources()
        self.validate_data_modules()
        
        self.generate_validation_report()
        
        return self.validation_results['validation_status'] in ['excellent', 'good', 'fair']
    
    def run_pipeline_validation(self):
        """è¿è¡Œæ•°æ®ç®¡é“éªŒè¯"""
        print("ğŸš€ å¼€å§‹æ•°æ®ç®¡é“éªŒè¯...\n")
        
        self.validate_data_modules()
        self.test_data_pipeline()
        
        self.generate_validation_report()
        
        return self.validation_results['validation_status'] in ['excellent', 'good']
    
    def run_all_validation(self):
        """è¿è¡Œæ‰€æœ‰éªŒè¯"""
        print("ğŸš€ å¼€å§‹å®Œæ•´æ•°æ®éªŒè¯...\n")
        
        self.validate_data_modules()
        self.validate_data_directories()
        self.validate_data_quality()
        self.validate_data_sources()
        self.test_data_pipeline()
        
        self.generate_validation_report()
        
        return self.validation_results['validation_status'] in ['excellent', 'good']

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='æ•°æ®éªŒè¯å™¨')
    parser.add_argument('mode', nargs='?', default='all',
                       choices=['integrity', 'quality', 'sources', 'pipeline', 'all'],
                       help='éªŒè¯æ¨¡å¼ (é»˜è®¤: all)')
    
    args = parser.parse_args()
    
    validator = DataValidator()
    
    try:
        if args.mode == 'integrity':
            success = validator.run_integrity_validation()
        elif args.mode == 'quality':
            success = validator.run_quality_validation()
        elif args.mode == 'sources':
            success = validator.run_sources_validation()
        elif args.mode == 'pipeline':
            success = validator.run_pipeline_validation()
        else:  # all
            success = validator.run_all_validation()
        
        if success:
            print("\nğŸ‰ æ•°æ®éªŒè¯é€šè¿‡ï¼")
            return 0
        else:
            print("\nâš ï¸ æ•°æ®éªŒè¯å‘ç°é—®é¢˜ï¼Œè¯·æŸ¥çœ‹æŠ¥å‘Šè¯¦æƒ…ã€‚")
            return 1
            
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        return 1
    except Exception as e:
        print(f"\nâŒ éªŒè¯ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        return 1

if __name__ == "__main__":
    exit(main())