#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å…¨é¢æ•°æ®å®Œæ•´æ€§éªŒè¯å™¨
æ£€æŸ¥æ‰€æœ‰æœ¬åœ°æ•°æ®çš„æ—¶é—´èŒƒå›´ã€è´¨é‡å’Œå®Œæ•´æ€§
ç”Ÿæˆè¯¦ç»†çš„æ•°æ®å®Œæ•´æ€§æŠ¥å‘Š
"""

import sys
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime, timedelta
import warnings
import json
import os
import glob
from collections import defaultdict, Counter
import re
warnings.filterwarnings('ignore')

class BulkDataIntegrityVerifier:
    """å…¨é¢æ•°æ®å®Œæ•´æ€§éªŒè¯å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–éªŒè¯å™¨"""
        self.base_path = Path("/Users/jackstudio/QuantTrade/data")
        self.target_start = pd.Timestamp('2000-01-01')
        self.target_end = pd.Timestamp('2025-08-31')
        self.verification_results = {}
        self.summary_stats = {
            'total_directories': 0,
            'total_files': 0,
            'total_size_gb': 0,
            'valid_files': 0,
            'invalid_files': 0,
            'missing_files': 0,
            'date_range_issues': 0,
            'data_quality_issues': 0
        }
        
        print(f"ğŸ” æ•°æ®å®Œæ•´æ€§éªŒè¯å™¨")
        print(f"ğŸ“ æ£€æŸ¥è·¯å¾„: {self.base_path}")
        print(f"ğŸ“… ç›®æ ‡æ—¶é—´èŒƒå›´: {self.target_start.date()} - {self.target_end.date()}")
        print("=" * 80)
    
    def discover_all_data_directories(self):
        """å‘ç°æ‰€æœ‰æ•°æ®ç›®å½•"""
        print("ğŸ—‚ï¸ æ‰«ææ‰€æœ‰æ•°æ®ç›®å½•...")
        
        data_directories = {}
        
        # éå†æ‰€æœ‰å­ç›®å½•
        for root, dirs, files in os.walk(self.base_path):
            root_path = Path(root)
            
            # è·³è¿‡ç³»ç»Ÿç›®å½•
            if any(skip in str(root_path) for skip in ['.DS_Store', '__pycache__', '.git']):
                continue
            
            # åªå…³æ³¨åŒ…å«CSVæˆ–parquetæ–‡ä»¶çš„ç›®å½•
            data_files = [f for f in files if f.endswith(('.csv', '.parquet', '.json'))]
            
            if data_files:
                # åˆ†ç±»æ•°æ®ç›®å½•
                relative_path = root_path.relative_to(self.base_path)
                category = self.categorize_directory(relative_path, data_files)
                
                if category not in data_directories:
                    data_directories[category] = []
                
                data_directories[category].append({
                    'path': root_path,
                    'relative_path': str(relative_path),
                    'files': data_files,
                    'file_count': len(data_files)
                })
        
        self.data_directories = data_directories
        self.summary_stats['total_directories'] = sum(len(dirs) for dirs in data_directories.values())
        
        print(f"âœ… å‘ç° {len(data_directories)} ä¸ªæ•°æ®ç±»åˆ«")
        for category, directories in data_directories.items():
            print(f"   ğŸ“Š {category}: {len(directories)} ä¸ªç›®å½•")
        
        return data_directories
    
    def categorize_directory(self, relative_path, files):
        """åˆ†ç±»æ•°æ®ç›®å½•"""
        path_str = str(relative_path).lower()
        
        # è‚¡ç¥¨è¡Œæƒ…æ•°æ®
        if any(keyword in path_str for keyword in ['mktequd', 'daily', 'stocks']):
            if 'adj' in path_str:
                return 'è‚¡ç¥¨æ—¥çº¿å¤æƒæ•°æ®'
            else:
                return 'è‚¡ç¥¨æ—¥çº¿æ•°æ®'
        
        # å‘¨çº¿æ•°æ®
        elif any(keyword in path_str for keyword in ['week', 'weekly']):
            if 'adj' in path_str:
                return 'è‚¡ç¥¨å‘¨çº¿å¤æƒæ•°æ®'
            else:
                return 'è‚¡ç¥¨å‘¨çº¿æ•°æ®'
        
        # æœˆçº¿æ•°æ®
        elif any(keyword in path_str for keyword in ['month', 'monthly']):
            if 'adj' in path_str:
                return 'è‚¡ç¥¨æœˆçº¿å¤æƒæ•°æ®'
            else:
                return 'è‚¡ç¥¨æœˆçº¿æ•°æ®'
        
        # å­£çº¿å¹´çº¿æ•°æ®
        elif any(keyword in path_str for keyword in ['quarter', 'yearly', 'annual']):
            return 'è‚¡ç¥¨å­£å¹´çº¿æ•°æ®'
        
        # è´¢åŠ¡æ•°æ®
        elif any(keyword in path_str for keyword in ['financial', 'fdmt', 'balance', 'income', 'cash']):
            return 'è´¢åŠ¡æŠ¥è¡¨æ•°æ®'
        
        # åŸºé‡‘æ•°æ®
        elif any(keyword in path_str for keyword in ['fund', 'fundnav', 'etf']):
            return 'åŸºé‡‘æ•°æ®'
        
        # æŒ‡æ•°æ•°æ®
        elif any(keyword in path_str for keyword in ['index', 'idx', 'benchmark']):
            return 'æŒ‡æ•°æ•°æ®'
        
        # å› å­æ•°æ®
        elif any(keyword in path_str for keyword in ['factor', 'alpha', 'risk']):
            return 'å› å­æ•°æ®'
        
        # å®è§‚æ•°æ®
        elif any(keyword in path_str for keyword in ['macro', 'economic', 'eco']):
            return 'å®è§‚ç»æµæ•°æ®'
        
        # æœŸè´§æœŸæƒæ•°æ®
        elif any(keyword in path_str for keyword in ['future', 'option', 'derivative']):
            return 'è¡ç”Ÿå“æ•°æ®'
        
        # æ–°é—»èˆ†æƒ…æ•°æ®
        elif any(keyword in path_str for keyword in ['news', 'sentiment', 'research']):
            return 'èµ„è®¯ç ”ç©¶æ•°æ®'
        
        # é…ç½®å’Œæ—¥å¿—
        elif any(keyword in path_str for keyword in ['config', 'log', 'temp', 'cache']):
            return 'ç³»ç»Ÿé…ç½®æ•°æ®'
        
        # æŠ¥å‘Šè¾“å‡º
        elif any(keyword in path_str for keyword in ['output', 'report', 'result']):
            return 'è¾“å‡ºæŠ¥å‘Šæ•°æ®'
        
        # åŸå§‹ä¸‹è½½æ•°æ®
        elif any(keyword in path_str for keyword in ['raw', 'download', 'backup', 'archive']):
            return 'åŸå§‹å¤‡ä»½æ•°æ®'
        
        # å…¶ä»–
        else:
            return 'å…¶ä»–æ•°æ®'
    
    def verify_single_file(self, file_path, category):
        """éªŒè¯å•ä¸ªæ–‡ä»¶"""
        file_result = {
            'file_name': file_path.name,
            'file_path': str(file_path),
            'size_mb': 0,
            'is_valid': False,
            'row_count': 0,
            'column_count': 0,
            'date_range': {
                'start_date': None,
                'end_date': None,
                'date_column': None
            },
            'issues': []
        }
        
        try:
            # è·å–æ–‡ä»¶å¤§å°
            file_result['size_mb'] = file_path.stat().st_size / (1024 * 1024)
            
            # æ ¹æ®æ–‡ä»¶ç±»å‹è¯»å–
            if file_path.suffix == '.csv':
                df = pd.read_csv(file_path, nrows=100)  # åªè¯»å‰100è¡Œè¿›è¡ŒéªŒè¯
            elif file_path.suffix == '.json':
                file_result['is_valid'] = True
                return file_result
            else:
                file_result['issues'].append(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_path.suffix}")
                return file_result
            
            if df is None or len(df) == 0:
                file_result['issues'].append("æ–‡ä»¶ä¸ºç©º")
                return file_result
            
            file_result['row_count'] = len(df)
            file_result['column_count'] = len(df.columns)
            
            # æ£€æŸ¥æ—¥æœŸåˆ—
            date_columns = [col for col in df.columns if any(date_word in col.lower() for date_word in ['date', 'time', 'æ—¥æœŸ'])]
            
            if date_columns:
                date_column = date_columns[0]
                file_result['date_range']['date_column'] = date_column
                
                try:
                    df[date_column] = pd.to_datetime(df[date_column])
                    start_date = df[date_column].min()
                    end_date = df[date_column].max()
                    
                    file_result['date_range']['start_date'] = start_date.strftime('%Y-%m-%d') if pd.notna(start_date) else None
                    file_result['date_range']['end_date'] = end_date.strftime('%Y-%m-%d') if pd.notna(end_date) else None
                    
                    # éªŒè¯æ—¶é—´èŒƒå›´
                    if category in ['è‚¡ç¥¨æ—¥çº¿æ•°æ®', 'è‚¡ç¥¨å‘¨çº¿æ•°æ®', 'è‚¡ç¥¨æœˆçº¿æ•°æ®']:
                        if end_date < pd.Timestamp('2024-01-01'):
                            file_result['issues'].append(f"ç»“æŸæ—¶é—´è¿‡æ—©: {end_date.date()}")
                    
                except Exception as e:
                    file_result['issues'].append(f"æ—¥æœŸè§£æå¤±è´¥: {str(e)}")
            
            # å¦‚æœæ²¡æœ‰ä¸¥é‡é—®é¢˜ï¼Œæ ‡è®°ä¸ºæœ‰æ•ˆ
            serious_issues = [issue for issue in file_result['issues'] if any(serious in issue for serious in ['è¯»å–å¤±è´¥', 'æ–‡ä»¶ä¸ºç©º', 'ç»“æŸæ—¶é—´è¿‡æ—©'])]
            file_result['is_valid'] = len(serious_issues) == 0
            
        except Exception as e:
            file_result['issues'].append(f"éªŒè¯è¿‡ç¨‹å¼‚å¸¸: {str(e)}")
        
        return file_result
    
    def generate_comprehensive_report(self):
        """ç”Ÿæˆå…¨é¢çš„å®Œæ•´æ€§æŠ¥å‘Š"""
        print("\nğŸ“Š ç”Ÿæˆå®Œæ•´æ€§æŠ¥å‘Š...")
        
        # æ”¶é›†æ‰€æœ‰ç›®å½•çš„æ•°æ®
        data_directories = self.discover_all_data_directories()
        
        # éªŒè¯æ¯ä¸ªç±»åˆ«
        for category, directories in data_directories.items():
            if directories:
                print(f"\nğŸ” éªŒè¯ {category}...")
                category_results = {
                    'category': category,
                    'directories': [],
                    'total_files': 0,
                    'valid_files': 0,
                    'total_size_mb': 0,
                    'sample_files': []
                }
                
                for dir_info in directories:
                    dir_path = dir_info['path']
                    print(f"   ğŸ“‚ æ£€æŸ¥: {dir_info['relative_path']}")
                    
                    # æ£€æŸ¥ç›®å½•ä¸­çš„æ–‡ä»¶ (é™åˆ¶å‰5ä¸ªæ–‡ä»¶)
                    valid_files_in_dir = 0
                    dir_size = 0
                    sample_files = []
                    
                    for file_name in dir_info['files'][:5]:
                        file_path = dir_path / file_name
                        if file_path.exists():
                            file_result = self.verify_single_file(file_path, category)
                            if file_result['is_valid']:
                                valid_files_in_dir += 1
                            dir_size += file_result['size_mb']
                            sample_files.append(file_result)
                    
                    dir_summary = {
                        'path': str(dir_path),
                        'relative_path': dir_info['relative_path'],
                        'total_files': len(dir_info['files']),
                        'valid_files': valid_files_in_dir,
                        'size_mb': dir_size
                    }
                    
                    category_results['directories'].append(dir_summary)
                    category_results['total_files'] += len(dir_info['files'])
                    category_results['valid_files'] += valid_files_in_dir
                    category_results['total_size_mb'] += dir_size
                    category_results['sample_files'].extend(sample_files[:2])  # æ¯ä¸ªç›®å½•å–2ä¸ªæ ·æœ¬
                
                self.verification_results[category] = category_results
                print(f"      ğŸ“Š {category}: {category_results['valid_files']}/{category_results['total_files']} æœ‰æ•ˆ")
                print(f"      ğŸ’¾ å¤§å°: {category_results['total_size_mb']:.1f} MB")
        
        # æ›´æ–°æ€»ä½“ç»Ÿè®¡
        for category_result in self.verification_results.values():
            self.summary_stats['total_files'] += category_result['total_files']
            self.summary_stats['valid_files'] += category_result['valid_files']
            self.summary_stats['total_size_gb'] += category_result['total_size_mb'] / 1024
        
        self.summary_stats['invalid_files'] = self.summary_stats['total_files'] - self.summary_stats['valid_files']
        
        # ç”ŸæˆæŠ¥å‘Šæ–‡ä»¶
        report = self.create_simple_report()
        return report
    
    def create_simple_report(self):
        """åˆ›å»ºç®€åŒ–æŠ¥å‘Š"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        report = {
            'verification_info': {
                'timestamp': datetime.now().isoformat(),
                'target_date_range': f"{self.target_start.date()} - {self.target_end.date()}",
                'verification_scope': 'å…¨éƒ¨æœ¬åœ°æ•°æ®'
            },
            'summary_statistics': {
                'total_categories': len(self.verification_results),
                'total_files': self.summary_stats['total_files'],
                'valid_files': self.summary_stats['valid_files'],
                'invalid_files': self.summary_stats['invalid_files'],
                'validity_rate': f"{self.summary_stats['valid_files']/max(self.summary_stats['total_files'], 1)*100:.1f}%",
                'total_size_gb': round(self.summary_stats['total_size_gb'], 2)
            },
            'category_details': self.verification_results
        }
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = f"data_integrity_report_{timestamp}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2, default=str)
        
        # ç”Ÿæˆå¯è¯»æŠ¥å‘Š
        readable_report = self.create_readable_summary(report)
        readable_file = f"data_integrity_report_{timestamp}.txt"
        with open(readable_file, 'w', encoding='utf-8') as f:
            f.write(readable_report)
        
        print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ç”Ÿæˆ:")
        print(f"   ğŸ“Š JSONæŠ¥å‘Š: {report_file}")
        print(f"   ğŸ“ å¯è¯»æŠ¥å‘Š: {readable_file}")
        
        return report, readable_report
    
    def create_readable_summary(self, report):
        """åˆ›å»ºå¯è¯»æ‘˜è¦"""
        lines = []
        lines.append("=" * 100)
        lines.append("ğŸ“Š QuantTrade æ•°æ®å®Œæ•´æ€§éªŒè¯æŠ¥å‘Š")
        lines.append("=" * 100)
        lines.append(f"ğŸ• éªŒè¯æ—¶é—´: {report['verification_info']['timestamp']}")
        lines.append(f"ğŸ“… ç›®æ ‡æ—¶é—´èŒƒå›´: {report['verification_info']['target_date_range']}")
        lines.append("")
        
        # æ€»ä½“ç»Ÿè®¡
        summary = report['summary_statistics']
        lines.append("ğŸ“ˆ æ€»ä½“ç»Ÿè®¡")
        lines.append("-" * 50)
        lines.append(f"æ•°æ®ç±»åˆ«æ•°é‡: {summary['total_categories']}")
        lines.append(f"æ•°æ®æ–‡ä»¶æ€»æ•°: {summary['total_files']}")
        lines.append(f"æœ‰æ•ˆæ–‡ä»¶æ•°é‡: {summary['valid_files']}")
        lines.append(f"æ— æ•ˆæ–‡ä»¶æ•°é‡: {summary['invalid_files']}")
        lines.append(f"æ•°æ®æœ‰æ•ˆç‡: {summary['validity_rate']}")
        lines.append(f"æ•°æ®æ€»å¤§å°: {summary['total_size_gb']} GB")
        lines.append("")
        
        # å„ç±»åˆ«è¯¦æƒ…
        lines.append("ğŸ“‹ å„ç±»åˆ«æ•°æ®æ˜ç»†")
        lines.append("=" * 100)
        
        for category, details in report['category_details'].items():
            lines.append(f"\nğŸ“Š {category}")
            lines.append("-" * 80)
            lines.append(f"ç›®å½•æ•°é‡: {len(details['directories'])}")
            lines.append(f"æ–‡ä»¶æ€»æ•°: {details['total_files']}")
            lines.append(f"æœ‰æ•ˆæ–‡ä»¶: {details['valid_files']}")
            lines.append(f"æ•°æ®å¤§å°: {details['total_size_mb']:.1f} MB")
            
            lines.append(f"\nğŸ“‚ ç›®å½•æ˜ç»†:")
            for dir_info in details['directories']:
                lines.append(f"   ğŸ“ {dir_info['relative_path']}")
                lines.append(f"      æ–‡ä»¶æ•°: {dir_info['total_files']}")
                lines.append(f"      æœ‰æ•ˆæ–‡ä»¶: {dir_info['valid_files']}")
                lines.append(f"      æ•°æ®å¤§å°: {dir_info['size_mb']:.1f} MB")
            
            if details['sample_files']:
                lines.append(f"\nğŸ“‹ æ ·æœ¬æ–‡ä»¶:")
                for sample in details['sample_files'][:3]:
                    status = "âœ…" if sample['is_valid'] else "âŒ"
                    lines.append(f"   {status} {sample['file_name']} ({sample['row_count']} è¡Œ, {sample['size_mb']:.1f} MB)")
                    if sample['date_range']['start_date']:
                        lines.append(f"      æ—¶é—´: {sample['date_range']['start_date']} - {sample['date_range']['end_date']}")
                    if sample['issues']:
                        lines.append(f"      é—®é¢˜: {'; '.join(sample['issues'])}")
        
        return "\n".join(lines)

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” å…¨é¢æ•°æ®å®Œæ•´æ€§éªŒè¯")
    print("=" * 80)
    print("ğŸ“‹ éªŒè¯ç›®æ ‡: æ£€æŸ¥æ‰€æœ‰æœ¬åœ°æ•°æ®çš„å®Œæ•´æ€§å’Œæ—¶é—´èŒƒå›´")
    print("ğŸ“… æ—¶é—´è¦æ±‚: 2000å¹´1æœˆ1æ—¥ - 2025å¹´8æœˆ31æ—¥")
    
    verifier = BulkDataIntegrityVerifier()
    report, readable_report = verifier.generate_comprehensive_report()
    
    print("\n" + "=" * 100)
    print("ğŸ“Š éªŒè¯å®Œæˆï¼ä»¥ä¸‹æ˜¯æŠ¥å‘Šæ‘˜è¦:")
    print("=" * 100)
    print(readable_report.split("ğŸ“‹ å„ç±»åˆ«æ•°æ®æ˜ç»†")[0])
    
    return report

if __name__ == "__main__":
    main()
