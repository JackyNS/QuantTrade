#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
QuantTradeæ¡†æ¶25å¹´å®Œæ•´æ•°æ®è¦†ç›–æŠ¥å‘Š
==============================

ç»¼åˆç»Ÿè®¡2000å¹´1æœˆ1æ—¥è‡³2025å¹´8æœˆ31æ—¥
å…¨å¸‚åœºAè‚¡æ•°æ®å®Œæ•´æ€§å’Œè¦†ç›–æƒ…å†µ
"""

import pandas as pd
from pathlib import Path
from datetime import datetime
import numpy as np

def generate_complete_25_year_report():
    """ç”Ÿæˆ25å¹´å®Œæ•´æ•°æ®è¦†ç›–æŠ¥å‘Š"""
    
    print("ğŸŠ QuantTradeæ¡†æ¶25å¹´å®Œæ•´æ•°æ®è¦†ç›–æŠ¥å‘Š")
    print("=" * 80)
    print(f"â° æŠ¥å‘Šæ—¶é—´: {datetime.now()}")
    print(f"ğŸ“… æ•°æ®è¦†ç›–: 2000å¹´1æœˆ1æ—¥ - 2025å¹´8æœˆ31æ—¥ (25å¹´)")
    print(f"ğŸ¯ ç›®æ ‡: å…¨å¸‚åœºAè‚¡é‡åŒ–äº¤æ˜“æ•°æ®å®Œæ•´æ€§è¯„ä¼°")
    print()
    
    data_path = Path("/Users/jackstudio/QuantTrade/data")
    
    # æ ¸å¿ƒ10ä¸ªAPIæ•°æ®ç»Ÿè®¡
    api_categories = {
        "basic_info": {
            "name": "è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯", 
            "api": "EquGet",
            "priority": "ğŸ”¥ æ ¸å¿ƒ",
            "patterns": ["equget.csv", "*è‚¡ç¥¨åŸºæœ¬*2000_2009.csv"],
            "required": True
        },
        "calendar": {
            "name": "äº¤æ˜“æ—¥å†", 
            "api": "TradeCalGet", 
            "priority": "ğŸ”¥ æ ¸å¿ƒ",
            "patterns": ["trading_calendar.csv", "*äº¤æ˜“æ—¥å†*2000_2009.csv"],
            "required": True
        },
        "daily": {
            "name": "è‚¡ç¥¨æ—¥è¡Œæƒ…", 
            "api": "MktEqudGet",
            "priority": "ğŸ”¥ æ ¸å¿ƒ", 
            "patterns": ["daily_*.csv", "*è‚¡ç¥¨æ—¥è¡Œæƒ…*2000_2009.csv"],
            "required": True
        },
        "adjustment": {
            "name": "å¤æƒå› å­", 
            "api": "MktAdjfGet",
            "priority": "ğŸ’° æŠ€æœ¯",
            "patterns": ["adjustment_*.csv", "*å¤æƒå› å­*2000_2009.csv"],
            "required": True
        },
        "dividend": {
            "name": "è‚¡ç¥¨åˆ†çº¢", 
            "api": "EquDivGet",
            "priority": "ğŸ’° æŠ€æœ¯",
            "patterns": ["dividend_*.csv", "*è‚¡ç¥¨åˆ†çº¢*2000_2009.csv"],
            "required": True
        },
        "financial": {
            "name": "è´¢åŠ¡æ•°æ®", 
            "api": "FdmtBs2018Getç­‰",
            "priority": "ğŸ’° æŠ€æœ¯",
            "patterns": ["*.csv", "*è´¢åŠ¡*2000_2009.csv"],
            "required": True
        },
        "capital_flow": {
            "name": "èµ„é‡‘æµå‘", 
            "api": "MktEquFlowGet",
            "priority": "ğŸ§  æƒ…ç»ª",
            "patterns": ["capital_flow_*.csv", "*èµ„é‡‘æµå‘*2000_2009.csv"],
            "required": False
        },
        "limit_info": {
            "name": "æ¶¨è·Œåœä¿¡æ¯", 
            "api": "MktLimitGet",
            "priority": "ğŸ§  æƒ…ç»ª",
            "patterns": ["limit_*.csv", "*æ¶¨è·Œåœ*2000_2009.csv"],
            "required": False
        },
        "rank_list": {
            "name": "é¾™è™æ¦œæ•°æ®", 
            "api": "MktRankListStocksGet",
            "priority": "ğŸ§  æƒ…ç»ª",
            "patterns": ["rank_*.csv", "*é¾™è™æ¦œ*2000_2009.csv"],
            "required": False
        }
    }
    
    # ç»Ÿè®¡å„æ•°æ®é›†æƒ…å†µ
    total_files = 0
    total_size_gb = 0
    total_records = 0
    coverage_summary = {}
    
    print("ğŸ“Š å„æ•°æ®é›†è¯¦ç»†ç»Ÿè®¡:")
    print("-" * 80)
    
    for category, info in api_categories.items():
        dir_path = data_path / category
        
        print(f"\n{info['priority']} {info['name']} ({info['api']})")
        print("-" * 50)
        
        if dir_path.exists():
            all_files = []
            for pattern in info['patterns']:
                all_files.extend(list(dir_path.glob(pattern)))
            
            # å»é‡
            all_files = list(set(all_files))
            
            if all_files:
                cat_size = 0
                cat_records = 0
                modern_files = []
                historical_files = []
                
                for file in all_files:
                    file_size_mb = file.stat().st_size / (1024*1024)
                    cat_size += file_size_mb
                    
                    try:
                        df = pd.read_csv(file)
                        records = len(df)
                        cat_records += records
                        
                        # åˆ†ç±»ç°ä»£å’Œå†å²æ–‡ä»¶
                        if "2000_2009" in file.name:
                            historical_files.append((file, records, file_size_mb))
                        else:
                            modern_files.append((file, records, file_size_mb))
                            
                    except Exception as e:
                        print(f"   âŒ {file.name}: è¯»å–å¤±è´¥ - {e}")
                        cat_size += file_size_mb
                
                # æ˜¾ç¤ºç°ä»£æ•°æ® (2010-2025)
                if modern_files:
                    print("   ğŸ“ˆ 2010-2025å¹´æ•°æ®:")
                    for file, records, size in modern_files[:3]:  # æ˜¾ç¤ºå‰3ä¸ªæ–‡ä»¶
                        print(f"      ğŸ“„ {file.name}: {records:,} æ¡è®°å½•, {size:.1f}MB")
                    if len(modern_files) > 3:
                        remaining = len(modern_files) - 3
                        remaining_records = sum(r for _, r, _ in modern_files[3:])
                        print(f"      ... è¿˜æœ‰{remaining}ä¸ªæ–‡ä»¶, {remaining_records:,} æ¡è®°å½•")
                
                # æ˜¾ç¤ºå†å²æ•°æ® (2000-2009)
                if historical_files:
                    print("   ğŸ“œ 2000-2009å¹´å†å²æ•°æ®:")
                    for file, records, size in historical_files:
                        print(f"      ğŸ“„ {file.name}: {records:,} æ¡è®°å½•, {size:.1f}MB")
                
                # æ•°æ®å®Œæ•´æ€§è¯„ä¼°
                has_modern = len(modern_files) > 0
                has_historical = len(historical_files) > 0
                
                if has_modern and has_historical:
                    status = "ğŸŠ å®Œæ•´è¦†ç›–"
                    coverage = "25å¹´"
                elif has_modern:
                    status = "â³ éƒ¨åˆ†è¦†ç›–" 
                    coverage = "15å¹´ (2010-2025)"
                elif has_historical:
                    status = "ğŸ“œ å†å²è¦†ç›–"
                    coverage = "10å¹´ (2000-2009)"
                else:
                    status = "âŒ æ— æ•°æ®"
                    coverage = "0å¹´"
                
                print(f"   ğŸ“Š {info['name']}ç»Ÿè®¡:")
                print(f"      ğŸ“ æ–‡ä»¶æ•°: {len(all_files)} ä¸ª")
                print(f"      ğŸ“‹ è®°å½•æ•°: {cat_records:,} æ¡")
                print(f"      ğŸ’¾ æ•°æ®é‡: {cat_size:.1f}MB")
                print(f"      {status} ({coverage})")
                
                coverage_summary[category] = {
                    'name': info['name'],
                    'priority': info['priority'],
                    'files': len(all_files),
                    'records': cat_records,
                    'size_mb': cat_size,
                    'coverage': coverage,
                    'status': status,
                    'required': info['required'],
                    'has_modern': has_modern,
                    'has_historical': has_historical
                }
                
                total_files += len(all_files)
                total_size_gb += cat_size / 1024
                total_records += cat_records
                
            else:
                print(f"   ğŸ“‚ ç›®å½•å­˜åœ¨ä½†æ— åŒ¹é…æ–‡ä»¶")
                coverage_summary[category] = {
                    'name': info['name'], 
                    'priority': info['priority'],
                    'files': 0,
                    'records': 0,
                    'size_mb': 0,
                    'coverage': "0å¹´",
                    'status': "âŒ æ— æ•°æ®",
                    'required': info['required'],
                    'has_modern': False,
                    'has_historical': False
                }
        else:
            print(f"   âŒ ç›®å½•ä¸å­˜åœ¨")
            coverage_summary[category] = {
                'name': info['name'],
                'priority': info['priority'], 
                'files': 0,
                'records': 0,
                'size_mb': 0,
                'coverage': "0å¹´",
                'status': "âŒ æ— æ•°æ®",
                'required': info['required'],
                'has_modern': False,
                'has_historical': False
            }
    
    # æŒ‰ä¼˜å…ˆçº§æ±‡æ€»
    print(f"\nğŸ“ˆ æŒ‰åŠŸèƒ½æ¨¡å—æ±‡æ€»:")
    print("=" * 60)
    
    priority_groups = {
        "ğŸ”¥ æ ¸å¿ƒ": [],
        "ğŸ’° æŠ€æœ¯": [], 
        "ğŸ§  æƒ…ç»ª": []
    }
    
    for cat, summary in coverage_summary.items():
        priority = summary['priority']
        priority_groups[priority].append(summary)
    
    for priority, items in priority_groups.items():
        if items:
            group_files = sum(item['files'] for item in items)
            group_records = sum(item['records'] for item in items)  
            group_size = sum(item['size_mb'] for item in items)
            complete_count = sum(1 for item in items if item['has_modern'] and item['has_historical'])
            
            print(f"\n{priority} æ•°æ® ({len(items)}ä¸ªæ•°æ®é›†)")
            print(f"   ğŸ“ æ€»æ–‡ä»¶: {group_files} ä¸ª")
            print(f"   ğŸ“‹ æ€»è®°å½•: {group_records:,} æ¡") 
            print(f"   ğŸ’¾ æ€»å¤§å°: {group_size:.1f}MB")
            print(f"   ğŸ¯ å®Œæ•´è¦†ç›–: {complete_count}/{len(items)} ä¸ª")
            
            for item in items:
                required_mark = "â­" if item['required'] else "  "
                print(f"   {required_mark} {item['status']} {item['name']}: {item['coverage']}")
    
    # æ€»ä½“è¯„ä¼°
    print(f"\nğŸ¯ QuantTradeæ¡†æ¶æ•°æ®å®Œæ•´æ€§è¯„ä¼°:")
    print("=" * 60)
    
    # è®¡ç®—å„ç§å®Œæˆåº¦
    total_datasets = len(coverage_summary)
    complete_datasets = sum(1 for s in coverage_summary.values() if s['has_modern'] and s['has_historical'])
    partial_datasets = sum(1 for s in coverage_summary.values() if s['has_modern'] or s['has_historical'])
    required_datasets = sum(1 for s in coverage_summary.values() if s['required'])
    required_complete = sum(1 for s in coverage_summary.values() if s['required'] and s['has_modern'] and s['has_historical'])
    
    print(f"ğŸ“Š æ•°æ®æ€»é‡ç»Ÿè®¡:")
    print(f"   ğŸ“ æ€»æ–‡ä»¶æ•°: {total_files} ä¸ª")
    print(f"   ğŸ“‹ æ€»è®°å½•æ•°: {total_records:,} æ¡")
    print(f"   ğŸ’¾ æ€»æ•°æ®é‡: {total_size_gb:.2f} GB")
    print(f"   ğŸ“… æ—¶é—´è·¨åº¦: 25å¹´ (2000-2025)")
    
    print(f"\nğŸ¯ å®Œæ•´æ€§è¯„ä¼°:")
    complete_rate = (complete_datasets / total_datasets) * 100
    partial_rate = (partial_datasets / total_datasets) * 100
    required_rate = (required_complete / required_datasets) * 100 if required_datasets > 0 else 0
    
    print(f"   ğŸ“ˆ 25å¹´å®Œæ•´è¦†ç›–: {complete_datasets}/{total_datasets} ({complete_rate:.1f}%)")
    print(f"   ğŸ“Š éƒ¨åˆ†æ•°æ®è¦†ç›–: {partial_datasets}/{total_datasets} ({partial_rate:.1f}%)")
    print(f"   â­ å¿…éœ€æ•°æ®å®Œæ•´: {required_complete}/{required_datasets} ({required_rate:.1f}%)")
    
    # æ¡†æ¶åŠŸèƒ½æ”¯æŒè¯„ä¼°
    print(f"\nğŸš€ QuantTradeæ¡†æ¶åŠŸèƒ½æ”¯æŒè¯„ä¼°:")
    print("=" * 60)
    
    # æ ¸å¿ƒåŠŸèƒ½è¯„ä¼° (åŸºæœ¬ä¿¡æ¯+æ—¥å†+æ—¥è¡Œæƒ…)
    core_complete = all(coverage_summary[api]['has_modern'] or coverage_summary[api]['has_historical'] 
                       for api in ['basic_info', 'calendar', 'daily'])
    
    # æŠ€æœ¯åˆ†æåŠŸèƒ½ (å¤æƒ+åˆ†çº¢)  
    technical_complete = all(coverage_summary[api]['has_modern'] or coverage_summary[api]['has_historical']
                           for api in ['adjustment', 'dividend'])
    
    # åŸºæœ¬é¢åˆ†æåŠŸèƒ½ (è´¢åŠ¡æ•°æ®)
    fundamental_complete = coverage_summary['financial']['has_modern'] or coverage_summary['financial']['has_historical']
    
    # æƒ…ç»ªåˆ†æåŠŸèƒ½ (èµ„é‡‘æµå‘)
    sentiment_complete = coverage_summary['capital_flow']['has_modern'] or coverage_summary['capital_flow']['has_historical']
    
    print(f"ğŸ”¥ æ ¸å¿ƒäº¤æ˜“åŠŸèƒ½: {'âœ… å®Œå…¨æ”¯æŒ' if core_complete else 'âš ï¸ éƒ¨åˆ†æ”¯æŒ'}")
    print(f"ğŸ’° æŠ€æœ¯åˆ†æåŠŸèƒ½: {'âœ… å®Œå…¨æ”¯æŒ' if technical_complete else 'âš ï¸ éƒ¨åˆ†æ”¯æŒ'}")
    print(f"ğŸ“Š åŸºæœ¬é¢åˆ†æåŠŸèƒ½: {'âœ… å®Œå…¨æ”¯æŒ' if fundamental_complete else 'âš ï¸ éƒ¨åˆ†æ”¯æŒ'}")  
    print(f"ğŸ§  æƒ…ç»ªåˆ†æåŠŸèƒ½: {'âœ… å®Œå…¨æ”¯æŒ' if sentiment_complete else 'âš ï¸ éƒ¨åˆ†æ”¯æŒ'}")
    
    # æœ€ç»ˆè¯„ä¼°
    if complete_rate >= 80 and required_rate >= 90:
        final_status = "ğŸŠ æ¡†æ¶æ•°æ®å°±ç»ª"
        recommendation = "âœ… å¯å¼€å§‹å…¨åŠŸèƒ½é‡åŒ–äº¤æ˜“ç­–ç•¥å¼€å‘!"
    elif complete_rate >= 60 or required_rate >= 80:
        final_status = "â³ æ¡†æ¶åŸºæœ¬å°±ç»ª"
        recommendation = "ğŸ”§ å»ºè®®è¡¥é½å†å²æ•°æ®ä»¥è·å¾—æœ€ä½³å›æµ‹æ•ˆæœ"
    else:
        final_status = "ğŸ”§ æ¡†æ¶æ•°æ®ä¸å®Œæ•´"
        recommendation = "âš ï¸ éœ€è¦ä¸‹è½½æ›´å¤šæ•°æ®æ‰èƒ½æ­£å¸¸ä½¿ç”¨"
    
    print(f"\n{final_status}")
    print(f"ğŸ“‹ ç»¼åˆå®Œæˆåº¦: {complete_rate:.1f}% (å®Œæ•´) + {partial_rate-complete_rate:.1f}% (éƒ¨åˆ†)")
    print(f"ğŸ’¡ {recommendation}")
    
    # æ•°æ®è´¨é‡æç¤º
    if total_size_gb > 1.0:
        print(f"\nğŸ’¾ æ•°æ®è§„æ¨¡: å¤§å‹æ•°æ®é›† ({total_size_gb:.1f}GB)")
        print(f"ğŸš€ æ”¯æŒ: é«˜é¢‘ç­–ç•¥ã€é•¿å‘¨æœŸå›æµ‹ã€å¤šå› å­æ¨¡å‹")
    elif total_size_gb > 0.5:
        print(f"\nğŸ’¾ æ•°æ®è§„æ¨¡: ä¸­å‹æ•°æ®é›† ({total_size_gb:.1f}GB)")
        print(f"ğŸ“ˆ æ”¯æŒ: ä¸­é¢‘ç­–ç•¥ã€ä¸­æœŸå›æµ‹ã€æŠ€æœ¯åˆ†æ")
    else:
        print(f"\nğŸ’¾ æ•°æ®è§„æ¨¡: å°å‹æ•°æ®é›† ({total_size_gb:.1f}GB)")
        print(f"ğŸ“Š æ”¯æŒ: åŸºç¡€ç­–ç•¥ã€çŸ­æœŸåˆ†æ")

if __name__ == "__main__":
    generate_complete_25_year_report()