#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æœ¬åœ°æ•°æ®å…¨é¢æ£€æŸ¥å’Œæ¢³ç†å·¥å…·
è¿™æ˜¯å”¯ä¸€ä¸”é¦–è¦çš„å·¥ä½œï¼šææ¸…æ¥šæœ¬åœ°æ•°æ®çš„æ‰€æœ‰æ˜ç»†
"""

import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
import json
import warnings
from collections import defaultdict
import os
import subprocess
warnings.filterwarnings('ignore')

class CompleteLocalDataInspector:
    """æœ¬åœ°æ•°æ®å…¨é¢æ£€æŸ¥å’Œæ¢³ç†å·¥å…·"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ£€æŸ¥å·¥å…·"""
        self.base_path = Path("/Users/jackstudio/QuantTrade/data")
        self.report = {
            'inspection_time': datetime.now().isoformat(),
            'total_size': None,
            'directory_structure': {},
            'file_inventory': {},
            'data_content_analysis': {},
            'stock_universe': {},
            'time_coverage': {},
            'data_quality': {},
            'recommendations': []
        }
    
    def get_total_size(self):
        """è·å–æ€»æ•°æ®å¤§å°"""
        print("ğŸ“ è·å–æ€»æ•°æ®å¤§å°...")
        try:
            result = subprocess.run(['du', '-sh', str(self.base_path)], 
                                  capture_output=True, text=True)
            total_size = result.stdout.split()[0]
            print(f"   ğŸ’¾ æ€»å¤§å°: {total_size}")
            self.report['total_size'] = total_size
            return total_size
        except Exception as e:
            print(f"   âŒ è·å–å¤§å°å¤±è´¥: {e}")
            return "Unknown"
    
    def analyze_directory_structure(self):
        """åˆ†æç›®å½•ç»“æ„"""
        print("\nğŸ“ åˆ†æç›®å½•ç»“æ„...")
        
        structure = {}
        
        for item in self.base_path.iterdir():
            if item.is_dir():
                try:
                    # è·å–ç›®å½•å¤§å°
                    result = subprocess.run(['du', '-sh', str(item)], 
                                          capture_output=True, text=True)
                    size = result.stdout.split()[0] if result.returncode == 0 else "Unknown"
                    
                    # ç»Ÿè®¡æ–‡ä»¶æ•°é‡
                    file_count = sum(1 for _ in item.rglob('*') if _.is_file())
                    
                    structure[item.name] = {
                        'path': str(item),
                        'size': size,
                        'file_count': file_count,
                        'subdirs': [sub.name for sub in item.iterdir() if sub.is_dir()]
                    }
                    
                    print(f"   ğŸ“‚ {item.name}: {size}, {file_count} ä¸ªæ–‡ä»¶")
                    
                except Exception as e:
                    structure[item.name] = {
                        'path': str(item),
                        'size': 'Error',
                        'error': str(e)
                    }
        
        self.report['directory_structure'] = structure
        return structure
    
    def inventory_batch_files(self):
        """æ¸…ç‚¹æ‰¹æ¬¡æ–‡ä»¶"""
        print("\nğŸ“„ æ¸…ç‚¹æ‰¹æ¬¡æ–‡ä»¶...")
        
        batch_dirs = [
            'priority_download/market_data/daily',
            'priority_download/market_data/weekly', 
            'priority_download/market_data/monthly'
        ]
        
        batch_inventory = {}
        
        for batch_dir in batch_dirs:
            batch_path = self.base_path / batch_dir
            data_type = batch_dir.split('/')[-1]
            
            if not batch_path.exists():
                batch_inventory[data_type] = {'exists': False}
                print(f"   âŒ {data_type}: ç›®å½•ä¸å­˜åœ¨")
                continue
            
            # è·å–æ‰€æœ‰CSVæ–‡ä»¶
            csv_files = list(batch_path.glob('*.csv'))
            
            # æŒ‰å¹´ä»½åˆ†ç»„ç»Ÿè®¡
            year_groups = defaultdict(list)
            for file_path in csv_files:
                try:
                    year = file_path.stem.split('_')[0]
                    year_groups[year].append(file_path.name)
                except:
                    year_groups['unknown'].append(file_path.name)
            
            batch_inventory[data_type] = {
                'exists': True,
                'total_files': len(csv_files),
                'year_distribution': {year: len(files) for year, files in year_groups.items()},
                'year_range': f"{min(year_groups.keys())} - {max(year_groups.keys())}" if year_groups else "None"
            }
            
            print(f"   ğŸ“Š {data_type}: {len(csv_files)} ä¸ªæ–‡ä»¶ ({min(year_groups.keys()) if year_groups else 'N/A'} - {max(year_groups.keys()) if year_groups else 'N/A'})")
        
        self.report['file_inventory']['batch_files'] = batch_inventory
        return batch_inventory
    
    def inventory_csv_complete_files(self):
        """æ¸…ç‚¹csv_completeæ–‡ä»¶"""
        print("\nğŸ“Š æ¸…ç‚¹csv_completeæ–‡ä»¶...")
        
        csv_complete_path = self.base_path / 'csv_complete'
        
        if not csv_complete_path.exists():
            print("   âŒ csv_completeç›®å½•ä¸å­˜åœ¨")
            self.report['file_inventory']['csv_complete'] = {'exists': False}
            return {'exists': False}
        
        csv_inventory = {'exists': True}
        
        # æ£€æŸ¥å„æ—¶é—´å‘¨æœŸç›®å½•
        timeframes = ['daily', 'weekly', 'monthly']
        
        for tf in timeframes:
            tf_path = csv_complete_path / tf
            
            if tf_path.exists():
                # é€’å½’ç»Ÿè®¡æ‰€æœ‰CSVæ–‡ä»¶
                csv_files = list(tf_path.rglob('*.csv'))
                
                # åˆ†ææ–‡ä»¶åˆ†å¸ƒ
                if csv_files:
                    # æŒ‰ç›®å½•å±‚çº§åˆ†æ
                    dir_levels = defaultdict(int)
                    for file_path in csv_files:
                        relative_path = file_path.relative_to(tf_path)
                        level = len(relative_path.parts) - 1  # å‡å»æ–‡ä»¶å
                        dir_levels[f"level_{level}"] += 1
                    
                    csv_inventory[tf] = {
                        'file_count': len(csv_files),
                        'directory_levels': dict(dir_levels),
                        'sample_files': [f.name for f in csv_files[:5]]  # å‰5ä¸ªæ–‡ä»¶æ ·æœ¬
                    }
                else:
                    csv_inventory[tf] = {'file_count': 0}
                
                print(f"   ğŸ“‚ {tf}: {len(csv_files)} ä¸ªæ–‡ä»¶")
            else:
                csv_inventory[tf] = {'exists': False, 'file_count': 0}
                print(f"   âŒ {tf}: ç›®å½•ä¸å­˜åœ¨")
        
        self.report['file_inventory']['csv_complete'] = csv_inventory
        return csv_inventory
    
    def analyze_sample_data_content(self):
        """åˆ†ææ ·æœ¬æ•°æ®å†…å®¹"""
        print("\nğŸ” åˆ†ææ ·æœ¬æ•°æ®å†…å®¹...")
        
        content_analysis = {}
        
        # åˆ†ææ‰¹æ¬¡æ–‡ä»¶æ ·æœ¬
        daily_path = self.base_path / 'priority_download/market_data/daily'
        if daily_path.exists():
            sample_files = list(daily_path.glob('*.csv'))[:5]  # å–5ä¸ªæ ·æœ¬
            
            batch_samples = []
            all_stocks = set()
            all_dates = []
            
            for sample_file in sample_files:
                try:
                    df = pd.read_csv(sample_file)
                    
                    # åˆ†ææ–‡ä»¶å†…å®¹
                    file_analysis = {
                        'filename': sample_file.name,
                        'records': len(df),
                        'columns': list(df.columns),
                        'stock_count': len(df['secID'].unique()) if 'secID' in df.columns else 0,
                        'date_range': None
                    }
                    
                    # åˆ†ææ—¥æœŸèŒƒå›´
                    if 'tradeDate' in df.columns:
                        df['tradeDate'] = pd.to_datetime(df['tradeDate'])
                        min_date = df['tradeDate'].min()
                        max_date = df['tradeDate'].max()
                        file_analysis['date_range'] = {
                            'start': min_date.strftime('%Y-%m-%d'),
                            'end': max_date.strftime('%Y-%m-%d')
                        }
                        all_dates.extend([min_date, max_date])
                    
                    # æ”¶é›†è‚¡ç¥¨ä»£ç 
                    if 'secID' in df.columns:
                        stocks = df['secID'].unique()
                        all_stocks.update(stocks)
                        file_analysis['sample_stocks'] = list(stocks[:5])
                    
                    batch_samples.append(file_analysis)
                    print(f"   ğŸ“„ {sample_file.name}: {len(df)} æ¡è®°å½•, {file_analysis['stock_count']} åªè‚¡ç¥¨")
                    
                except Exception as e:
                    batch_samples.append({
                        'filename': sample_file.name,
                        'error': str(e)
                    })
                    print(f"   âŒ {sample_file.name}: è¯»å–å¤±è´¥")
            
            content_analysis['batch_samples'] = {
                'sample_files': batch_samples,
                'total_unique_stocks': len(all_stocks),
                'sample_stock_list': list(all_stocks)[:20],  # å‰20åªæ ·æœ¬è‚¡ç¥¨
                'overall_date_range': {
                    'earliest': min(all_dates).strftime('%Y-%m-%d') if all_dates else None,
                    'latest': max(all_dates).strftime('%Y-%m-%d') if all_dates else None
                }
            }
            
            print(f"   ğŸ“ˆ æ ·æœ¬ç»Ÿè®¡: {len(all_stocks)} åªè‚¡ç¥¨")
            if all_dates:
                print(f"   ğŸ“… æ—¶é—´è·¨åº¦: {min(all_dates).date()} - {max(all_dates).date()}")
        
        # åˆ†æcsv_completeæ ·æœ¬
        csv_daily_path = self.base_path / 'csv_complete/daily'
        if csv_daily_path.exists():
            csv_files = list(csv_daily_path.rglob('*.csv'))
            
            if csv_files:
                sample_csv = csv_files[0]  # å–ä¸€ä¸ªæ ·æœ¬
                try:
                    df_csv = pd.read_csv(sample_csv)
                    
                    csv_sample = {
                        'sample_file': sample_csv.name,
                        'records': len(df_csv),
                        'columns': list(df_csv.columns),
                        'structure': 'individual_stock_files'
                    }
                    
                    if 'tradeDate' in df_csv.columns:
                        df_csv['tradeDate'] = pd.to_datetime(df_csv['tradeDate'])
                        csv_sample['date_range'] = {
                            'start': df_csv['tradeDate'].min().strftime('%Y-%m-%d'),
                            'end': df_csv['tradeDate'].max().strftime('%Y-%m-%d')
                        }
                    
                    content_analysis['csv_complete_sample'] = csv_sample
                    print(f"   ğŸ“Š CSVæ ·æœ¬: {sample_csv.name}, {len(df_csv)} æ¡è®°å½•")
                    
                except Exception as e:
                    content_analysis['csv_complete_sample'] = {'error': str(e)}
        
        self.report['data_content_analysis'] = content_analysis
        return content_analysis
    
    def estimate_full_coverage(self):
        """ä¼°ç®—å®Œæ•´æ•°æ®è¦†ç›–æƒ…å†µ"""
        print("\nğŸ¯ ä¼°ç®—å®Œæ•´æ•°æ®è¦†ç›–...")
        
        coverage_estimate = {}
        
        # åŸºäºæ‰¹æ¬¡æ–‡ä»¶ä¼°ç®—
        daily_path = self.base_path / 'priority_download/market_data/daily'
        if daily_path.exists():
            batch_files = list(daily_path.glob('*.csv'))
            
            # æŒ‰å¹´ä»½ç»Ÿè®¡
            year_stats = defaultdict(lambda: {'files': 0, 'estimated_stocks': 0})
            
            for file_path in batch_files:
                try:
                    year = file_path.stem.split('_')[0]
                    year_stats[year]['files'] += 1
                    year_stats[year]['estimated_stocks'] += 100  # å‡è®¾æ¯æ–‡ä»¶100åªè‚¡ç¥¨
                except:
                    continue
            
            # æ€»ä½“ä¼°ç®—
            total_files = len(batch_files)
            estimated_total_stocks = len(batch_files) * 100  # ç²—ç•¥ä¼°ç®—
            
            coverage_estimate['batch_files'] = {
                'total_batch_files': total_files,
                'estimated_stock_records': estimated_total_stocks,
                'year_coverage': dict(year_stats),
                'time_span': f"{min(year_stats.keys())} - {max(year_stats.keys())}" if year_stats else "Unknown"
            }
            
            print(f"   ğŸ“„ æ‰¹æ¬¡æ–‡ä»¶: {total_files} ä¸ª")
            print(f"   ğŸ“ˆ ä¼°ç®—è‚¡ç¥¨è®°å½•: {estimated_total_stocks:,} æ¡")
            print(f"   ğŸ“… å¹´ä»½è¦†ç›–: {min(year_stats.keys())} - {max(year_stats.keys())}")
        
        # CSVå®Œæ•´æ–‡ä»¶ä¼°ç®—
        csv_path = self.base_path / 'csv_complete'
        if csv_path.exists():
            daily_csv_files = list((csv_path / 'daily').rglob('*.csv'))
            
            coverage_estimate['csv_complete'] = {
                'individual_stock_files': len(daily_csv_files),
                'estimated_unique_stocks': len(daily_csv_files),  # å‡è®¾æ¯æ–‡ä»¶ä¸€åªè‚¡ç¥¨
                'organization': 'by_stock'
            }
            
            print(f"   ğŸ“Š ä¸ªè‚¡æ–‡ä»¶: {len(daily_csv_files)} ä¸ª")
        
        self.report['stock_universe'] = coverage_estimate
        return coverage_estimate
    
    def generate_recommendations(self):
        """ç”Ÿæˆä½¿ç”¨å»ºè®®"""
        recommendations = []
        
        # åŸºäºåˆ†æç»“æœç”Ÿæˆå»ºè®®
        if 'batch_files' in self.report.get('file_inventory', {}):
            batch_info = self.report['file_inventory']['batch_files']
            if batch_info.get('daily', {}).get('total_files', 0) > 0:
                recommendations.append("æ‰¹æ¬¡æ•°æ®æ–‡ä»¶å®Œæ•´ï¼Œé€‚åˆè¿›è¡Œå¤§è§„æ¨¡è‚¡ç¥¨ç­›é€‰å’Œåˆ†æ")
        
        if 'csv_complete' in self.report.get('file_inventory', {}):
            csv_info = self.report['file_inventory']['csv_complete']
            if csv_info.get('daily', {}).get('file_count', 0) > 0:
                recommendations.append("ä¸ªè‚¡CSVæ–‡ä»¶å®Œæ•´ï¼Œé€‚åˆå•è‚¡ç¥¨è¯¦ç»†åˆ†æ")
        
        if self.report.get('stock_universe', {}).get('batch_files', {}).get('time_span'):
            time_span = self.report['stock_universe']['batch_files']['time_span']
            if '2000' in time_span and '2025' in time_span:
                recommendations.append("æ—¶é—´è¦†ç›–å®Œæ•´(2000-2025)ï¼Œæ»¡è¶³é•¿æœŸå†å²åˆ†æéœ€æ±‚")
        
        recommendations.append("æ•°æ®é‡åºå¤§(220GB)ï¼Œå»ºè®®ä½¿ç”¨é«˜æ•ˆçš„åˆ†æç­–ç•¥")
        recommendations.append("ä¼˜å…ˆä½¿ç”¨æ‰¹æ¬¡æ–‡ä»¶è¿›è¡Œç­–ç•¥ç­›é€‰ï¼Œå†ç”¨ä¸ªè‚¡æ–‡ä»¶è¿›è¡Œè¯¦ç»†åˆ†æ")
        
        self.report['recommendations'] = recommendations
        return recommendations
    
    def generate_complete_report(self):
        """ç”Ÿæˆå®Œæ•´æ£€æŸ¥æŠ¥å‘Š"""
        print("ğŸ” å¼€å§‹æœ¬åœ°æ•°æ®å…¨é¢æ£€æŸ¥å’Œæ¢³ç†...")
        print("ğŸ¯ ç›®æ ‡: ææ¸…æ¥šæœ¬åœ°æ•°æ®çš„æ‰€æœ‰æ˜ç»†")
        print("=" * 80)
        
        # æ‰§è¡Œæ‰€æœ‰æ£€æŸ¥
        self.get_total_size()
        self.analyze_directory_structure()
        self.inventory_batch_files()
        self.inventory_csv_complete_files()
        self.analyze_sample_data_content()
        self.estimate_full_coverage()
        self.generate_recommendations()
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        report_file = self.base_path / 'complete_local_data_inspection_report.json'
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(self.report, f, ensure_ascii=False, indent=2, default=str)
        
        # ç”Ÿæˆå¯è¯»æ€§æ€»ç»“
        self.print_summary(report_file)
    
    def print_summary(self, report_file):
        """æ‰“å°æ€»ç»“æŠ¥å‘Š"""
        print("\nğŸŠ æœ¬åœ°æ•°æ®å…¨é¢æ£€æŸ¥å®Œæˆ!")
        print("=" * 80)
        print("ğŸ“Š æ•°æ®æ¦‚è§ˆ:")
        print(f"   ğŸ’¾ æ€»å¤§å°: {self.report.get('total_size', 'Unknown')}")
        
        # ç›®å½•ç»“æ„æ€»ç»“
        dir_count = len(self.report.get('directory_structure', {}))
        print(f"   ğŸ“ ä¸»è¦ç›®å½•: {dir_count} ä¸ª")
        
        # æ‰¹æ¬¡æ–‡ä»¶æ€»ç»“
        batch_info = self.report.get('file_inventory', {}).get('batch_files', {})
        if 'daily' in batch_info:
            daily_files = batch_info['daily'].get('total_files', 0)
            print(f"   ğŸ“„ æ—¥çº¿æ‰¹æ¬¡æ–‡ä»¶: {daily_files} ä¸ª")
        
        # CSVæ–‡ä»¶æ€»ç»“
        csv_info = self.report.get('file_inventory', {}).get('csv_complete', {})
        if 'daily' in csv_info:
            csv_files = csv_info['daily'].get('file_count', 0)
            print(f"   ğŸ“Š ä¸ªè‚¡CSVæ–‡ä»¶: {csv_files:,} ä¸ª")
        
        # æ•°æ®èŒƒå›´æ€»ç»“
        coverage = self.report.get('stock_universe', {}).get('batch_files', {})
        if 'time_span' in coverage:
            print(f"   ğŸ“… æ—¶é—´è·¨åº¦: {coverage['time_span']}")
        if 'estimated_stock_records' in coverage:
            print(f"   ğŸ“ˆ ä¼°ç®—è®°å½•: {coverage['estimated_stock_records']:,} æ¡")
        
        print(f"\nğŸ’¡ æ ¸å¿ƒå»ºè®®:")
        for i, rec in enumerate(self.report.get('recommendations', []), 1):
            print(f"   {i}. {rec}")
        
        print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Š: {report_file}")
        print("âœ… æœ¬åœ°æ•°æ®æ˜ç»†å·²å…¨éƒ¨æ¢³ç†å®Œæˆ!")

def main():
    """ä¸»å‡½æ•°"""
    inspector = CompleteLocalDataInspector()
    inspector.generate_complete_report()

if __name__ == "__main__":
    main()