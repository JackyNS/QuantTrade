#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å…¨é¢Aè‚¡æ•°æ®ä¸‹è½½å™¨
åŸºäºä¸“ä¸šç‰ˆéªŒè¯æˆåŠŸï¼Œæ‰©å¤§èŒƒå›´åˆ°å…¨éƒ¨Aè‚¡
"""

import sys
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime, timedelta
import warnings
import json
import time
import os
from io import StringIO
import concurrent.futures
from threading import Lock
warnings.filterwarnings('ignore')

try:
    import uqer
    print("âœ… UQER API å¯ç”¨")
    UQER_AVAILABLE = True
except ImportError:
    print("âŒ UQER API ä¸å¯ç”¨")
    UQER_AVAILABLE = False
    sys.exit(1)

class ComprehensiveAStockDownloader:
    """å…¨é¢Aè‚¡æ•°æ®ä¸‹è½½å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–ä¸‹è½½å™¨"""
        self.setup_uqer()
        self.setup_paths()
        self.stats = {
            'start_time': datetime.now(),
            'total_stocks': 0,
            'successful_stocks': 0,
            'failed_stocks': 0,
            'total_api_calls': 0,
            'total_records': 0,
            'total_files': 0
        }
        
    def setup_uqer(self):
        """è®¾ç½®UQERè¿æ¥"""
        try:
            uqer_token = "68b9922817ae6273137bda7acba81e293582ba347281dfcc056dcb245b23faf3"
            uqer.Client(token=uqer_token)
            print("âœ… UQERæé€Ÿç‰ˆè¿æ¥æˆåŠŸ")
            self.uqer_connected = True
        except Exception as e:
            print(f"âŒ UQERè¿æ¥å¤±è´¥: {e}")
            self.uqer_connected = False
            sys.exit(1)
    
    def setup_paths(self):
        """è®¾ç½®è·¯å¾„"""
        # å…¨é¢æ•°æ®è·¯å¾„
        self.base_path = Path("/Users/jackstudio/QuantTrade/data/comprehensive_complete")
        self.base_path.mkdir(exist_ok=True)
        
        # æ•°æ®ç±»å‹æ˜ å°„
        self.data_types = {
            'daily': 'æ—¥çº¿æ•°æ®',
            'daily_adj': 'å‰å¤æƒæ—¥çº¿ [æé€Ÿç‰ˆ]',
            'weekly': 'å‘¨çº¿æ•°æ®', 
            'weekly_adj': 'å‰å¤æƒå‘¨çº¿ [æé€Ÿç‰ˆ]',
            'monthly': 'æœˆçº¿æ•°æ®',
            'monthly_adj': 'å‰å¤æƒæœˆçº¿'
        }
        
        # APIæ˜ å°„
        self.api_mappings = {
            'daily': uqer.DataAPI.MktEqudGet,
            'daily_adj': uqer.DataAPI.MktEqudAdjGet,
            'weekly': uqer.DataAPI.MktEquwGet,
            'weekly_adj': uqer.DataAPI.MktEquwAdjGet,
            'monthly': uqer.DataAPI.MktEqumGet,
            'monthly_adj': uqer.DataAPI.MktEqumAdjGet
        }
        
        # åˆ›å»ºç›®å½•ç»“æ„
        for data_type in self.data_types:
            type_path = self.base_path / data_type / "stocks"
            type_path.mkdir(parents=True, exist_ok=True)
            
        print(f"ğŸ“ å…¨é¢æ•°æ®å­˜å‚¨è·¯å¾„: {self.base_path}")
        
    def get_all_a_stocks(self):
        """è·å–å…¨éƒ¨Aè‚¡åˆ—è¡¨ï¼ˆä¸Šå¸‚+é€€å¸‚ï¼‰"""
        print("ğŸ“‹ è·å–å…¨éƒ¨Aè‚¡è‚¡ç¥¨åˆ—è¡¨...")
        
        all_stocks = []
        
        try:
            # è·å–å½“å‰ä¸Šå¸‚çš„Aè‚¡
            print("   ğŸ“ˆ è·å–ä¸Šå¸‚Aè‚¡...")
            listed_result = uqer.DataAPI.EquGet(
                listStatusCD="L",  # ä¸Šå¸‚çŠ¶æ€
                pandas=1
            )
            
            if isinstance(listed_result, str):
                listed_df = pd.read_csv(StringIO(listed_result))
            else:
                listed_df = listed_result
                
            listed_a_stocks = listed_df[listed_df['secID'].str.contains(r'\.(XSHE|XSHG)$', na=False)]
            all_stocks.extend(listed_a_stocks['secID'].tolist())
            print(f"      âœ… ä¸Šå¸‚Aè‚¡: {len(listed_a_stocks)} åª")
            
            # è·å–é€€å¸‚çš„Aè‚¡
            print("   ğŸ“‰ è·å–é€€å¸‚Aè‚¡...")
            delisted_result = uqer.DataAPI.EquGet(
                listStatusCD="DE",  # é€€å¸‚çŠ¶æ€
                pandas=1
            )
            
            if isinstance(delisted_result, str):
                delisted_df = pd.read_csv(StringIO(delisted_result))
            else:
                delisted_df = delisted_result
                
            delisted_a_stocks = delisted_df[delisted_df['secID'].str.contains(r'\.(XSHE|XSHG)$', na=False)]
            all_stocks.extend(delisted_a_stocks['secID'].tolist())
            print(f"      âœ… é€€å¸‚Aè‚¡: {len(delisted_a_stocks)} åª")
            
        except Exception as e:
            print(f"âŒ è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")
            return []
        
        # å»é‡å¹¶æ’åº
        all_stocks = list(set(all_stocks))
        all_stocks.sort()
        
        print(f"   ğŸ“Š å…¨éƒ¨Aè‚¡æ€»æ•°: {len(all_stocks)} åª")
        
        # ä¿å­˜è‚¡ç¥¨åˆ—è¡¨
        stocks_info = {
            'total_count': len(all_stocks),
            'listed_count': len(listed_a_stocks),
            'delisted_count': len(delisted_a_stocks),
            'stock_list': all_stocks,
            'update_time': datetime.now().isoformat()
        }
        
        with open(self.base_path / 'a_stocks_list.json', 'w', encoding='utf-8') as f:
            json.dump(stocks_info, f, ensure_ascii=False, indent=2)
            
        return all_stocks
    
    def download_stock_data(self, stock_id, batch_id=1, total_stocks=1):
        """ä¸‹è½½å•åªè‚¡ç¥¨çš„æ‰€æœ‰æ—¶é—´å‘¨æœŸæ•°æ®"""
        
        print(f"ğŸ“ˆ [{batch_id}/{total_stocks}] å¤„ç†: {stock_id}")
        
        success_count = 0
        total_records = 0
        
        for data_type, api_func in self.api_mappings.items():
            try:
                result = api_func(
                    secID=stock_id,
                    beginDate="20000101",
                    endDate="20250831",  # æ¢å¤åˆ°åŸå§‹æ­£ç¡®ç›®æ ‡
                    pandas=1
                )
                
                self.stats['total_api_calls'] += 1
                
                if result is None:
                    continue
                
                # å¤„ç†APIè¿”å›æ•°æ®
                if isinstance(result, str):
                    df = pd.read_csv(StringIO(result))
                elif isinstance(result, pd.DataFrame):
                    df = result.copy()
                else:
                    continue
                
                if len(df) == 0:
                    continue
                
                # éªŒè¯æ—¶é—´èŒƒå›´
                date_column = 'tradeDate' if 'tradeDate' in df.columns else 'endDate'
                if date_column in df.columns:
                    df[date_column] = pd.to_datetime(df[date_column])
                    latest_date = df[date_column].max()
                    earliest_date = df[date_column].min()
                    
                    # ä¿å­˜æ•°æ®
                    save_path = self.base_path / data_type / "stocks"
                    file_name = f"{stock_id.replace('.', '_')}.csv"
                    file_path = save_path / file_name
                    
                    df.to_csv(file_path, index=False, encoding='utf-8')
                    success_count += 1
                    total_records += len(df)
                    self.stats['total_files'] += 1
                    
                    print(f"   âœ… {data_type}: {len(df)} æ¡è®°å½• ({earliest_date.date()} - {latest_date.date()})")
                
                time.sleep(0.15)  # é€‚å½“é™é€Ÿ
                
            except Exception as e:
                print(f"   âŒ {data_type}: ä¸‹è½½å¤±è´¥")
                continue
        
        if success_count > 0:
            print(f"   âœ… æˆåŠŸ: {success_count}/6 ä¸ªæ—¶é—´å‘¨æœŸï¼Œæ€»è®¡ {total_records} æ¡è®°å½•")
            self.stats['successful_stocks'] += 1
            self.stats['total_records'] += total_records
        else:
            print(f"   âŒ å¤±è´¥: 0/6 ä¸ªæ—¶é—´å‘¨æœŸæˆåŠŸ")
            self.stats['failed_stocks'] += 1
        
        return success_count > 0
    
    def download_comprehensive_data(self, stocks_list, batch_size=100, max_stocks=None):
        """å…¨é¢ä¸‹è½½Aè‚¡æ•°æ®"""
        print(f"ğŸš€ å¼€å§‹å…¨é¢Aè‚¡æ•°æ®ä¸‹è½½")
        print(f"ğŸ“… æ—¶é—´èŒƒå›´: 2000å¹´1æœˆ1æ—¥ - 2025å¹´8æœˆ31æ—¥")
        print(f"ğŸ“Š è‚¡ç¥¨æ€»æ•°: {len(stocks_list)}")
        print(f"ğŸ“¦ æ‰¹æ¬¡å¤§å°: {batch_size}")
        
        if max_stocks:
            stocks_list = stocks_list[:max_stocks]
            print(f"ğŸ¯ æœ¬æ¬¡é™åˆ¶: {max_stocks} åªè‚¡ç¥¨")
            
        print("=" * 80)
        
        total_stocks = len(stocks_list)
        self.stats['total_stocks'] = total_stocks
        
        # åˆ†æ‰¹å¤„ç†
        for batch_start in range(0, total_stocks, batch_size):
            batch_end = min(batch_start + batch_size, total_stocks)
            batch_stocks = stocks_list[batch_start:batch_end]
            batch_num = batch_start // batch_size + 1
            
            print(f"\\nğŸ“¦ ç¬¬{batch_num}æ‰¹: å¤„ç†ç¬¬{batch_start+1}-{batch_end}åªè‚¡ç¥¨")
            print("-" * 60)
            
            # å¤„ç†å½“å‰æ‰¹æ¬¡
            for i, stock_id in enumerate(batch_stocks, 1):
                global_index = batch_start + i
                self.download_stock_data(stock_id, global_index, total_stocks)
                
                # æ¯25åªæ˜¾ç¤ºè¿›åº¦
                if i % 25 == 0 or i == len(batch_stocks):
                    progress = (global_index / total_stocks) * 100
                    print(f"   ğŸ“ˆ æ‰¹æ¬¡è¿›åº¦: {i}/{len(batch_stocks)} | æ€»ä½“è¿›åº¦: {global_index}/{total_stocks} ({progress:.1f}%)")
            
            # æ‰¹æ¬¡é—´ä¼‘æ¯
            if batch_end < total_stocks:
                print(f"â¸ï¸ æ‰¹æ¬¡å®Œæˆï¼Œä¼‘æ¯60ç§’...")
                time.sleep(60)
        
        self.create_summary()
    
    def create_summary(self):
        """åˆ›å»ºä¸‹è½½æ€»ç»“"""
        end_time = datetime.now()
        duration = end_time - self.stats['start_time']
        success_rate = (self.stats['successful_stocks'] / self.stats['total_stocks'] * 100) if self.stats['total_stocks'] > 0 else 0
        
        summary = {
            'comprehensive_download_info': {
                'start_time': self.stats['start_time'].isoformat(),
                'end_time': end_time.isoformat(),
                'duration_minutes': round(duration.total_seconds() / 60, 2),
                'data_range': '2000å¹´1æœˆ1æ—¥ - 2025å¹´8æœˆ31æ—¥',
                'download_strategy': 'å…¨é¢Aè‚¡æ•°æ®ä¸‹è½½ï¼ˆä¸Šå¸‚+é€€å¸‚ï¼‰'
            },
            'statistics': {
                'total_stocks': self.stats['total_stocks'],
                'successful_stocks': self.stats['successful_stocks'],
                'failed_stocks': self.stats['failed_stocks'],
                'success_rate': f"{success_rate:.1f}%",
                'total_api_calls': self.stats['total_api_calls'],
                'total_records': self.stats['total_records'],
                'total_files': self.stats['total_files']
            },
            'data_types': self.data_types,
            'file_structure': {
                'base_path': str(self.base_path),
                'organization': 'æŒ‰æ•°æ®ç±»å‹å’Œè‚¡ç¥¨ç»„ç»‡',
                'naming_convention': 'XXXXXX_XXXX.csv'
            }
        }
        
        # ä¿å­˜æ€»ç»“
        summary_file = self.base_path / 'comprehensive_download_summary.json'
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        print(f"\\nğŸŠ å…¨é¢Aè‚¡æ•°æ®ä¸‹è½½å®Œæˆ!")
        print("=" * 80)
        print(f"ğŸ“Š ä¸‹è½½ç»Ÿè®¡:")
        print(f"   ğŸ“ˆ å¤„ç†è‚¡ç¥¨: {summary['statistics']['total_stocks']}")
        print(f"   âœ… æˆåŠŸè‚¡ç¥¨: {summary['statistics']['successful_stocks']}")
        print(f"   âŒ å¤±è´¥è‚¡ç¥¨: {summary['statistics']['failed_stocks']}")
        print(f"   ğŸ¯ æˆåŠŸç‡: {summary['statistics']['success_rate']}")
        print(f"   ğŸ“ APIè°ƒç”¨: {summary['statistics']['total_api_calls']}")
        print(f"   ğŸ“‹ æ€»è®°å½•: {summary['statistics']['total_records']}")
        print(f"   ğŸ“„ æ€»æ–‡ä»¶: {summary['statistics']['total_files']}")
        print(f"   â±ï¸ ç”¨æ—¶: {summary['comprehensive_download_info']['duration_minutes']} åˆ†é’Ÿ")
        print(f"   ğŸ“ å­˜å‚¨ä½ç½®: {self.base_path}")
        print(f"   ğŸ“„ è¯¦ç»†æŠ¥å‘Š: {summary_file}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸŒŸ å…¨é¢Aè‚¡æ•°æ®ä¸‹è½½å™¨")
    print("=" * 80)
    print("ğŸ¯ ç›®æ ‡: åŸºäºä¸“ä¸šç‰ˆéªŒè¯æˆåŠŸï¼Œä¸‹è½½å…¨éƒ¨Aè‚¡æ•°æ®")
    print("ğŸ“¡ æ•°æ®æº: UQERæé€Ÿç‰ˆ (268ä¸ªAPI)")
    print("ğŸ“… æ—¶é—´: 2000å¹´1æœˆ1æ—¥ - 2025å¹´8æœˆ31æ—¥")
    print("ğŸ—ï¸ ç­–ç•¥: åˆ†æ‰¹ä¸‹è½½ï¼Œå…ˆæµ‹è¯•500åª")
    
    downloader = ComprehensiveAStockDownloader()
    
    # è·å–å…¨éƒ¨Aè‚¡åˆ—è¡¨
    stocks = downloader.get_all_a_stocks()
    
    if not stocks:
        print("âŒ æ— æ³•è·å–è‚¡ç¥¨åˆ—è¡¨")
        return
    
    # å¼€å§‹ä¸‹è½½ï¼ˆå…ˆæµ‹è¯•500åªï¼‰
    downloader.download_comprehensive_data(stocks, batch_size=50, max_stocks=500)

if __name__ == "__main__":
    main()