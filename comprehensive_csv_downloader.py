#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å…¨é¢CSVæ ¼å¼Aè‚¡æ•°æ®è¡¥å…¨å™¨
ä¸“é—¨ç”¨äºè¡¥å…¨è¢«æ¸…ç†çš„æ•°æ®ï¼Œç»Ÿä¸€ä½¿ç”¨CSVæ ¼å¼
"""

import sys
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime, timedelta
import warnings
import json
import time
import os
from io import StringIO
import concurrent.futures
from threading import Lock
warnings.filterwarnings('ignore')

try:
    import uqer
    print("âœ… UQER API å¯ç”¨")
    UQER_AVAILABLE = True
except ImportError:
    print("âŒ UQER API ä¸å¯ç”¨")
    UQER_AVAILABLE = False
    sys.exit(1)

class ComprehensiveCSVDownloader:
    """å…¨é¢CSVæ•°æ®è¡¥å…¨å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–ä¸‹è½½å™¨"""
        self.setup_uqer()
        self.setup_paths()
        self.download_stats = {
            'start_time': datetime.now(),
            'total_stocks': 0,
            'successful_downloads': 0,
            'failed_downloads': 0,
            'skipped_existing': 0,
            'total_records': 0,
            'data_size_mb': 0
        }
        self.progress_lock = Lock()
        
    def setup_uqer(self):
        """è®¾ç½®UQERè¿æ¥"""
        try:
            uqer_token = "68b9922817ae6273137bda7acba81e293582ba347281dfcc056dcb245b23faf3"
            uqer.Client(token=uqer_token)
            print("âœ… UQERè¿æ¥æˆåŠŸ")
            self.uqer_connected = True
        except Exception as e:
            print(f"âŒ UQERè¿æ¥å¤±è´¥: {e}")
            self.uqer_connected = False
            sys.exit(1)
    
    def setup_paths(self):
        """è®¾ç½®å­˜å‚¨è·¯å¾„"""
        # ä¸»æ•°æ®ç›®å½•
        self.base_path = Path("/Users/jackstudio/QuantTrade/data/csv_complete")
        self.base_path.mkdir(exist_ok=True)
        
        # æ—¥çº¿æ•°æ®ç›®å½•
        self.daily_path = self.base_path / "daily"
        self.daily_path.mkdir(exist_ok=True)
        
        # æŒ‰è‚¡ç¥¨ä»£ç ç»„ç»‡ï¼ˆä¾¿äºæŸ¥æ‰¾å’Œä½¿ç”¨ï¼‰
        self.stock_path = self.daily_path / "stocks"
        self.stock_path.mkdir(exist_ok=True)
        
        # å¹´åº¦æ±‡æ€»ç›®å½•ï¼ˆä¾¿äºæŒ‰å¹´ä»½æŸ¥çœ‹ï¼‰
        self.yearly_path = self.daily_path / "yearly"
        self.yearly_path.mkdir(exist_ok=True)
        
        for year in range(2000, 2026):  # åŒ…å«2025å¹´
            year_dir = self.yearly_path / f"year_{year}"
            year_dir.mkdir(exist_ok=True)
        
        print(f"ğŸ“ CSVæ•°æ®å­˜å‚¨è·¯å¾„: {self.base_path}")
        print(f"   ğŸ“Š æŒ‰è‚¡ç¥¨: {self.stock_path}")
        print(f"   ğŸ“… æŒ‰å¹´ä»½: {self.yearly_path}")
    
    def get_all_a_stocks(self):
        """è·å–å…¨éƒ¨Aè‚¡è‚¡ç¥¨åˆ—è¡¨"""
        print("ğŸ“‹ è·å–å…¨éƒ¨Aè‚¡è‚¡ç¥¨åˆ—è¡¨...")
        
        try:
            # è·å–å½“å‰ä¸Šå¸‚è‚¡ç¥¨
            current_result = uqer.DataAPI.EquGet(
                listStatusCD='L',
                pandas=1
            )
            
            # è·å–å·²é€€å¸‚è‚¡ç¥¨  
            delisted_result = uqer.DataAPI.EquGet(
                listStatusCD='DE',
                pandas=1
            )
            
            all_stocks = []
            
            # å¤„ç†å½“å‰ä¸Šå¸‚è‚¡ç¥¨
            if current_result is not None:
                if isinstance(current_result, str):
                    current_df = pd.read_csv(StringIO(current_result))
                else:
                    current_df = current_result
                
                if len(current_df) > 0:
                    all_stocks.append(current_df)
                    print(f"   ğŸ“ˆ å½“å‰ä¸Šå¸‚: {len(current_df)} åª")
            
            # å¤„ç†å·²é€€å¸‚è‚¡ç¥¨
            if delisted_result is not None:
                if isinstance(delisted_result, str):
                    delisted_df = pd.read_csv(StringIO(delisted_result))
                else:
                    delisted_df = delisted_result
                
                if len(delisted_df) > 0:
                    all_stocks.append(delisted_df)
                    print(f"   ğŸ“‰ å·²é€€å¸‚: {len(delisted_df)} åª")
            
            if not all_stocks:
                print("âŒ è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥")
                return []
            
            # åˆå¹¶æ•°æ®
            df = pd.concat(all_stocks, ignore_index=True)
            
            # è¿‡æ»¤Aè‚¡
            a_stocks = df[
                df['secID'].str.contains('.XSHE|.XSHG', na=False)
            ].copy()
            
            # æ’é™¤æŒ‡æ•°
            a_stocks = a_stocks[
                ~a_stocks['secID'].str.contains('.ZICN|.INDX|.XBEI', na=False)
            ]
            
            stock_list = a_stocks['secID'].unique().tolist()
            stock_list.sort()  # æ’åºä¾¿äºç®¡ç†
            
            print(f"âœ… è·å–åˆ° {len(stock_list)} åªAè‚¡")
            self.download_stats['total_stocks'] = len(stock_list)
            
            # ä¿å­˜è‚¡ç¥¨åˆ—è¡¨
            list_file = self.base_path / 'a_stock_list.csv'
            a_stocks[['secID', 'ticker', 'secShortName', 'exchangeCD', 'listStatusCD', 'listDate', 'delistDate']].to_csv(
                list_file, index=False, encoding='utf-8'
            )
            print(f"ğŸ“„ è‚¡ç¥¨åˆ—è¡¨å·²ä¿å­˜: {list_file}")
            
            return stock_list
            
        except Exception as e:
            print(f"âŒ è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")
            return []
    
    def download_stock_data(self, stock_code, start_date='2000-01-01', end_date='2025-08-31'):
        """ä¸‹è½½å•åªè‚¡ç¥¨çš„å®Œæ•´å†å²æ•°æ®"""
        try:
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            stock_file = self.stock_path / f"{stock_code.replace('.', '_')}.csv"
            if stock_file.exists():
                with self.progress_lock:
                    self.download_stats['skipped_existing'] += 1
                return {'status': 'exists', 'file': stock_file}
            
            # è°ƒç”¨UQER API
            result = uqer.DataAPI.MktEqudGet(
                secID=stock_code,
                beginDate=start_date.replace('-', ''),
                endDate=end_date.replace('-', ''),
                pandas=1
            )
            
            if result is None:
                return {'status': 'no_data', 'stock_code': stock_code}
            
            # å¤„ç†APIè¿”å›çš„æ•°æ®
            if isinstance(result, str):
                df = pd.read_csv(StringIO(result))
            elif isinstance(result, pd.DataFrame):
                df = result.copy()
            else:
                return {'status': 'invalid_format', 'stock_code': stock_code}
            
            if len(df) == 0:
                return {'status': 'empty_data', 'stock_code': stock_code}
            
            # æ•°æ®å¤„ç†å’Œæ ‡å‡†åŒ–
            df = self.process_stock_data(df, stock_code)
            
            if df is None or len(df) == 0:
                return {'status': 'processing_failed', 'stock_code': stock_code}
            
            # ä¿å­˜åˆ°æŒ‰è‚¡ç¥¨ç»„ç»‡çš„ç›®å½•
            df.to_csv(stock_file, index=False, encoding='utf-8')
            
            # åŒæ—¶æŒ‰å¹´ä»½ä¿å­˜
            self.save_by_year(df, stock_code)
            
            # æ›´æ–°ç»Ÿè®¡
            with self.progress_lock:
                self.download_stats['successful_downloads'] += 1
                self.download_stats['total_records'] += len(df)
                
                file_size = stock_file.stat().st_size / 1024 / 1024  # MB
                self.download_stats['data_size_mb'] += file_size
            
            return {
                'status': 'success',
                'stock_code': stock_code,
                'records': len(df),
                'file': stock_file,
                'date_range': f"{df['tradeDate'].min()} - {df['tradeDate'].max()}"
            }
            
        except Exception as e:
            with self.progress_lock:
                self.download_stats['failed_downloads'] += 1
            return {'status': 'error', 'stock_code': stock_code, 'error': str(e)}
    
    def process_stock_data(self, df, stock_code):
        """å¤„ç†å’Œæ ‡å‡†åŒ–è‚¡ç¥¨æ•°æ®"""
        try:
            # ç¡®ä¿å¿…è¦çš„åˆ—å­˜åœ¨
            if 'tradeDate' not in df.columns or 'closePrice' not in df.columns:
                return None
            
            # è½¬æ¢æ—¥æœŸ
            df['tradeDate'] = pd.to_datetime(df['tradeDate'])
            
            # æ ‡å‡†åŒ–åˆ—å
            column_mapping = {
                'highestPrice': 'highPrice',
                'lowestPrice': 'lowPrice',
                'turnoverVol': 'volume',
                'turnoverValue': 'amount',
                'chgPct': 'changePct'
            }
            df = df.rename(columns=column_mapping)
            
            # è¿‡æ»¤æœ‰æ•ˆæ•°æ®
            df = df.dropna(subset=['closePrice'])
            df = df[df['closePrice'] > 0]
            df = df.sort_values('tradeDate')
            
            # ç¡®ä¿è‚¡ç¥¨ä»£ç æ­£ç¡®
            df['secID'] = stock_code
            
            # æ·»åŠ è¡ç”Ÿå­—æ®µ
            if 'changePct' not in df.columns and 'preClosePrice' in df.columns:
                df['changePct'] = (df['closePrice'] - df['preClosePrice']) / df['preClosePrice'] * 100
            
            # é‡æ–°æ’åˆ—åˆ—é¡ºåº
            standard_columns = [
                'secID', 'ticker', 'secShortName', 'exchangeCD', 'tradeDate',
                'preClosePrice', 'openPrice', 'highPrice', 'lowPrice', 'closePrice',
                'volume', 'amount', 'changePct'
            ]
            
            # ä¿ç•™å­˜åœ¨çš„åˆ—
            available_columns = [col for col in standard_columns if col in df.columns]
            other_columns = [col for col in df.columns if col not in standard_columns]
            final_columns = available_columns + other_columns
            
            df = df[final_columns]
            
            return df.reset_index(drop=True)
            
        except Exception as e:
            return None
    
    def save_by_year(self, df, stock_code):
        """æŒ‰å¹´ä»½ä¿å­˜æ•°æ®"""
        try:
            df['year'] = df['tradeDate'].dt.year
            
            for year, year_data in df.groupby('year'):
                if 2000 <= year <= 2025:
                    year_file = self.yearly_path / f"year_{year}" / f"{stock_code.replace('.', '_')}.csv"
                    
                    # å¦‚æœå¹´ä»½æ–‡ä»¶å·²å­˜åœ¨ï¼Œè¿½åŠ æ•°æ®
                    if year_file.exists():
                        existing_df = pd.read_csv(year_file)
                        existing_df['tradeDate'] = pd.to_datetime(existing_df['tradeDate'])
                        
                        # åˆå¹¶å¹¶å»é‡
                        combined_df = pd.concat([existing_df, year_data], ignore_index=True)
                        combined_df = combined_df.drop_duplicates(subset=['tradeDate']).sort_values('tradeDate')
                        combined_df.to_csv(year_file, index=False, encoding='utf-8')
                    else:
                        year_data_clean = year_data.drop(columns=['year'])
                        year_data_clean.to_csv(year_file, index=False, encoding='utf-8')
                        
        except Exception as e:
            pass  # å¹´ä»½ä¿å­˜å¤±è´¥ä¸å½±å“ä¸»æµç¨‹
    
    def download_all_stocks(self, batch_size=10, max_workers=5):
        """æ‰¹é‡ä¸‹è½½æ‰€æœ‰è‚¡ç¥¨æ•°æ®"""
        print("ğŸš€ å¼€å§‹å…¨é¢è¡¥å…¨Aè‚¡CSVæ•°æ®")
        print("   ğŸ“Š æ ¼å¼: ç»Ÿä¸€CSVæ ¼å¼")
        print("   ğŸ¯ èŒƒå›´: å…¨éƒ¨Aè‚¡ 2000å¹´1æœˆ1æ—¥-2025å¹´8æœˆ31æ—¥")
        print("   ğŸ”§ æ–¹å¼: å¤šçº¿ç¨‹å¹¶å‘ä¸‹è½½")
        print("=" * 80)
        
        stock_list = self.get_all_a_stocks()
        
        if not stock_list:
            print("âŒ æ— æ³•è·å–è‚¡ç¥¨åˆ—è¡¨")
            return
        
        print(f"ğŸ“Š å‡†å¤‡ä¸‹è½½ {len(stock_list)} åªè‚¡ç¥¨...")
        
        # åˆ†æ‰¹å¤„ç†
        total_batches = (len(stock_list) + batch_size - 1) // batch_size
        
        for batch_idx in range(0, len(stock_list), batch_size):
            batch_stocks = stock_list[batch_idx:batch_idx + batch_size]
            current_batch = batch_idx // batch_size + 1
            
            print(f"\nğŸ“¦ æ‰¹æ¬¡ {current_batch}/{total_batches}: å¤„ç† {len(batch_stocks)} åªè‚¡ç¥¨")
            
            # å¤šçº¿ç¨‹ä¸‹è½½å½“å‰æ‰¹æ¬¡
            with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
                futures = {
                    executor.submit(self.download_stock_data, stock_code): stock_code 
                    for stock_code in batch_stocks
                }
                
                batch_results = []
                for future in concurrent.futures.as_completed(futures):
                    stock_code = futures[future]
                    try:
                        result = future.result()
                        batch_results.append(result)
                        
                        # æ‰“å°è¿›åº¦
                        if result['status'] == 'success':
                            print(f"   âœ… {stock_code}: {result['records']} æ¡è®°å½•")
                        elif result['status'] == 'exists':
                            print(f"   â© {stock_code}: å·²å­˜åœ¨")
                        else:
                            print(f"   âŒ {stock_code}: {result['status']}")
                            
                    except Exception as e:
                        print(f"   ğŸ’¥ {stock_code}: ä¸‹è½½å¼‚å¸¸ {e}")
            
            # æ‰¹æ¬¡å®Œæˆç»Ÿè®¡
            batch_success = sum(1 for r in batch_results if r['status'] == 'success')
            batch_exists = sum(1 for r in batch_results if r['status'] == 'exists')
            batch_failed = len(batch_results) - batch_success - batch_exists
            
            print(f"   ğŸ“Š æ‰¹æ¬¡ç»“æœ: âœ…{batch_success} â©{batch_exists} âŒ{batch_failed}")
            print(f"   ğŸ“ˆ æ€»ä½“è¿›åº¦: {self.download_stats['successful_downloads'] + self.download_stats['skipped_existing']}/{len(stock_list)}")
            
            # æ‰¹æ¬¡é—´ä¼‘æ¯
            if current_batch < total_batches:
                time.sleep(2)
        
        # åˆ›å»ºä¸‹è½½æ€»ç»“
        self.create_download_summary()
    
    def create_download_summary(self):
        """åˆ›å»ºä¸‹è½½æ€»ç»“"""
        end_time = datetime.now()
        duration = end_time - self.download_stats['start_time']
        
        summary = {
            'download_info': {
                'start_time': self.download_stats['start_time'].isoformat(),
                'end_time': end_time.isoformat(),
                'duration_hours': round(duration.total_seconds() / 3600, 2),
                'data_format': 'CSV',
                'data_range': '2000å¹´1æœˆ1æ—¥-2025å¹´8æœˆ31æ—¥',
                'organization': 'stocks/ å’Œ yearly/ åŒé‡ç»„ç»‡'
            },
            'statistics': {
                'total_stocks': self.download_stats['total_stocks'],
                'successful_downloads': self.download_stats['successful_downloads'],
                'skipped_existing': self.download_stats['skipped_existing'],
                'failed_downloads': self.download_stats['failed_downloads'],
                'total_completed': self.download_stats['successful_downloads'] + self.download_stats['skipped_existing'],
                'completion_rate': f"{(self.download_stats['successful_downloads'] + self.download_stats['skipped_existing'])/self.download_stats['total_stocks']*100:.1f}%",
                'total_records': self.download_stats['total_records'],
                'total_size_mb': round(self.download_stats['data_size_mb'], 2),
                'total_size_gb': round(self.download_stats['data_size_mb'] / 1024, 2)
            },
            'file_structure': {
                'base_path': str(self.base_path),
                'stocks_directory': str(self.stock_path),
                'yearly_directory': str(self.yearly_path),
                'stock_list_file': str(self.base_path / 'a_stock_list.csv'),
                'file_naming': 'XXXXXX_XXXX.csv (å¦‚ 000001_XSHE.csv)'
            }
        }
        
        # ä¿å­˜æ€»ç»“
        summary_file = self.base_path / 'download_summary.json'
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸŠ Aè‚¡CSVæ•°æ®è¡¥å…¨å®Œæˆ!")
        print(f"=" * 80)
        print(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
        print(f"   ğŸ¯ ç›®æ ‡è‚¡ç¥¨: {summary['statistics']['total_stocks']}")
        print(f"   âœ… æˆåŠŸä¸‹è½½: {summary['statistics']['successful_downloads']}")
        print(f"   â© å·²å­˜åœ¨: {summary['statistics']['skipped_existing']}")
        print(f"   âŒ å¤±è´¥æ•°é‡: {summary['statistics']['failed_downloads']}")
        print(f"   ğŸ“ˆ å®Œæˆç‡: {summary['statistics']['completion_rate']}")
        print(f"   ğŸ“ æ€»è®°å½•æ•°: {summary['statistics']['total_records']:,}")
        print(f"   ğŸ’¾ æ•°æ®å¤§å°: {summary['statistics']['total_size_gb']} GB")
        print(f"   â±ï¸ ç”¨æ—¶: {summary['download_info']['duration_hours']} å°æ—¶")
        print(f"   ğŸ“ å­˜å‚¨ä½ç½®: {self.base_path}")
        print(f"   ğŸ“„ è¯¦ç»†æŠ¥å‘Š: {summary_file}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”„ å…¨é¢Aè‚¡CSVæ•°æ®è¡¥å…¨å™¨")
    print("=" * 80)
    print("ğŸ¯ ç›®æ ‡: è¡¥å…¨è¢«æ¸…ç†çš„æ•°æ®ï¼Œç»Ÿä¸€CSVæ ¼å¼")
    print("ğŸ“… æ—¶é—´: 2000å¹´1æœˆ1æ—¥ - 2025å¹´8æœˆ31æ—¥")
    print("ğŸ“¡ æ•°æ®æº: UQER MktEqudGet API")
    print("ğŸ”§ ç‰¹ç‚¹: å¤šçº¿ç¨‹å¹¶å‘ + åŒé‡æ–‡ä»¶ç»„ç»‡")
    
    downloader = ComprehensiveCSVDownloader()
    downloader.download_all_stocks(batch_size=20, max_workers=8)

if __name__ == "__main__":
    main()