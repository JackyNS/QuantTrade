#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å…¨é¢æ‹†è§£æœ¬åœ°æ•°æ®ç»“æ„åˆ†æå™¨
============================

ç›®æ ‡ï¼š
1. å…¨é¢æ‰«ææ‰€æœ‰æ•°æ®ç›®å½•å’Œæ–‡ä»¶
2. æŒ‰APIæ¥æºåˆ†ç±»ç»Ÿè®¡æ•°æ®
3. åˆ†ææ•°æ®å®Œæ•´æ€§å’Œæ—¶é—´è¦†ç›–
4. è¯†åˆ«é‡å¤ã€ç¼ºå¤±å’Œä¸ä¸€è‡´çš„æ•°æ®
5. ä¸ºæ•°æ®é‡ç»„æä¾›åŸºç¡€ä¿¡æ¯

"""

import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
import json
import warnings
from collections import defaultdict, Counter
import os
import re

warnings.filterwarnings('ignore')

class ComprehensiveDataDissection:
    """å…¨é¢æ•°æ®æ‹†è§£åˆ†æå™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–"""
        self.base_path = Path("/Users/jackstudio/QuantTrade/data")
        self.analysis_results = {}
        
    def scan_all_data_directories(self):
        """æ‰«ææ‰€æœ‰æ•°æ®ç›®å½•"""
        print("ğŸ” å…¨é¢æ‰«ææœ¬åœ°æ•°æ®ç›®å½•ç»“æ„...")
        print("=" * 80)
        
        directory_structure = {}
        
        # é€’å½’æ‰«ææ‰€æœ‰ç›®å½•
        for root, dirs, files in os.walk(self.base_path):
            rel_path = Path(root).relative_to(self.base_path)
            
            # ç»Ÿè®¡æ–‡ä»¶ä¿¡æ¯
            csv_files = [f for f in files if f.endswith('.csv')]
            json_files = [f for f in files if f.endswith('.json')]
            other_files = [f for f in files if not f.endswith(('.csv', '.json'))]
            
            if csv_files or json_files or other_files:
                directory_structure[str(rel_path)] = {
                    'csv_count': len(csv_files),
                    'json_count': len(json_files),
                    'other_count': len(other_files),
                    'total_files': len(files),
                    'csv_files': csv_files[:10],  # åªæ˜¾ç¤ºå‰10ä¸ªæ–‡ä»¶å
                    'directory_size_mb': self._calculate_directory_size(Path(root))
                }
        
        self.analysis_results['directory_structure'] = directory_structure
        return directory_structure
    
    def _calculate_directory_size(self, directory):
        """è®¡ç®—ç›®å½•å¤§å°ï¼ˆMBï¼‰"""
        total_size = 0
        try:
            for file_path in directory.rglob('*'):
                if file_path.is_file():
                    total_size += file_path.stat().st_size
        except:
            pass
        return round(total_size / (1024 * 1024), 2)
    
    def analyze_batch_files(self):
        """åˆ†ææ‰¹æ¬¡æ–‡ä»¶ï¼ˆpriority_downloadï¼‰"""
        print("\nğŸ“„ åˆ†ææ‰¹æ¬¡æ–‡ä»¶...")
        print("-" * 60)
        
        batch_path = self.base_path / "priority_download/market_data/daily"
        batch_analysis = {}
        
        if not batch_path.exists():
            print("âŒ æ‰¹æ¬¡æ–‡ä»¶ç›®å½•ä¸å­˜åœ¨")
            return {}
        
        batch_files = list(batch_path.glob("*.csv"))
        print(f"ğŸ“Š æ‰¹æ¬¡æ–‡ä»¶æ€»æ•°: {len(batch_files)}")
        
        # æŒ‰å¹´ä»½åˆ†ç»„åˆ†æ
        year_groups = defaultdict(list)
        for file_path in batch_files:
            try:
                year = file_path.stem.split('_')[0]
                year_groups[year].append(file_path)
            except:
                continue
        
        print(f"ğŸ“… è¦†ç›–å¹´ä»½: {min(year_groups.keys())} - {max(year_groups.keys())}")
        
        # è¯¦ç»†åˆ†ææ¯å¹´çš„æ•°æ®
        year_details = {}
        for year in sorted(year_groups.keys()):
            files = year_groups[year]
            print(f"\nğŸ“‹ {year}å¹´æ‰¹æ¬¡æ–‡ä»¶: {len(files)} ä¸ª")
            
            # æŠ½æ ·åˆ†ææ–‡ä»¶å†…å®¹
            sample_stats = []
            for sample_file in files[:3]:  # åˆ†æå‰3ä¸ªæ–‡ä»¶
                try:
                    df = pd.read_csv(sample_file)
                    if 'tradeDate' in df.columns and 'secID' in df.columns:
                        df['tradeDate'] = pd.to_datetime(df['tradeDate'])
                        stats = {
                            'file': sample_file.name,
                            'stocks': len(df['secID'].unique()),
                            'records': len(df),
                            'date_range': f"{df['tradeDate'].min().date()} - {df['tradeDate'].max().date()}",
                            'columns': list(df.columns)
                        }
                        sample_stats.append(stats)
                        print(f"   ğŸ“ˆ {sample_file.name}: {stats['stocks']} åªè‚¡ç¥¨, {stats['records']} æ¡è®°å½•")
                except Exception as e:
                    print(f"   âŒ {sample_file.name}: è¯»å–å¤±è´¥")
            
            year_details[year] = {
                'file_count': len(files),
                'sample_stats': sample_stats,
                'total_size_mb': sum(self._calculate_directory_size(f.parent) for f in files[:1])
            }
        
        batch_analysis = {
            'total_files': len(batch_files),
            'year_range': f"{min(year_groups.keys())} - {max(year_groups.keys())}",
            'years_covered': len(year_groups),
            'year_details': year_details
        }
        
        self.analysis_results['batch_files'] = batch_analysis
        return batch_analysis
    
    def analyze_individual_stock_files(self):
        """åˆ†æä¸ªè‚¡æ–‡ä»¶ï¼ˆcsv_completeï¼‰"""
        print("\nğŸ“Š åˆ†æä¸ªè‚¡æ–‡ä»¶...")
        print("-" * 60)
        
        individual_analysis = {}
        
        # 1. åˆ†æä¸»è¦ä¸ªè‚¡æ–‡ä»¶ç›®å½•
        stock_dirs = {
            'daily_stocks': self.base_path / "csv_complete/daily/stocks",
            'weekly_stocks': self.base_path / "csv_complete/weekly/stocks", 
            'monthly_stocks': self.base_path / "csv_complete/monthly/stocks"
        }
        
        for dir_name, dir_path in stock_dirs.items():
            if not dir_path.exists():
                print(f"âŒ {dir_name} ç›®å½•ä¸å­˜åœ¨")
                continue
                
            stock_files = list(dir_path.glob("*.csv"))
            print(f"ğŸ“ˆ {dir_name}: {len(stock_files)} ä¸ªæ–‡ä»¶")
            
            # æŠ½æ ·åˆ†æ
            sample_files = stock_files[:20]  # åˆ†æå‰20ä¸ªæ–‡ä»¶
            sample_analysis = []
            
            for file_path in sample_files:
                try:
                    df = pd.read_csv(file_path)
                    stock_id = file_path.stem.replace('_', '.')
                    
                    if 'tradeDate' in df.columns and len(df) > 0:
                        df['tradeDate'] = pd.to_datetime(df['tradeDate'])
                        analysis = {
                            'stock_id': stock_id,
                            'records': len(df),
                            'date_range': f"{df['tradeDate'].min().date()} - {df['tradeDate'].max().date()}",
                            'years': round((df['tradeDate'].max() - df['tradeDate'].min()).days / 365.25, 1)
                        }
                        sample_analysis.append(analysis)
                except Exception as e:
                    continue
            
            individual_analysis[dir_name] = {
                'total_files': len(stock_files),
                'sample_analysis': sample_analysis[:10]  # åªä¿å­˜å‰10ä¸ªæ ·æœ¬
            }
        
        # 2. åˆ†ææŒ‰å¹´åˆ†ç±»çš„æ–‡ä»¶
        yearly_path = self.base_path / "csv_complete/daily/yearly"
        if yearly_path.exists():
            year_dirs = [d for d in yearly_path.iterdir() if d.is_dir() and d.name.startswith('year_')]
            year_analysis = {}
            
            print(f"\nğŸ“… æŒ‰å¹´åˆ†ç±»ç›®å½•: {len(year_dirs)} ä¸ªå¹´ä»½")
            
            for year_dir in sorted(year_dirs):
                year = year_dir.name.replace('year_', '')
                year_files = list(year_dir.glob("*.csv"))
                
                year_analysis[year] = {
                    'file_count': len(year_files),
                    'size_mb': self._calculate_directory_size(year_dir)
                }
                
                print(f"   ğŸ“‹ {year}: {len(year_files)} ä¸ªæ–‡ä»¶")
            
            individual_analysis['yearly_classification'] = year_analysis
        
        self.analysis_results['individual_files'] = individual_analysis
        return individual_analysis
    
    def analyze_data_consistency(self):
        """åˆ†ææ•°æ®ä¸€è‡´æ€§"""
        print("\nğŸ” åˆ†ææ•°æ®ä¸€è‡´æ€§...")
        print("-" * 60)
        
        consistency_issues = []
        
        # æ£€æŸ¥åŒä¸€è‚¡ç¥¨åœ¨ä¸åŒä½ç½®çš„æ•°æ®æ˜¯å¦ä¸€è‡´
        test_stocks = ['000001_XSHE', '000002_XSHE', '600000_XSHG']
        
        for stock in test_stocks:
            stock_data_locations = {}
            
            # 1. æ£€æŸ¥ä¸»è¦ä¸ªè‚¡æ–‡ä»¶
            main_file = self.base_path / f"csv_complete/daily/stocks/{stock}.csv"
            if main_file.exists():
                try:
                    df = pd.read_csv(main_file)
                    stock_data_locations['main_file'] = {
                        'records': len(df),
                        'date_range': f"{df['tradeDate'].min()} - {df['tradeDate'].max()}" if 'tradeDate' in df.columns else 'No dates'
                    }
                except:
                    stock_data_locations['main_file'] = {'error': 'Failed to read'}
            
            # 2. æ£€æŸ¥å¹´åº¦åˆ†ç±»æ–‡ä»¶ï¼ˆ2024å¹´ï¼‰
            year_file = self.base_path / f"csv_complete/daily/yearly/year_2024/{stock}.csv"
            if year_file.exists():
                try:
                    df = pd.read_csv(year_file)
                    stock_data_locations['year_2024'] = {
                        'records': len(df),
                        'date_range': f"{df['tradeDate'].min()} - {df['tradeDate'].max()}" if 'tradeDate' in df.columns else 'No dates'
                    }
                except:
                    stock_data_locations['year_2024'] = {'error': 'Failed to read'}
            
            if len(stock_data_locations) > 1:
                consistency_issues.append({
                    'stock': stock,
                    'locations': stock_data_locations
                })
        
        print(f"ğŸ” ä¸€è‡´æ€§æ£€æŸ¥æ ·æœ¬: {len(test_stocks)} åªè‚¡ç¥¨")
        for issue in consistency_issues:
            print(f"   ğŸ“Š {issue['stock']}:")
            for location, data in issue['locations'].items():
                if 'error' not in data:
                    print(f"      {location}: {data['records']} æ¡è®°å½•, {data['date_range']}")
                else:
                    print(f"      {location}: {data['error']}")
        
        self.analysis_results['consistency_issues'] = consistency_issues
        return consistency_issues
    
    def identify_data_gaps(self):
        """è¯†åˆ«æ•°æ®ç¼ºå£"""
        print("\nâš ï¸ è¯†åˆ«æ•°æ®ç¼ºå£...")
        print("-" * 60)
        
        gaps_analysis = {}
        
        # 1. æ£€æŸ¥æ‰¹æ¬¡æ–‡ä»¶çš„æ—¶é—´è¿ç»­æ€§
        batch_path = self.base_path / "priority_download/market_data/daily"
        if batch_path.exists():
            batch_files = sorted(batch_path.glob("*.csv"))
            
            # åˆ†ææ‰¹æ¬¡æ–‡ä»¶çš„å¹´ä»½è¦†ç›–
            batch_years = set()
            for file_path in batch_files:
                try:
                    year = file_path.stem.split('_')[0]
                    batch_years.add(int(year))
                except:
                    continue
            
            if batch_years:
                year_range = list(range(min(batch_years), max(batch_years) + 1))
                missing_years = set(year_range) - batch_years
                
                gaps_analysis['batch_files'] = {
                    'covered_years': sorted(batch_years),
                    'missing_years': sorted(missing_years),
                    'year_range': f"{min(batch_years)}-{max(batch_years)}"
                }
                
                print(f"ğŸ“… æ‰¹æ¬¡æ–‡ä»¶å¹´ä»½è¦†ç›–: {min(batch_years)}-{max(batch_years)}")
                if missing_years:
                    print(f"âš ï¸ ç¼ºå¤±å¹´ä»½: {sorted(missing_years)}")
                else:
                    print("âœ… å¹´ä»½è¦†ç›–è¿ç»­")
        
        # 2. æ£€æŸ¥ä¸ªè‚¡æ–‡ä»¶ä¸æ‰¹æ¬¡æ–‡ä»¶çš„å·®å¼‚
        individual_path = self.base_path / "csv_complete/daily/stocks"
        if individual_path.exists() and batch_path.exists():
            # ä»æ‰¹æ¬¡æ–‡ä»¶ä¸­ç»Ÿè®¡æ€»è‚¡ç¥¨æ•°
            all_stocks_in_batch = set()
            for file_path in batch_files[:10]:  # æ£€æŸ¥å‰10ä¸ªæ‰¹æ¬¡æ–‡ä»¶
                try:
                    df = pd.read_csv(file_path)
                    if 'secID' in df.columns:
                        all_stocks_in_batch.update(df['secID'].unique())
                except:
                    continue
            
            # ç»Ÿè®¡ä¸ªè‚¡æ–‡ä»¶æ•°
            individual_files = list(individual_path.glob("*.csv"))
            individual_stocks = {f.stem.replace('_', '.') for f in individual_files}
            
            gaps_analysis['stock_coverage'] = {
                'batch_stocks_sample': len(all_stocks_in_batch),
                'individual_files': len(individual_files),
                'missing_individual_files': len(all_stocks_in_batch - individual_stocks) if all_stocks_in_batch else 0
            }
            
            print(f"ğŸ“Š æ‰¹æ¬¡æ–‡ä»¶è‚¡ç¥¨æ•°ï¼ˆæŠ½æ ·ï¼‰: {len(all_stocks_in_batch)}")
            print(f"ğŸ“ˆ ä¸ªè‚¡æ–‡ä»¶æ•°: {len(individual_files)}")
            if all_stocks_in_batch:
                missing_count = len(all_stocks_in_batch - individual_stocks)
                print(f"âš ï¸ å¯èƒ½ç¼ºå¤±çš„ä¸ªè‚¡æ–‡ä»¶: {missing_count}")
        
        self.analysis_results['data_gaps'] = gaps_analysis
        return gaps_analysis
    
    def generate_comprehensive_report(self):
        """ç”Ÿæˆå…¨é¢çš„æ•°æ®æ‹†è§£æŠ¥å‘Š"""
        print("\nğŸŠ ç”Ÿæˆå…¨é¢æ•°æ®æ‹†è§£æŠ¥å‘Š")
        print("=" * 80)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # 1. æ‰«æç›®å½•ç»“æ„
        directory_structure = self.scan_all_data_directories()
        
        # 2. åˆ†ææ‰¹æ¬¡æ–‡ä»¶
        batch_analysis = self.analyze_batch_files()
        
        # 3. åˆ†æä¸ªè‚¡æ–‡ä»¶
        individual_analysis = self.analyze_individual_stock_files()
        
        # 4. åˆ†ææ•°æ®ä¸€è‡´æ€§
        consistency_analysis = self.analyze_data_consistency()
        
        # 5. è¯†åˆ«æ•°æ®ç¼ºå£
        gaps_analysis = self.identify_data_gaps()
        
        # 6. ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        summary_report = {
            'analysis_time': datetime.now().isoformat(),
            'total_directories': len(directory_structure),
            'total_data_size_gb': sum(d['directory_size_mb'] for d in directory_structure.values()) / 1024,
            'key_findings': self._generate_key_findings(),
            'data_organization_status': self._assess_data_organization(),
            'recommendations': self._generate_recommendations()
        }
        
        # ä¿å­˜å®Œæ•´æŠ¥å‘Š
        complete_report = {
            'summary': summary_report,
            'directory_structure': directory_structure,
            'batch_files_analysis': batch_analysis,
            'individual_files_analysis': individual_analysis,
            'consistency_analysis': consistency_analysis,
            'data_gaps_analysis': gaps_analysis
        }
        
        # ä¿å­˜æŠ¥å‘Šæ–‡ä»¶
        report_file = Path(f"data_dissection_report_{timestamp}.json")
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(complete_report, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"\nğŸ“„ å®Œæ•´æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        
        # æ˜¾ç¤ºå…³é”®å‘ç°
        self._display_key_findings(summary_report)
        
        return complete_report
    
    def _generate_key_findings(self):
        """ç”Ÿæˆå…³é”®å‘ç°"""
        findings = []
        
        # åŸºäºåˆ†æç»“æœç”Ÿæˆå‘ç°
        if 'batch_files' in self.analysis_results:
            batch_info = self.analysis_results['batch_files']
            findings.append(f"æ‰¹æ¬¡æ–‡ä»¶è¦†ç›– {batch_info['years_covered']} ä¸ªå¹´ä»½")
        
        if 'individual_files' in self.analysis_results:
            individual_info = self.analysis_results['individual_files']
            if 'daily_stocks' in individual_info:
                findings.append(f"ä¸ªè‚¡æ—¥çº¿æ–‡ä»¶ {individual_info['daily_stocks']['total_files']} ä¸ª")
        
        if 'consistency_issues' in self.analysis_results:
            consistency_info = self.analysis_results['consistency_issues']
            findings.append(f"å‘ç° {len(consistency_info)} ä¸ªä¸€è‡´æ€§æ£€æŸ¥æ ·æœ¬")
        
        return findings
    
    def _assess_data_organization(self):
        """è¯„ä¼°æ•°æ®ç»„ç»‡çŠ¶å†µ"""
        return {
            'batch_files_status': 'Complete' if 'batch_files' in self.analysis_results else 'Missing',
            'individual_files_status': 'Partial' if 'individual_files' in self.analysis_results else 'Missing',
            'yearly_classification_status': 'Exists' if 'yearly_classification' in self.analysis_results.get('individual_files', {}) else 'Missing'
        }
    
    def _generate_recommendations(self):
        """ç”Ÿæˆå»ºè®®"""
        recommendations = [
            "1. ç»Ÿä¸€æ•°æ®å­˜å‚¨ç»“æ„ï¼Œé¿å…é‡å¤å­˜å‚¨",
            "2. å»ºç«‹æ•°æ®åŒæ­¥æœºåˆ¶ï¼Œç¡®ä¿æ‰¹æ¬¡æ–‡ä»¶ä¸ä¸ªè‚¡æ–‡ä»¶ä¸€è‡´",
            "3. å®æ–½æ•°æ®å®Œæ•´æ€§æ£€æŸ¥ï¼Œå®šæœŸéªŒè¯æ•°æ®è´¨é‡",
            "4. ä¼˜åŒ–å­˜å‚¨ç©ºé—´ï¼Œåˆ é™¤å†—ä½™å’Œè¿‡æ—¶æ•°æ®"
        ]
        return recommendations
    
    def _display_key_findings(self, summary_report):
        """æ˜¾ç¤ºå…³é”®å‘ç°"""
        print("\nğŸ’¡ å…³é”®å‘ç°:")
        for finding in summary_report['key_findings']:
            print(f"   âœ“ {finding}")
        
        print(f"\nğŸ“Š æ•°æ®æ€»é‡: {summary_report['total_data_size_gb']:.2f} GB")
        print(f"ğŸ“ ç›®å½•æ•°é‡: {summary_report['total_directories']}")
        
        print("\nğŸ¯ æ•°æ®ç»„ç»‡çŠ¶å†µ:")
        for status_key, status_value in summary_report['data_organization_status'].items():
            print(f"   â€¢ {status_key}: {status_value}")
        
        print("\nğŸ’¡ å»ºè®®:")
        for rec in summary_report['recommendations']:
            print(f"   {rec}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” å…¨é¢æ‹†è§£æœ¬åœ°æ•°æ®ç»“æ„")
    print("ğŸ¯ ç›®æ ‡ï¼šä¸ºæ•°æ®é‡ç»„æä¾›è¯¦ç»†åŸºç¡€ä¿¡æ¯")
    print("=" * 80)
    
    dissector = ComprehensiveDataDissection()
    report = dissector.generate_comprehensive_report()
    
    print("\nâœ… æ•°æ®æ‹†è§£åˆ†æå®Œæˆï¼")
    print("æ¥ä¸‹æ¥å¯ä»¥åŸºäºæ­¤æŠ¥å‘Šè¿›è¡Œæ•°æ®é‡ç»„å·¥ä½œã€‚")

if __name__ == "__main__":
    main()