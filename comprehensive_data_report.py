#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»¼åˆæ•°æ®è´¨é‡æŠ¥å‘Šç”Ÿæˆå™¨ - ç”Ÿæˆå®Œæ•´çš„æ•°æ®ç°çŠ¶æŠ¥å‘Š
"""

import pandas as pd
import numpy as np
from pathlib import Path
import logging
from datetime import datetime
import json

class ComprehensiveDataReporter:
    """ç»¼åˆæ•°æ®è´¨é‡æŠ¥å‘Šç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.base_dir = Path("data/final_comprehensive_download")
        self.setup_logging()
        
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        log_file = Path("comprehensive_report.log")
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
    
    def load_existing_reports(self):
        """åŠ è½½å·²æœ‰çš„æŠ¥å‘Šæ•°æ®"""
        reports = {}
        
        # åŠ è½½APIæ¸…å•
        if Path('data_api_inventory.csv').exists():
            reports['inventory'] = pd.read_csv('data_api_inventory.csv')
        
        # åŠ è½½è´¨é‡æ¦‚è§ˆ
        if Path('data_quality_overview.csv').exists():
            reports['quality'] = pd.read_csv('data_quality_overview.csv')
            
        # åŠ è½½éªŒè¯æŠ¥å‘Š
        if Path('quick_validation_report.csv').exists():
            reports['validation'] = pd.read_csv('quick_validation_report.csv')
        
        return reports
    
    def analyze_data_coverage(self):
        """åˆ†ææ•°æ®è¦†ç›–èŒƒå›´"""
        coverage_analysis = {
            'time_coverage': {},
            'stock_coverage': {},
            'data_completeness': {}
        }
        
        # åˆ†ææ—¶é—´è¦†ç›–
        for category_dir in self.base_dir.iterdir():
            if category_dir.is_dir():
                category_name = category_dir.name
                category_files = []
                
                for api_dir in category_dir.iterdir():
                    if api_dir.is_dir():
                        csv_files = list(api_dir.glob("*.csv"))
                        for csv_file in csv_files[:3]:  # é‡‡æ ·åˆ†æ
                            try:
                                df_sample = pd.read_csv(csv_file, nrows=100)
                                
                                # å¯»æ‰¾æ—¥æœŸåˆ—
                                date_columns = []
                                for col in df_sample.columns:
                                    if any(date_word in col.lower() for date_word in ['date', 'time', 'year']):
                                        date_columns.append(col)
                                
                                if date_columns:
                                    category_files.append({
                                        'api': api_dir.name,
                                        'file': csv_file.name,
                                        'date_columns': date_columns,
                                        'row_count': len(df_sample)
                                    })
                            except:
                                continue
                
                coverage_analysis['time_coverage'][category_name] = category_files
        
        return coverage_analysis
    
    def calculate_api_completeness(self):
        """è®¡ç®—APIå®Œæ•´æ€§"""
        
        # å®šä¹‰æœŸæœ›çš„APIåˆ†ç±»å’Œæ•°é‡ï¼ˆåŸºäºä¸šåŠ¡éœ€æ±‚ï¼‰
        expected_apis = {
            'basic_info': {
                'expected': 10,  # åŸºç¡€ä¿¡æ¯ç±»API
                'critical': ['secidget', 'equget', 'equindustryget']
            },
            'financial': {
                'expected': 15,  # è´¢åŠ¡æ•°æ®API  
                'critical': ['fdmtisalllatestget', 'fdmtbsalllatestget', 'fdmtcfalllatestget']
            },
            'special_trading': {
                'expected': 20,  # ç‰¹æ®Šäº¤æ˜“æ•°æ®API
                'critical': ['mktlimitget', 'fstdetailget', 'mktblockdget']
            },
            'governance': {
                'expected': 25,  # å…¬å¸æ²»ç†API
                'critical': ['equshtenget', 'equfloatshtenget']
            },
            'additional_apis': {
                'expected': 10,  # é¢å¤–API
                'critical': ['equ_fancy_factors_lite', 'fst_total']
            }
        }
        
        completeness_report = {}
        
        for category_dir in self.base_dir.iterdir():
            if category_dir.is_dir():
                category_name = category_dir.name
                
                if category_name in expected_apis:
                    # ç»Ÿè®¡å®é™…APIæ•°é‡
                    actual_apis = []
                    for api_dir in category_dir.iterdir():
                        if api_dir.is_dir():
                            csv_files = list(api_dir.glob("*.csv"))
                            if csv_files:  # åªè®¡ç®—æœ‰æ•°æ®çš„API
                                actual_apis.append(api_dir.name)
                    
                    expected_count = expected_apis[category_name]['expected']
                    actual_count = len(actual_apis)
                    critical_apis = expected_apis[category_name]['critical']
                    
                    # æ£€æŸ¥å…³é”®APIçš„å¯ç”¨æ€§
                    critical_available = []
                    for critical_api in critical_apis:
                        if any(critical_api in api for api in actual_apis):
                            critical_available.append(critical_api)
                    
                    completeness_report[category_name] = {
                        'actual_count': actual_count,
                        'expected_count': expected_count,
                        'completeness_pct': min((actual_count / expected_count) * 100, 100),
                        'critical_apis_available': len(critical_available),
                        'critical_apis_total': len(critical_apis),
                        'critical_completeness_pct': (len(critical_available) / len(critical_apis)) * 100,
                        'api_list': actual_apis[:10]  # åªæ˜¾ç¤ºå‰10ä¸ª
                    }
        
        return completeness_report
    
    def generate_executive_summary(self, reports, coverage, completeness):
        """ç”Ÿæˆæ‰§è¡Œæ‘˜è¦"""
        
        total_files = sum(cat['total_files'] for cat in reports.get('quality', pd.DataFrame()).to_dict('records'))
        total_size = sum(cat['size_gb'] for cat in reports.get('quality', pd.DataFrame()).to_dict('records'))
        total_apis = sum(cat['api_count'] for cat in reports.get('quality', pd.DataFrame()).to_dict('records'))
        
        avg_quality = np.mean([cat['quality_score'] for cat in reports.get('quality', pd.DataFrame()).to_dict('records')])
        
        summary = {
            'report_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'overall_stats': {
                'total_api_interfaces': total_apis,
                'total_data_files': total_files, 
                'total_data_size_gb': total_size,
                'average_data_quality': avg_quality,
                'data_categories': len(reports.get('quality', pd.DataFrame())),
            },
            'quality_assessment': {
                'excellent': len([cat for cat in reports.get('quality', pd.DataFrame()).to_dict('records') if cat.get('quality_score', 0) >= 95]),
                'good': len([cat for cat in reports.get('quality', pd.DataFrame()).to_dict('records') if 85 <= cat.get('quality_score', 0) < 95]),
                'fair': len([cat for cat in reports.get('quality', pd.DataFrame()).to_dict('records') if 70 <= cat.get('quality_score', 0) < 85]),
                'poor': len([cat for cat in reports.get('quality', pd.DataFrame()).to_dict('records') if cat.get('quality_score', 0) < 70])
            },
            'completeness_summary': {
                category: data['completeness_pct'] 
                for category, data in completeness.items()
            },
            'top_data_sources': []
        }
        
        # æ‰¾å‡ºæœ€å¤§çš„æ•°æ®æº
        if 'quality' in reports:
            top_sources = reports['quality'].nlargest(5, 'size_gb').to_dict('records')
            summary['top_data_sources'] = [
                {'category': src['category'], 'size_gb': src['size_gb']} 
                for src in top_sources
            ]
        
        return summary
    
    def generate_comprehensive_report(self):
        """ç”Ÿæˆç»¼åˆæŠ¥å‘Š"""
        logging.info("ğŸ“Š ç”Ÿæˆç»¼åˆæ•°æ®è´¨é‡æŠ¥å‘Š...")
        
        # åŠ è½½å·²æœ‰æŠ¥å‘Š
        reports = self.load_existing_reports()
        
        # åˆ†ææ•°æ®è¦†ç›–
        coverage = self.analyze_data_coverage()
        
        # è®¡ç®—å®Œæ•´æ€§
        completeness = self.calculate_api_completeness()
        
        # ç”Ÿæˆæ‰§è¡Œæ‘˜è¦
        summary = self.generate_executive_summary(reports, coverage, completeness)
        
        # è¾“å‡ºæ§åˆ¶å°æŠ¥å‘Š
        self.print_comprehensive_report(summary, completeness, reports)
        
        # ä¿å­˜JSONæŠ¥å‘Š
        with open('comprehensive_data_report.json', 'w', encoding='utf-8') as f:
            json.dump({
                'executive_summary': summary,
                'api_completeness': completeness,
                'data_coverage': coverage
            }, f, ensure_ascii=False, indent=2)
        
        # ç”ŸæˆExcelæŠ¥å‘Š
        self.create_excel_report(reports, summary, completeness)
        
        return summary, completeness, coverage
    
    def print_comprehensive_report(self, summary, completeness, reports):
        """æ‰“å°ç»¼åˆæŠ¥å‘Š"""
        print("\n" + "="*100)
        print("ğŸ¯ **QuantTrade æ•°æ®èµ„äº§ç»¼åˆè´¨é‡æŠ¥å‘Š**")
        print("="*100)
        print(f"ğŸ“… æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {summary['report_date']}")
        
        # æ‰§è¡Œæ‘˜è¦
        stats = summary['overall_stats']
        print(f"\nğŸ“Š **æ•°æ®èµ„äº§æ¦‚è§ˆ**:")
        print(f"  ğŸ”Œ APIæ¥å£æ€»æ•°: {stats['total_api_interfaces']} ä¸ª")
        print(f"  ğŸ“„ æ•°æ®æ–‡ä»¶æ€»æ•°: {stats['total_data_files']:,} ä¸ª")
        print(f"  ğŸ’¾ æ•°æ®æ€»å®¹é‡: {stats['total_data_size_gb']:.1f} GB")
        print(f"  ğŸ“ æ•°æ®åˆ†ç±»: {stats['data_categories']} ä¸ª")
        print(f"  ğŸ¯ å¹³å‡æ•°æ®è´¨é‡: {stats['average_data_quality']:.1f}%")
        
        # è´¨é‡è¯„ä¼°
        quality = summary['quality_assessment']
        print(f"\nğŸ† **æ•°æ®è´¨é‡åˆ†å¸ƒ**:")
        print(f"  ğŸŸ¢ ä¼˜ç§€ (â‰¥95%): {quality['excellent']} ä¸ªåˆ†ç±»")
        print(f"  ğŸŸ¡ è‰¯å¥½ (85-95%): {quality['good']} ä¸ªåˆ†ç±»")
        print(f"  ğŸŸ  ä¸€èˆ¬ (70-85%): {quality['fair']} ä¸ªåˆ†ç±»") 
        print(f"  ğŸ”´ è¾ƒå·® (<70%): {quality['poor']} ä¸ªåˆ†ç±»")
        
        # APIå®Œæ•´æ€§
        print(f"\nğŸ“‹ **APIå®Œæ•´æ€§åˆ†æ**:")
        for category, comp_data in completeness.items():
            status = "ğŸŸ¢" if comp_data['completeness_pct'] >= 90 else "ğŸŸ¡" if comp_data['completeness_pct'] >= 70 else "ğŸ”´"
            print(f"  {status} {category}: {comp_data['actual_count']}/{comp_data['expected_count']} APIs ({comp_data['completeness_pct']:.1f}%)")
            print(f"     å…³é”®API: {comp_data['critical_apis_available']}/{comp_data['critical_apis_total']} ({comp_data['critical_completeness_pct']:.1f}%)")
        
        # æœ€å¤§æ•°æ®æº
        print(f"\nğŸ” **ä¸»è¦æ•°æ®æº (æŒ‰å®¹é‡æ’åº)**:")
        for source in summary['top_data_sources'][:5]:
            print(f"  ğŸ“Š {source['category']}: {source['size_gb']:.1f} GB")
        
        # æ¨èå»ºè®®
        print(f"\nğŸ’¡ **ä¼˜åŒ–å»ºè®®**:")
        
        # åŸºäºå®Œæ•´æ€§ç»™å‡ºå»ºè®®
        for category, comp_data in completeness.items():
            if comp_data['completeness_pct'] < 90:
                missing = comp_data['expected_count'] - comp_data['actual_count']
                print(f"  ğŸ”§ {category}: å»ºè®®è¡¥å…… {missing} ä¸ªAPIæ¥å£ä»¥è¾¾åˆ°å®Œæ•´è¦†ç›–")
            
            if comp_data['critical_completeness_pct'] < 100:
                print(f"  âš ï¸ {category}: å…³é”®APIå°šæœªå®Œå…¨è¦†ç›–ï¼Œå»ºè®®ä¼˜å…ˆè¡¥å……")
        
        # æ•°æ®è´¨é‡å»ºè®®
        if stats['average_data_quality'] < 95:
            print(f"  ğŸ§¹ æ•°æ®æ¸…æ´—: å¹³å‡è´¨é‡ä¸º {stats['average_data_quality']:.1f}%ï¼Œå»ºè®®è¿›è¡Œæ•°æ®æ¸…æ´—ä¼˜åŒ–")
        
        if stats['total_data_size_gb'] > 200:
            print(f"  ğŸ“¦ å­˜å‚¨ä¼˜åŒ–: æ•°æ®é‡å·²è¾¾ {stats['total_data_size_gb']:.1f}GBï¼Œå»ºè®®è€ƒè™‘æ•°æ®å‹ç¼©æˆ–å½’æ¡£ç­–ç•¥")
    
    def create_excel_report(self, reports, summary, completeness):
        """åˆ›å»ºExcelæŠ¥å‘Š"""
        try:
            with pd.ExcelWriter('QuantTrade_æ•°æ®è´¨é‡ç»¼åˆæŠ¥å‘Š.xlsx', engine='openpyxl') as writer:
                
                # æ‰§è¡Œæ‘˜è¦
                summary_df = pd.DataFrame([summary['overall_stats']])
                summary_df.to_excel(writer, sheet_name='æ‰§è¡Œæ‘˜è¦', index=False)
                
                # APIæ¸…å•
                if 'inventory' in reports:
                    reports['inventory'].to_excel(writer, sheet_name='APIæ¸…å•', index=False)
                
                # è´¨é‡æ¦‚è§ˆ
                if 'quality' in reports:
                    reports['quality'].to_excel(writer, sheet_name='è´¨é‡æ¦‚è§ˆ', index=False)
                
                # å®Œæ•´æ€§åˆ†æ
                completeness_df = pd.DataFrame.from_dict(completeness, orient='index')
                completeness_df.to_excel(writer, sheet_name='å®Œæ•´æ€§åˆ†æ')
                
            logging.info("âœ… ç”ŸæˆExcelæŠ¥å‘Š: QuantTrade_æ•°æ®è´¨é‡ç»¼åˆæŠ¥å‘Š.xlsx")
            
        except Exception as e:
            logging.warning(f"âš ï¸ ExcelæŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")

if __name__ == "__main__":
    reporter = ComprehensiveDataReporter()
    summary, completeness, coverage = reporter.generate_comprehensive_report()