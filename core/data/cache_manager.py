#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ™ºèƒ½ç¼“å­˜ç®¡ç†å™¨
=============

é«˜æ€§èƒ½ã€åˆ†å±‚çš„æ•°æ®ç¼“å­˜ç³»ç»Ÿ
"""

import os
import pickle
import json
import hashlib
import gzip
import shutil
from typing import Dict, List, Optional, Union, Any, Callable
import pandas as pd
from datetime import datetime, timedelta
from pathlib import Path
import logging
import threading
from concurrent.futures import ThreadPoolExecutor
import sqlite3

logger = logging.getLogger(__name__)

class SmartCacheManager:
    """æ™ºèƒ½ç¼“å­˜ç®¡ç†å™¨
    
    æä¾›å¤šå±‚ç¼“å­˜ç­–ç•¥ï¼š
    - å†…å­˜ç¼“å­˜ (æœ€å¿«)
    - ç£ç›˜ç¼“å­˜ (æŒä¹…åŒ–)
    - å‹ç¼©ç¼“å­˜ (èŠ‚çœç©ºé—´)
    - SQLiteç¼“å­˜ (ç»“æ„åŒ–æ•°æ®)
    """
    
    def __init__(self, config: Optional[Dict] = None):
        """åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨
        
        Args:
            config: é…ç½®å‚æ•°
        """
        self.config = config or {}
        
        # ç¼“å­˜é…ç½®
        self.cache_dir = Path(self.config.get('cache_dir', './cache'))
        self.max_memory_size = self.config.get('max_memory_size', 100 * 1024 * 1024)  # 100MB
        self.max_disk_size = self.config.get('max_disk_size', 1024 * 1024 * 1024)    # 1GB
        self.default_expire_hours = self.config.get('default_expire_hours', 24)
        self.compression_enabled = self.config.get('compression_enabled', True)
        self.cleanup_interval_hours = self.config.get('cleanup_interval_hours', 6)
        
        # åˆ›å»ºç¼“å­˜ç›®å½•
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # ç¼“å­˜å­ç›®å½•
        self.memory_cache_dir = self.cache_dir / 'memory'
        self.disk_cache_dir = self.cache_dir / 'disk'
        self.compressed_cache_dir = self.cache_dir / 'compressed'
        self.sqlite_cache_dir = self.cache_dir / 'sqlite'
        
        for dir_path in [self.memory_cache_dir, self.disk_cache_dir, 
                        self.compressed_cache_dir, self.sqlite_cache_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)
        
        # å†…å­˜ç¼“å­˜
        self._memory_cache: Dict[str, Dict] = {}
        self._memory_usage = 0
        self._cache_lock = threading.RLock()
        
        # SQLiteæ•°æ®åº“è¿æ¥
        self.db_path = self.sqlite_cache_dir / 'cache.db'
        self._init_sqlite_db()
        
        # ç¼“å­˜ç»Ÿè®¡
        self.stats = {
            'hits': 0,
            'misses': 0,
            'memory_hits': 0,
            'disk_hits': 0,
            'compressed_hits': 0,
            'sqlite_hits': 0
        }
        
        # å¯åŠ¨æ¸…ç†çº¿ç¨‹
        self._cleanup_thread = None
        self._start_cleanup_thread()
        
        logger.info(f"âœ… æ™ºèƒ½ç¼“å­˜ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆï¼Œç¼“å­˜ç›®å½•: {self.cache_dir}")
    
    def _init_sqlite_db(self):
        """åˆå§‹åŒ–SQLiteæ•°æ®åº“"""
        try:
            with sqlite3.connect(str(self.db_path)) as conn:
                conn.execute('''
                    CREATE TABLE IF NOT EXISTS cache_data (
                        key TEXT PRIMARY KEY,
                        data BLOB,
                        metadata TEXT,
                        created_time TIMESTAMP,
                        expire_time TIMESTAMP,
                        access_count INTEGER DEFAULT 0,
                        last_access TIMESTAMP
                    )
                ''')
                conn.execute('''
                    CREATE INDEX IF NOT EXISTS idx_expire_time ON cache_data(expire_time)
                ''')
                conn.execute('''
                    CREATE INDEX IF NOT EXISTS idx_last_access ON cache_data(last_access)
                ''')
                conn.commit()
                
            logger.info("âœ… SQLiteç¼“å­˜æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ SQLiteæ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {str(e)}")
    
    def _generate_cache_key(self, 
                           data_type: str,
                           params: Dict[str, Any]) -> str:
        """ç”Ÿæˆç¼“å­˜é”®
        
        Args:
            data_type: æ•°æ®ç±»å‹
            params: å‚æ•°å­—å…¸
            
        Returns:
            str: ç¼“å­˜é”®
        """
        # æ ‡å‡†åŒ–å‚æ•°
        sorted_params = sorted(params.items())
        param_str = json.dumps(sorted_params, sort_keys=True, default=str)
        
        # ç”Ÿæˆå“ˆå¸Œ
        cache_key = f"{data_type}_{hashlib.md5(param_str.encode()).hexdigest()}"
        return cache_key
    
    def _get_cache_metadata(self, 
                           expire_hours: Optional[int] = None,
                           tags: Optional[List[str]] = None) -> Dict[str, Any]:
        """è·å–ç¼“å­˜å…ƒæ•°æ®"""
        expire_hours = expire_hours or self.default_expire_hours
        expire_time = datetime.now() + timedelta(hours=expire_hours)
        
        return {
            'created_time': datetime.now(),
            'expire_time': expire_time,
            'tags': tags or [],
            'size_bytes': 0,
            'access_count': 0,
            'last_access': datetime.now()
        }
    
    def _serialize_data(self, data: Any) -> bytes:
        """åºåˆ—åŒ–æ•°æ®"""
        if isinstance(data, pd.DataFrame):
            # DataFrameä½¿ç”¨pickleåºåˆ—åŒ–
            return pickle.dumps(data)
        else:
            # å…¶ä»–æ•°æ®ç±»å‹ä½¿ç”¨pickle
            return pickle.dumps(data)
    
    def _deserialize_data(self, data_bytes: bytes) -> Any:
        """ååºåˆ—åŒ–æ•°æ®"""
        return pickle.loads(data_bytes)
    
    def _compress_data(self, data_bytes: bytes) -> bytes:
        """å‹ç¼©æ•°æ®"""
        if self.compression_enabled:
            return gzip.compress(data_bytes)
        return data_bytes
    
    def _decompress_data(self, compressed_bytes: bytes) -> bytes:
        """è§£å‹ç¼©æ•°æ®"""
        if self.compression_enabled:
            try:
                return gzip.decompress(compressed_bytes)
            except:
                # å¦‚æœè§£å‹å¤±è´¥ï¼Œå¯èƒ½æ˜¯æœªå‹ç¼©çš„æ•°æ®
                return compressed_bytes
        return compressed_bytes
    
    def get(self, 
           data_type: str,
           params: Dict[str, Any],
           default: Any = None) -> Any:
        """è·å–ç¼“å­˜æ•°æ®
        
        Args:
            data_type: æ•°æ®ç±»å‹
            params: å‚æ•°å­—å…¸
            default: é»˜è®¤å€¼
            
        Returns:
            Any: ç¼“å­˜çš„æ•°æ®ï¼Œå¦‚æœä¸å­˜åœ¨è¿”å›default
        """
        cache_key = self._generate_cache_key(data_type, params)
        
        try:
            # 1. å°è¯•å†…å­˜ç¼“å­˜
            result = self._get_from_memory(cache_key)
            if result is not None:
                self.stats['hits'] += 1
                self.stats['memory_hits'] += 1
                logger.debug(f"âœ… å†…å­˜ç¼“å­˜å‘½ä¸­: {cache_key}")
                return result
            
            # 2. å°è¯•ç£ç›˜ç¼“å­˜
            result = self._get_from_disk(cache_key)
            if result is not None:
                # å°†æ•°æ®åŠ è½½åˆ°å†…å­˜ç¼“å­˜
                self._put_to_memory(cache_key, result)
                self.stats['hits'] += 1
                self.stats['disk_hits'] += 1
                logger.debug(f"âœ… ç£ç›˜ç¼“å­˜å‘½ä¸­: {cache_key}")
                return result
            
            # 3. å°è¯•å‹ç¼©ç¼“å­˜
            result = self._get_from_compressed(cache_key)
            if result is not None:
                # å°†æ•°æ®åŠ è½½åˆ°å†…å­˜å’Œç£ç›˜ç¼“å­˜
                self._put_to_memory(cache_key, result)
                self._put_to_disk(cache_key, result)
                self.stats['hits'] += 1
                self.stats['compressed_hits'] += 1
                logger.debug(f"âœ… å‹ç¼©ç¼“å­˜å‘½ä¸­: {cache_key}")
                return result
            
            # 4. å°è¯•SQLiteç¼“å­˜
            result = self._get_from_sqlite(cache_key)
            if result is not None:
                # å°†æ•°æ®åŠ è½½åˆ°ä¸Šå±‚ç¼“å­˜
                self._put_to_memory(cache_key, result)
                self._put_to_disk(cache_key, result)
                self.stats['hits'] += 1
                self.stats['sqlite_hits'] += 1
                logger.debug(f"âœ… SQLiteç¼“å­˜å‘½ä¸­: {cache_key}")
                return result
            
            # æ‰€æœ‰ç¼“å­˜éƒ½æœªå‘½ä¸­
            self.stats['misses'] += 1
            logger.debug(f"âŒ ç¼“å­˜æœªå‘½ä¸­: {cache_key}")
            return default
            
        except Exception as e:
            logger.error(f"âŒ è·å–ç¼“å­˜å¤±è´¥: {str(e)}")
            return default
    
    def put(self, 
           data_type: str,
           params: Dict[str, Any],
           data: Any,
           expire_hours: Optional[int] = None,
           tags: Optional[List[str]] = None):
        """å­˜å‚¨æ•°æ®åˆ°ç¼“å­˜
        
        Args:
            data_type: æ•°æ®ç±»å‹
            params: å‚æ•°å­—å…¸
            data: è¦ç¼“å­˜çš„æ•°æ®
            expire_hours: è¿‡æœŸå°æ—¶æ•°
            tags: æ ‡ç­¾åˆ—è¡¨
        """
        cache_key = self._generate_cache_key(data_type, params)
        metadata = self._get_cache_metadata(expire_hours, tags)
        
        try:
            # è®¡ç®—æ•°æ®å¤§å°
            serialized_data = self._serialize_data(data)
            data_size = len(serialized_data)
            metadata['size_bytes'] = data_size
            
            # åˆ†å±‚å­˜å‚¨ç­–ç•¥
            if data_size < 1024 * 1024:  # å°äº1MBï¼Œå­˜å‚¨åˆ°å†…å­˜ç¼“å­˜
                self._put_to_memory(cache_key, data, metadata)
                
            if data_size < 10 * 1024 * 1024:  # å°äº10MBï¼Œå­˜å‚¨åˆ°ç£ç›˜ç¼“å­˜
                self._put_to_disk(cache_key, data, metadata)
            else:
                # å¤§æ•°æ®ä½¿ç”¨å‹ç¼©ç¼“å­˜
                self._put_to_compressed(cache_key, data, metadata)
            
            # ç»“æ„åŒ–æ•°æ®å­˜å‚¨åˆ°SQLite
            if isinstance(data, (pd.DataFrame, dict, list)):
                self._put_to_sqlite(cache_key, data, metadata)
            
            logger.debug(f"âœ… æ•°æ®å·²ç¼“å­˜: {cache_key} ({data_size} bytes)")
            
        except Exception as e:
            logger.error(f"âŒ ç¼“å­˜å­˜å‚¨å¤±è´¥: {str(e)}")
    
    def _get_from_memory(self, cache_key: str) -> Any:
        """ä»å†…å­˜ç¼“å­˜è·å–æ•°æ®"""
        with self._cache_lock:
            if cache_key in self._memory_cache:
                cache_item = self._memory_cache[cache_key]
                
                # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
                if datetime.now() > cache_item['metadata']['expire_time']:
                    del self._memory_cache[cache_key]
                    return None
                
                # æ›´æ–°è®¿é—®ç»Ÿè®¡
                cache_item['metadata']['access_count'] += 1
                cache_item['metadata']['last_access'] = datetime.now()
                
                return cache_item['data']
            
            return None
    
    def _put_to_memory(self, 
                      cache_key: str, 
                      data: Any,
                      metadata: Optional[Dict] = None):
        """å­˜å‚¨æ•°æ®åˆ°å†…å­˜ç¼“å­˜"""
        if metadata is None:
            metadata = self._get_cache_metadata()
        
        with self._cache_lock:
            # æ£€æŸ¥å†…å­˜ä½¿ç”¨é‡
            data_size = metadata.get('size_bytes', 0)
            if self._memory_usage + data_size > self.max_memory_size:
                self._evict_memory_cache(data_size)
            
            self._memory_cache[cache_key] = {
                'data': data,
                'metadata': metadata
            }
            self._memory_usage += data_size
    
    def _evict_memory_cache(self, required_size: int):
        """å†…å­˜ç¼“å­˜æ·˜æ±°ç®—æ³• (LRU)"""
        with self._cache_lock:
            # æŒ‰æœ€åè®¿é—®æ—¶é—´æ’åº
            items = list(self._memory_cache.items())
            items.sort(key=lambda x: x[1]['metadata']['last_access'])
            
            freed_size = 0
            for cache_key, cache_item in items:
                if freed_size >= required_size:
                    break
                
                item_size = cache_item['metadata'].get('size_bytes', 0)
                del self._memory_cache[cache_key]
                self._memory_usage -= item_size
                freed_size += item_size
                
                logger.debug(f"ğŸ—‘ï¸ ä»å†…å­˜ç¼“å­˜æ·˜æ±°: {cache_key}")
    
    def _get_from_disk(self, cache_key: str) -> Any:
        """ä»ç£ç›˜ç¼“å­˜è·å–æ•°æ®"""
        cache_file = self.disk_cache_dir / f"{cache_key}.pkl"
        metadata_file = self.disk_cache_dir / f"{cache_key}.meta"
        
        if cache_file.exists() and metadata_file.exists():
            try:
                # è¯»å–å…ƒæ•°æ®
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f, object_hook=self._datetime_parser)
                
                # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
                if datetime.now() > metadata['expire_time']:
                    cache_file.unlink(missing_ok=True)
                    metadata_file.unlink(missing_ok=True)
                    return None
                
                # è¯»å–æ•°æ®
                with open(cache_file, 'rb') as f:
                    data = pickle.load(f)
                
                # æ›´æ–°è®¿é—®ç»Ÿè®¡
                metadata['access_count'] += 1
                metadata['last_access'] = datetime.now()
                
                with open(metadata_file, 'w', encoding='utf-8') as f:
                    json.dump(metadata, f, default=str, indent=2)
                
                return data
                
            except Exception as e:
                logger.warning(f"âš ï¸ è¯»å–ç£ç›˜ç¼“å­˜å¤±è´¥: {str(e)}")
                cache_file.unlink(missing_ok=True)
                metadata_file.unlink(missing_ok=True)
        
        return None
    
    def _put_to_disk(self, 
                    cache_key: str,
                    data: Any,
                    metadata: Optional[Dict] = None):
        """å­˜å‚¨æ•°æ®åˆ°ç£ç›˜ç¼“å­˜"""
        if metadata is None:
            metadata = self._get_cache_metadata()
        
        cache_file = self.disk_cache_dir / f"{cache_key}.pkl"
        metadata_file = self.disk_cache_dir / f"{cache_key}.meta"
        
        try:
            # å†™å…¥æ•°æ®
            with open(cache_file, 'wb') as f:
                pickle.dump(data, f)
            
            # å†™å…¥å…ƒæ•°æ®
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, default=str, indent=2)
                
        except Exception as e:
            logger.error(f"âŒ ç£ç›˜ç¼“å­˜å†™å…¥å¤±è´¥: {str(e)}")
            cache_file.unlink(missing_ok=True)
            metadata_file.unlink(missing_ok=True)
    
    def _get_from_compressed(self, cache_key: str) -> Any:
        """ä»å‹ç¼©ç¼“å­˜è·å–æ•°æ®"""
        cache_file = self.compressed_cache_dir / f"{cache_key}.pkl.gz"
        metadata_file = self.compressed_cache_dir / f"{cache_key}.meta"
        
        if cache_file.exists() and metadata_file.exists():
            try:
                # è¯»å–å…ƒæ•°æ®
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f, object_hook=self._datetime_parser)
                
                # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
                if datetime.now() > metadata['expire_time']:
                    cache_file.unlink(missing_ok=True)
                    metadata_file.unlink(missing_ok=True)
                    return None
                
                # è¯»å–å¹¶è§£å‹æ•°æ®
                with gzip.open(cache_file, 'rb') as f:
                    data = pickle.load(f)
                
                return data
                
            except Exception as e:
                logger.warning(f"âš ï¸ è¯»å–å‹ç¼©ç¼“å­˜å¤±è´¥: {str(e)}")
                cache_file.unlink(missing_ok=True)
                metadata_file.unlink(missing_ok=True)
        
        return None
    
    def _put_to_compressed(self, 
                          cache_key: str,
                          data: Any,
                          metadata: Optional[Dict] = None):
        """å­˜å‚¨æ•°æ®åˆ°å‹ç¼©ç¼“å­˜"""
        if metadata is None:
            metadata = self._get_cache_metadata()
        
        cache_file = self.compressed_cache_dir / f"{cache_key}.pkl.gz"
        metadata_file = self.compressed_cache_dir / f"{cache_key}.meta"
        
        try:
            # å†™å…¥å‹ç¼©æ•°æ®
            with gzip.open(cache_file, 'wb') as f:
                pickle.dump(data, f)
            
            # å†™å…¥å…ƒæ•°æ®
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, default=str, indent=2)
                
        except Exception as e:
            logger.error(f"âŒ å‹ç¼©ç¼“å­˜å†™å…¥å¤±è´¥: {str(e)}")
            cache_file.unlink(missing_ok=True)
            metadata_file.unlink(missing_ok=True)
    
    def _get_from_sqlite(self, cache_key: str) -> Any:
        """ä»SQLiteç¼“å­˜è·å–æ•°æ®"""
        try:
            with sqlite3.connect(str(self.db_path)) as conn:
                cursor = conn.execute('''
                    SELECT data, metadata, expire_time, access_count 
                    FROM cache_data 
                    WHERE key = ? AND expire_time > ?
                ''', (cache_key, datetime.now()))
                
                row = cursor.fetchone()
                if row:
                    data_bytes, metadata_str, expire_time, access_count = row
                    
                    # ååºåˆ—åŒ–æ•°æ®
                    data = pickle.loads(data_bytes)
                    
                    # æ›´æ–°è®¿é—®ç»Ÿè®¡
                    conn.execute('''
                        UPDATE cache_data 
                        SET access_count = access_count + 1, last_access = ?
                        WHERE key = ?
                    ''', (datetime.now(), cache_key))
                    
                    conn.commit()
                    return data
                    
        except Exception as e:
            logger.warning(f"âš ï¸ è¯»å–SQLiteç¼“å­˜å¤±è´¥: {str(e)}")
        
        return None
    
    def _put_to_sqlite(self, 
                      cache_key: str,
                      data: Any,
                      metadata: Optional[Dict] = None):
        """å­˜å‚¨æ•°æ®åˆ°SQLiteç¼“å­˜"""
        if metadata is None:
            metadata = self._get_cache_metadata()
        
        try:
            # åºåˆ—åŒ–æ•°æ®
            data_bytes = pickle.dumps(data)
            metadata_str = json.dumps(metadata, default=str)
            
            with sqlite3.connect(str(self.db_path)) as conn:
                conn.execute('''
                    INSERT OR REPLACE INTO cache_data
                    (key, data, metadata, created_time, expire_time, access_count, last_access)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                ''', (
                    cache_key, data_bytes, metadata_str,
                    metadata['created_time'], metadata['expire_time'],
                    0, datetime.now()
                ))
                conn.commit()
                
        except Exception as e:
            logger.error(f"âŒ SQLiteç¼“å­˜å†™å…¥å¤±è´¥: {str(e)}")
    
    def _datetime_parser(self, dct: Dict) -> Dict:
        """JSONæ—¥æœŸæ—¶é—´è§£æå™¨"""
        for key, value in dct.items():
            if isinstance(value, str) and key.endswith('_time'):
                try:
                    dct[key] = datetime.fromisoformat(value.replace('Z', '+00:00'))
                except:
                    pass
        return dct
    
    def clear_cache(self, 
                   cache_type: Optional[str] = None,
                   tags: Optional[List[str]] = None):
        """æ¸…ç†ç¼“å­˜
        
        Args:
            cache_type: ç¼“å­˜ç±»å‹ ('memory', 'disk', 'compressed', 'sqlite', Noneä¸ºå…¨éƒ¨)
            tags: æ ‡ç­¾è¿‡æ»¤
        """
        logger.info(f"ğŸ§¹ å¼€å§‹æ¸…ç†ç¼“å­˜ (ç±»å‹: {cache_type or 'å…¨éƒ¨'})")
        
        if cache_type is None or cache_type == 'memory':
            self._clear_memory_cache(tags)
        
        if cache_type is None or cache_type == 'disk':
            self._clear_disk_cache(tags)
        
        if cache_type is None or cache_type == 'compressed':
            self._clear_compressed_cache(tags)
        
        if cache_type is None or cache_type == 'sqlite':
            self._clear_sqlite_cache(tags)
        
        logger.info("âœ… ç¼“å­˜æ¸…ç†å®Œæˆ")
    
    def _clear_memory_cache(self, tags: Optional[List[str]] = None):
        """æ¸…ç†å†…å­˜ç¼“å­˜"""
        with self._cache_lock:
            if tags is None:
                # æ¸…ç†å…¨éƒ¨å†…å­˜ç¼“å­˜
                self._memory_cache.clear()
                self._memory_usage = 0
            else:
                # æŒ‰æ ‡ç­¾æ¸…ç†
                keys_to_remove = []
                for key, item in self._memory_cache.items():
                    item_tags = item['metadata'].get('tags', [])
                    if any(tag in item_tags for tag in tags):
                        keys_to_remove.append(key)
                
                for key in keys_to_remove:
                    item = self._memory_cache.pop(key)
                    self._memory_usage -= item['metadata'].get('size_bytes', 0)
    
    def _clear_disk_cache(self, tags: Optional[List[str]] = None):
        """æ¸…ç†ç£ç›˜ç¼“å­˜"""
        if tags is None:
            # æ¸…ç†å…¨éƒ¨ç£ç›˜ç¼“å­˜
            shutil.rmtree(self.disk_cache_dir, ignore_errors=True)
            self.disk_cache_dir.mkdir(parents=True, exist_ok=True)
        else:
            # æŒ‰æ ‡ç­¾æ¸…ç†
            for meta_file in self.disk_cache_dir.glob("*.meta"):
                try:
                    with open(meta_file, 'r', encoding='utf-8') as f:
                        metadata = json.load(f, object_hook=self._datetime_parser)
                    
                    item_tags = metadata.get('tags', [])
                    if any(tag in item_tags for tag in tags):
                        # åˆ é™¤æ•°æ®æ–‡ä»¶å’Œå…ƒæ•°æ®æ–‡ä»¶
                        cache_key = meta_file.stem
                        cache_file = self.disk_cache_dir / f"{cache_key}.pkl"
                        cache_file.unlink(missing_ok=True)
                        meta_file.unlink(missing_ok=True)
                        
                except Exception as e:
                    logger.warning(f"âš ï¸ æ¸…ç†ç£ç›˜ç¼“å­˜å¤±è´¥: {str(e)}")
    
    def _clear_compressed_cache(self, tags: Optional[List[str]] = None):
        """æ¸…ç†å‹ç¼©ç¼“å­˜"""
        if tags is None:
            # æ¸…ç†å…¨éƒ¨å‹ç¼©ç¼“å­˜
            shutil.rmtree(self.compressed_cache_dir, ignore_errors=True)
            self.compressed_cache_dir.mkdir(parents=True, exist_ok=True)
        else:
            # æŒ‰æ ‡ç­¾æ¸…ç†
            for meta_file in self.compressed_cache_dir.glob("*.meta"):
                try:
                    with open(meta_file, 'r', encoding='utf-8') as f:
                        metadata = json.load(f, object_hook=self._datetime_parser)
                    
                    item_tags = metadata.get('tags', [])
                    if any(tag in item_tags for tag in tags):
                        # åˆ é™¤æ•°æ®æ–‡ä»¶å’Œå…ƒæ•°æ®æ–‡ä»¶
                        cache_key = meta_file.stem
                        cache_file = self.compressed_cache_dir / f"{cache_key}.pkl.gz"
                        cache_file.unlink(missing_ok=True)
                        meta_file.unlink(missing_ok=True)
                        
                except Exception as e:
                    logger.warning(f"âš ï¸ æ¸…ç†å‹ç¼©ç¼“å­˜å¤±è´¥: {str(e)}")
    
    def _clear_sqlite_cache(self, tags: Optional[List[str]] = None):
        """æ¸…ç†SQLiteç¼“å­˜"""
        try:
            with sqlite3.connect(str(self.db_path)) as conn:
                if tags is None:
                    # æ¸…ç†å…¨éƒ¨SQLiteç¼“å­˜
                    conn.execute('DELETE FROM cache_data')
                else:
                    # æŒ‰æ ‡ç­¾æ¸…ç†ï¼ˆéœ€è¦è§£æmetadata JSONï¼‰
                    cursor = conn.execute('SELECT key, metadata FROM cache_data')
                    keys_to_delete = []
                    
                    for key, metadata_str in cursor.fetchall():
                        try:
                            metadata = json.loads(metadata_str)
                            item_tags = metadata.get('tags', [])
                            if any(tag in item_tags for tag in tags):
                                keys_to_delete.append(key)
                        except:
                            continue
                    
                    for key in keys_to_delete:
                        conn.execute('DELETE FROM cache_data WHERE key = ?', (key,))
                
                conn.commit()
                
        except Exception as e:
            logger.error(f"âŒ æ¸…ç†SQLiteç¼“å­˜å¤±è´¥: {str(e)}")
    
    def cleanup_expired(self):
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        logger.info("ğŸ§¹ æ¸…ç†è¿‡æœŸç¼“å­˜...")
        
        current_time = datetime.now()
        
        # æ¸…ç†å†…å­˜è¿‡æœŸæ•°æ®
        with self._cache_lock:
            expired_keys = []
            for key, item in self._memory_cache.items():
                if current_time > item['metadata']['expire_time']:
                    expired_keys.append(key)
            
            for key in expired_keys:
                item = self._memory_cache.pop(key)
                self._memory_usage -= item['metadata'].get('size_bytes', 0)
        
        # æ¸…ç†ç£ç›˜è¿‡æœŸæ•°æ®
        for meta_file in self.disk_cache_dir.glob("*.meta"):
            try:
                with open(meta_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f, object_hook=self._datetime_parser)
                
                if current_time > metadata['expire_time']:
                    cache_key = meta_file.stem
                    cache_file = self.disk_cache_dir / f"{cache_key}.pkl"
                    cache_file.unlink(missing_ok=True)
                    meta_file.unlink(missing_ok=True)
                    
            except Exception:
                # å¦‚æœå…ƒæ•°æ®æ–‡ä»¶æŸåï¼Œåˆ é™¤å¯¹åº”çš„ç¼“å­˜æ–‡ä»¶
                cache_key = meta_file.stem
                cache_file = self.disk_cache_dir / f"{cache_key}.pkl"
                cache_file.unlink(missing_ok=True)
                meta_file.unlink(missing_ok=True)
        
        # æ¸…ç†å‹ç¼©è¿‡æœŸæ•°æ®
        for meta_file in self.compressed_cache_dir.glob("*.meta"):
            try:
                with open(meta_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f, object_hook=self._datetime_parser)
                
                if current_time > metadata['expire_time']:
                    cache_key = meta_file.stem
                    cache_file = self.compressed_cache_dir / f"{cache_key}.pkl.gz"
                    cache_file.unlink(missing_ok=True)
                    meta_file.unlink(missing_ok=True)
                    
            except Exception:
                # å¦‚æœå…ƒæ•°æ®æ–‡ä»¶æŸåï¼Œåˆ é™¤å¯¹åº”çš„ç¼“å­˜æ–‡ä»¶
                cache_key = meta_file.stem
                cache_file = self.compressed_cache_dir / f"{cache_key}.pkl.gz"
                cache_file.unlink(missing_ok=True)
                meta_file.unlink(missing_ok=True)
        
        # æ¸…ç†SQLiteè¿‡æœŸæ•°æ®
        try:
            with sqlite3.connect(str(self.db_path)) as conn:
                conn.execute('DELETE FROM cache_data WHERE expire_time < ?', (current_time,))
                conn.commit()
                
                # ä¼˜åŒ–æ•°æ®åº“
                conn.execute('VACUUM')
                
        except Exception as e:
            logger.error(f"âŒ æ¸…ç†SQLiteè¿‡æœŸæ•°æ®å¤±è´¥: {str(e)}")
        
        logger.info("âœ… è¿‡æœŸç¼“å­˜æ¸…ç†å®Œæˆ")
    
    def _start_cleanup_thread(self):
        """å¯åŠ¨æ¸…ç†çº¿ç¨‹"""
        if self._cleanup_thread is not None:
            return
        
        def cleanup_worker():
            import time
            while True:
                try:
                    time.sleep(self.cleanup_interval_hours * 3600)  # è½¬æ¢ä¸ºç§’
                    self.cleanup_expired()
                except Exception as e:
                    logger.error(f"âŒ æ¸…ç†çº¿ç¨‹å¼‚å¸¸: {str(e)}")
        
        self._cleanup_thread = threading.Thread(target=cleanup_worker, daemon=True)
        self._cleanup_thread.start()
        
        logger.info("âœ… ç¼“å­˜æ¸…ç†çº¿ç¨‹å·²å¯åŠ¨")
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        # è®¡ç®—ç¼“å­˜å¤§å°
        def get_dir_size(path: Path) -> int:
            return sum(f.stat().st_size for f in path.rglob('*') if f.is_file())
        
        memory_items = len(self._memory_cache)
        disk_size = get_dir_size(self.disk_cache_dir)
        compressed_size = get_dir_size(self.compressed_cache_dir)
        
        # SQLiteå¤§å°
        sqlite_size = 0
        if self.db_path.exists():
            sqlite_size = self.db_path.stat().st_size
        
        hit_rate = 0
        if self.stats['hits'] + self.stats['misses'] > 0:
            hit_rate = self.stats['hits'] / (self.stats['hits'] + self.stats['misses'])
        
        return {
            'memory_cache': {
                'items': memory_items,
                'usage_bytes': self._memory_usage,
                'usage_mb': self._memory_usage / 1024 / 1024,
                'max_size_mb': self.max_memory_size / 1024 / 1024
            },
            'disk_cache': {
                'size_bytes': disk_size,
                'size_mb': disk_size / 1024 / 1024
            },
            'compressed_cache': {
                'size_bytes': compressed_size,
                'size_mb': compressed_size / 1024 / 1024
            },
            'sqlite_cache': {
                'size_bytes': sqlite_size,
                'size_mb': sqlite_size / 1024 / 1024
            },
            'statistics': {
                'total_hits': self.stats['hits'],
                'total_misses': self.stats['misses'],
                'hit_rate': hit_rate,
                'memory_hits': self.stats['memory_hits'],
                'disk_hits': self.stats['disk_hits'],
                'compressed_hits': self.stats['compressed_hits'],
                'sqlite_hits': self.stats['sqlite_hits']
            }
        }
    
    def __del__(self):
        """ææ„å‡½æ•°"""
        if hasattr(self, '_cleanup_thread') and self._cleanup_thread:
            self._cleanup_thread.join(timeout=1.0)