#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®åŠ è½½å™¨å®Œæ•´å®ç° - data_loader.py
====================================

è´Ÿè´£ä»å„ç§æ•°æ®æºè·å–é‡‘èæ•°æ®ï¼ŒåŒ…æ‹¬ï¼š
- ä¼˜çŸ¿APIæ•°æ®è·å–ï¼ˆå·²ä¿®å¤APIæ–¹æ³•åï¼‰
- è‚¡ç¥¨åŸºç¡€ä¿¡æ¯
- ä»·æ ¼è¡Œæƒ…æ•°æ®  
- è´¢åŠ¡æ•°æ®
- æŒ‡æ•°æˆåˆ†è‚¡

è®¾è®¡ç‰¹ç‚¹ï¼š
1. æ”¯æŒå¤šç§æ•°æ®æº
2. è‡ªåŠ¨ç¼“å­˜æœºåˆ¶
3. é”™è¯¯é‡è¯•æœºåˆ¶
4. æ•°æ®è´¨é‡æ£€æŸ¥
5. ä¿®å¤ä¼˜çŸ¿APIå…¼å®¹æ€§é—®é¢˜

ç‰ˆæœ¬: 2.0.0
æ›´æ–°: 2024-08-26 (ä¿®å¤APIæ–¹æ³•å)
"""

import os
import time
import pickle
import hashlib
import warnings
import json
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Union, Tuple
import pandas as pd
import numpy as np

# å¯¼å…¥é…ç½®
try:
    from config.settings import Config
    from config.database_config import DatabaseConfig
except ImportError:
    # å¦‚æœç›´æ¥è¿è¡Œæ­¤æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
    class Config:
        UQER_TOKEN = "68b9922817ae6273137bda7acba81e293582ba347281dfcc056dcb245b23faf3"
        START_DATE = '2020-01-01'
        END_DATE = '2024-08-20'
        UNIVERSE = 'CSI300'
        INDEX_CODE = '000300'
        CACHE_DIR = './cache'
        ENABLE_CACHE = True
        CACHE_EXPIRE_HOURS = 24

warnings.filterwarnings('ignore')


class DataLoader:
    """æ•°æ®è·å–å™¨ä¸»ç±»"""
    
    def __init__(self, config: Optional[Config] = None):
        """
        åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
        
        Args:
            config: é…ç½®å¯¹è±¡
        """
        self.config = config if config else Config()
        self.cache_dir = self.config.CACHE_DIR
        
        # åˆå§‹åŒ–ä¼˜çŸ¿å®¢æˆ·ç«¯
        self.client = None
        self.DataAPI = None
        self._init_uqer_client()
        
        # åˆ›å»ºç¼“å­˜ç›®å½•
        os.makedirs(self.cache_dir, exist_ok=True)
        
        print(f"ğŸš€ æ•°æ®åŠ è½½å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   ğŸ“ ç¼“å­˜ç›®å½•: {self.cache_dir}")
        print(f"   ğŸ”— APIçŠ¶æ€: {'âœ… å·²è¿æ¥' if self.client else 'âŒ æœªè¿æ¥'}")
    
    def _init_uqer_client(self):
        """åˆå§‹åŒ–ä¼˜çŸ¿APIå®¢æˆ·ç«¯"""
        try:
            import uqer
            from uqer import Client, DataAPI
            
            # è¿æ¥ä¼˜çŸ¿
            self.client = Client(token=self.config.UQER_TOKEN)
            self.DataAPI = DataAPI
            
            # æµ‹è¯•è¿æ¥ï¼ˆä½¿ç”¨æ­£ç¡®çš„APIæ–¹æ³•åï¼‰
            test_result = DataAPI.EquGet(
                field='ticker,shortName',
                ticker='000001',
                limit=1
            )
            
            if not test_result.empty:
                print("âœ… ä¼˜çŸ¿APIè¿æ¥æˆåŠŸ")
            else:
                print("âš ï¸ ä¼˜çŸ¿APIæµ‹è¯•æŸ¥è¯¢è¿”å›ç©ºç»“æœ")
                
        except ImportError:
            print("âŒ ä¼˜çŸ¿æ¨¡å—æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install uqer")
            self.client = None
        except Exception as e:
            print(f"âŒ ä¼˜çŸ¿APIè¿æ¥å¤±è´¥: {str(e)}")
            self.client = None
    
    def _generate_cache_key(self, *args) -> str:
        """ç”Ÿæˆç¼“å­˜æ–‡ä»¶çš„é”®å€¼"""
        key_str = '_'.join(str(arg) for arg in args)
        return hashlib.md5(key_str.encode()).hexdigest()
    
    def _is_cache_valid(self, cache_path: str) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ"""
        if not os.path.exists(cache_path):
            return False
        
        # æ£€æŸ¥æ–‡ä»¶ä¿®æ”¹æ—¶é—´
        file_time = datetime.fromtimestamp(os.path.getmtime(cache_path))
        expire_time = datetime.now() - timedelta(hours=self.config.CACHE_EXPIRE_HOURS)
        
        return file_time > expire_time
    
    def _load_cache(self, cache_path: str):
        """ä»ç¼“å­˜åŠ è½½æ•°æ®"""
        try:
            with open(cache_path, 'rb') as f:
                return pickle.load(f)
        except Exception as e:
            print(f"âš ï¸ ç¼“å­˜åŠ è½½å¤±è´¥ {cache_path}: {e}")
            return None
    
    def _save_cache(self, data, cache_path: str):
        """ä¿å­˜æ•°æ®åˆ°ç¼“å­˜"""
        try:
            with open(cache_path, 'wb') as f:
                pickle.dump(data, f)
        except Exception as e:
            print(f"âš ï¸ ç¼“å­˜ä¿å­˜å¤±è´¥ {cache_path}: {e}")
    
    def get_index_components(self, index_code: str = '000300') -> List[str]:
        """
        è·å–æŒ‡æ•°æˆåˆ†è‚¡ï¼ˆä¿®å¤ç‰ˆAPIè°ƒç”¨ï¼‰
        
        Args:
            index_code: æŒ‡æ•°ä»£ç 
            
        Returns:
            æˆåˆ†è‚¡ä»£ç åˆ—è¡¨
        """
        print(f"ğŸ“‹ è·å–æŒ‡æ•° {index_code} æˆåˆ†è‚¡...")
        
        # ç¼“å­˜é”®å€¼
        cache_key = self._generate_cache_key('index_components', index_code)
        cache_path = os.path.join(self.cache_dir, f"idx_comp_{cache_key}.pkl")
        
        # æ£€æŸ¥ç¼“å­˜
        if self.config.ENABLE_CACHE and self._is_cache_valid(cache_path):
            cached_data = self._load_cache(cache_path)
            if cached_data is not None:
                print(f"ğŸ“¥ ä»ç¼“å­˜è·å–æˆåˆ†è‚¡: {len(cached_data)} åª")
                return cached_data
        
        if not self.client:
            print("âŒ APIæœªè¿æ¥ï¼Œä½¿ç”¨é»˜è®¤è‚¡ç¥¨æ± ")
            default_stocks = ['000001.XSHE', '000002.XSHE', '600000.XSHG', 
                             '600036.XSHG', '000858.XSHE']
            return default_stocks
        
        try:
            # ä½¿ç”¨æ­£ç¡®çš„APIæ–¹æ³•åï¼šIdxConsGetï¼ˆè€Œä¸æ˜¯getIdxConsï¼‰
            result = self.DataAPI.IdxConsGet(
                ticker=index_code,
                field='consTickerSymbol'
            )
            
            if not result.empty:
                stock_list = result['consTickerSymbol'].tolist()
                print(f"âœ… è·å–åˆ° {len(stock_list)} åªæˆåˆ†è‚¡")
                
                # ä¿å­˜ç¼“å­˜
                if self.config.ENABLE_CACHE:
                    self._save_cache(stock_list, cache_path)
                
                return stock_list
            else:
                print("âš ï¸ æŒ‡æ•°æˆåˆ†è‚¡æŸ¥è¯¢ç»“æœä¸ºç©º")
                
        except Exception as e:
            print(f"âŒ æŒ‡æ•°æˆåˆ†è‚¡è·å–å¤±è´¥: {e}")
        
        # è¿”å›é»˜è®¤è‚¡ç¥¨æ± 
        print("ğŸ”„ ä½¿ç”¨é»˜è®¤è‚¡ç¥¨æ± ")
        default_stocks = ['000001.XSHE', '000002.XSHE', '600000.XSHG', 
                         '600036.XSHG', '000858.XSHE', '000166.XSHE',
                         '000338.XSHE', '600519.XSHG', '000858.XSHE', '002415.XSHE']
        return default_stocks
    
    def get_stock_info(self, stock_list: List[str]) -> pd.DataFrame:
        """
        è·å–è‚¡ç¥¨åŸºç¡€ä¿¡æ¯ï¼ˆä¿®å¤ç‰ˆAPIè°ƒç”¨ï¼‰
        
        Args:
            stock_list: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            
        Returns:
            è‚¡ç¥¨åŸºç¡€ä¿¡æ¯DataFrame
        """
        print(f"ğŸ“Š è·å– {len(stock_list)} åªè‚¡ç¥¨åŸºç¡€ä¿¡æ¯...")
        
        # ç¼“å­˜é”®å€¼
        cache_key = self._generate_cache_key('stock_info', str(sorted(stock_list)))
        cache_path = os.path.join(self.cache_dir, f"stock_info_{cache_key}.pkl")
        
        # æ£€æŸ¥ç¼“å­˜
        if self.config.ENABLE_CACHE and self._is_cache_valid(cache_path):
            cached_data = self._load_cache(cache_path)
            if cached_data is not None:
                print(f"ğŸ“¥ ä»ç¼“å­˜è·å–è‚¡ç¥¨ä¿¡æ¯")
                return cached_data
        
        if not self.client:
            print("âŒ APIæœªè¿æ¥ï¼Œè¿”å›ç©ºDataFrame")
            return pd.DataFrame()
        
        try:
            # åˆ†æ‰¹è·å–è‚¡ç¥¨ä¿¡æ¯ï¼ˆé¿å…APIé™åˆ¶ï¼‰
            all_info = []
            batch_size = 50
            
            for i in range(0, len(stock_list), batch_size):
                batch = stock_list[i:i+batch_size]
                tickers = ','.join(batch)
                
                # ä½¿ç”¨æ­£ç¡®çš„APIæ–¹æ³•åï¼šEquGetï¼ˆè€Œä¸æ˜¯getEquï¼‰
                result = self.DataAPI.EquGet(
                    ticker=tickers,
                    field='ticker,shortName,industry,listDate,delistDate'
                )
                
                if not result.empty:
                    all_info.append(result)
                
                time.sleep(0.1)  # é¿å…APIé™åˆ¶
            
            if all_info:
                stock_info = pd.concat(all_info, ignore_index=True)
                print(f"âœ… è·å–åˆ° {len(stock_info)} åªè‚¡ç¥¨åŸºç¡€ä¿¡æ¯")
                
                # ä¿å­˜ç¼“å­˜
                if self.config.ENABLE_CACHE:
                    self._save_cache(stock_info, cache_path)
                
                return stock_info
            else:
                print("âš ï¸ è‚¡ç¥¨åŸºç¡€ä¿¡æ¯æŸ¥è¯¢ç»“æœä¸ºç©º")
                return pd.DataFrame()
                
        except Exception as e:
            print(f"âŒ è‚¡ç¥¨åŸºç¡€ä¿¡æ¯è·å–å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def get_price_data(self, stock_list: List[str], start_date: str = None, 
                      end_date: str = None) -> pd.DataFrame:
        """
        è·å–è‚¡ç¥¨ä»·æ ¼æ•°æ®ï¼ˆä¿®å¤ç‰ˆAPIè°ƒç”¨ï¼‰
        
        Args:
            stock_list: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            
        Returns:
            ä»·æ ¼æ•°æ®DataFrame
        """
        start_date = start_date or self.config.START_DATE
        end_date = end_date or self.config.END_DATE
        
        print(f"ğŸ’° è·å–ä»·æ ¼æ•°æ®: {len(stock_list)} åªè‚¡ç¥¨")
        print(f"   ğŸ“… æ—¶é—´èŒƒå›´: {start_date} â†’ {end_date}")
        
        # ç¼“å­˜é”®å€¼
        cache_key = self._generate_cache_key('price_data', str(sorted(stock_list)), 
                                           start_date, end_date)
        cache_path = os.path.join(self.cache_dir, f"price_data_{cache_key}.pkl")
        
        # æ£€æŸ¥ç¼“å­˜
        if self.config.ENABLE_CACHE and self._is_cache_valid(cache_path):
            cached_data = self._load_cache(cache_path)
            if cached_data is not None:
                print(f"ğŸ“¥ ä»ç¼“å­˜è·å–ä»·æ ¼æ•°æ®")
                return cached_data
        
        if not self.client:
            print("âŒ APIæœªè¿æ¥ï¼Œè¿”å›ç©ºDataFrame")
            return pd.DataFrame()
        
        try:
            all_data = []
            batch_size = 30  # é™ä½æ‰¹å¤„ç†å¤§å°ï¼Œé¿å…APIé™åˆ¶
            
            for i in range(0, len(stock_list), batch_size):
                batch = stock_list[i:i+batch_size]
                tickers = ','.join(batch)
                
                print(f"   ğŸ“Š è·å–æ‰¹æ¬¡ {i//batch_size + 1}: {len(batch)} åªè‚¡ç¥¨")
                
                # ä½¿ç”¨æ­£ç¡®çš„APIæ–¹æ³•åï¼šMktEqudGetï¼ˆè€Œä¸æ˜¯getMktEqudï¼‰
                result = self.DataAPI.MktEqudGet(
                    ticker=tickers,
                    beginDate=start_date.replace('-', ''),
                    endDate=end_date.replace('-', ''),
                    field='ticker,tradeDate,openPrice,highestPrice,lowestPrice,closePrice,turnoverVol,turnoverValue'
                )
                
                if not result.empty:
                    all_data.append(result)
                
                time.sleep(0.2)  # å¢åŠ å»¶è¿Ÿé¿å…APIé™åˆ¶
            
            if all_data:
                price_data = pd.concat(all_data, ignore_index=True)
                
                # æ•°æ®é¢„å¤„ç†
                price_data['tradeDate'] = pd.to_datetime(price_data['tradeDate'])
                price_data = price_data.sort_values(['ticker', 'tradeDate'])
                
                print(f"âœ… è·å–ä»·æ ¼æ•°æ®: {price_data.shape}")
                
                # ä¿å­˜ç¼“å­˜
                if self.config.ENABLE_CACHE:
                    self._save_cache(price_data, cache_path)
                
                return price_data
            else:
                print("âš ï¸ ä»·æ ¼æ•°æ®æŸ¥è¯¢ç»“æœä¸ºç©º")
                return pd.DataFrame()
                
        except Exception as e:
            print(f"âŒ ä»·æ ¼æ•°æ®è·å–å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def get_financial_data(self, stock_list: List[str], start_date: str = None, 
                          end_date: str = None) -> pd.DataFrame:
        """
        è·å–è´¢åŠ¡æ•°æ®
        
        Args:
            stock_list: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            
        Returns:
            è´¢åŠ¡æ•°æ®DataFrame
        """
        print(f"ğŸ“ˆ è·å–è´¢åŠ¡æ•°æ®: {len(stock_list)} åªè‚¡ç¥¨")
        
        if not self.client:
            print("âŒ APIæœªè¿æ¥ï¼Œè¿”å›ç©ºDataFrame")
            return pd.DataFrame()
        
        try:
            # ç®€åŒ–è´¢åŠ¡æ•°æ®è·å–ï¼Œä»…è·å–åŸºç¡€æŒ‡æ ‡
            tickers = ','.join(stock_list[:10])  # é™åˆ¶æ•°é‡é¿å…è¶…æ—¶
            
            result = self.DataAPI.FdmtEfGet(
                ticker=tickers,
                beginDate=start_date.replace('-', '') if start_date else '20240101',
                endDate=end_date.replace('-', '') if end_date else '20241231',
                field='ticker,publishDate,revenue,netProfit,totalAssets,totalLiab'
            )
            
            if not result.empty:
                print(f"âœ… è·å–è´¢åŠ¡æ•°æ®: {result.shape}")
                return result
            else:
                print("âš ï¸ è´¢åŠ¡æ•°æ®ä¸ºç©º")
                return pd.DataFrame()
                
        except Exception as e:
            print(f"âŒ è´¢åŠ¡æ•°æ®è·å–å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def get_complete_dataset(self, index_code: str = None, 
                           start_date: str = None, end_date: str = None,
                           include_financial: bool = False) -> Dict[str, pd.DataFrame]:
        """
        è·å–å®Œæ•´æ•°æ®é›†
        
        Args:
            index_code: æŒ‡æ•°ä»£ç 
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            include_financial: æ˜¯å¦åŒ…å«è´¢åŠ¡æ•°æ®
            
        Returns:
            åŒ…å«æ‰€æœ‰æ•°æ®çš„å­—å…¸
        """
        print(f"ğŸ¯ è·å–å®Œæ•´æ•°æ®é›†...")
        
        index_code = index_code or self.config.INDEX_CODE
        start_date = start_date or self.config.START_DATE
        end_date = end_date or self.config.END_DATE
        
        dataset = {}
        
        # 1. è·å–è‚¡ç¥¨æ± 
        stock_list = self.get_index_components(index_code)
        dataset['stock_list'] = stock_list
        
        if not stock_list:
            print("âŒ æœªè·å–åˆ°è‚¡ç¥¨åˆ—è¡¨ï¼Œæ— æ³•ç»§ç»­")
            return dataset
        
        # 2. è·å–è‚¡ç¥¨åŸºç¡€ä¿¡æ¯
        stock_info = self.get_stock_info(stock_list)
        dataset['stock_info'] = stock_info
        
        # 3. è·å–ä»·æ ¼æ•°æ®
        price_data = self.get_price_data(stock_list, start_date, end_date)
        dataset['price_data'] = price_data
        
        # 4. è·å–è´¢åŠ¡æ•°æ®ï¼ˆå¯é€‰ï¼‰
        if include_financial:
            financial_data = self.get_financial_data(stock_list, start_date, end_date)
            dataset['financial_data'] = financial_data
        
        # æ•°æ®æ‘˜è¦
        print(f"\nğŸ“Š æ•°æ®é›†æ‘˜è¦:")
        print(f"   è‚¡ç¥¨æ•°é‡: {len(stock_list)}")
        print(f"   ä»·æ ¼æ•°æ®: {price_data.shape if not price_data.empty else 'N/A'}")
        if include_financial:
            print(f"   è´¢åŠ¡æ•°æ®: {financial_data.shape if not financial_data.empty else 'N/A'}")
        
        return dataset
    
    def clear_cache(self):
        """æ¸…ç†æ‰€æœ‰ç¼“å­˜æ–‡ä»¶"""
        import glob
        
        cache_files = glob.glob(os.path.join(self.cache_dir, "*.pkl"))
        cleaned_count = 0
        
        for file_path in cache_files:
            try:
                os.remove(file_path)
                cleaned_count += 1
            except Exception as e:
                print(f"âš ï¸ åˆ é™¤ç¼“å­˜æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
        
        print(f"ğŸ§¹ æ¸…ç†ç¼“å­˜å®Œæˆ: {cleaned_count} ä¸ªæ–‡ä»¶")
    
    def __repr__(self) -> str:
        """æ•°æ®åŠ è½½å™¨ä¿¡æ¯"""
        return f"""
DataLoader Summary:
==================
ğŸ”— APIçŠ¶æ€: {'âœ… å·²è¿æ¥' if self.client else 'âŒ æœªè¿æ¥'}
ğŸ“ ç¼“å­˜ç›®å½•: {self.cache_dir}
ğŸ—‚ï¸ ç¼“å­˜å¯ç”¨: {self.config.ENABLE_CACHE}
â±ï¸ ç¼“å­˜è¿‡æœŸ: {self.config.CACHE_EXPIRE_HOURS}å°æ—¶
==================
"""


# å·¥å‚å‡½æ•°
def create_data_loader(config: Optional[Config] = None) -> DataLoader:
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨å®ä¾‹"""
    return DataLoader(config)


# æ¨¡å—å¯¼å‡º
__all__ = ['DataLoader', 'create_data_loader']


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    loader = DataLoader()
    
    # è·å–å®Œæ•´æ•°æ®é›†
    dataset = loader.get_complete_dataset(
        index_code='000300',  # æ²ªæ·±300
        include_financial=False
    )
    
    print("\nğŸ‰ æ•°æ®è·å–æ¼”ç¤ºå®Œæˆï¼")