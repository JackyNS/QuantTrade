#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®ç®¡ç†å™¨å®Œæ•´å®ç° - data_manager.py
===================================

ç»Ÿä¸€åè°ƒæ‰€æœ‰æ•°æ®ç»„ä»¶çš„ç®¡ç†å™¨ï¼Œæä¾›ï¼š
- ğŸ¯ å®Œæ•´çš„æ•°æ®æµæ°´çº¿ç®¡ç†
- ğŸ”„ è‡ªåŠ¨åŒ–æ•°æ®è·å–â†’å¤„ç†â†’ç‰¹å¾å·¥ç¨‹
- ğŸ’¾ æ™ºèƒ½ç¼“å­˜å’Œå¢é‡æ›´æ–°
- ğŸ“Š æ•°æ®è´¨é‡ç›‘æ§å’ŒæŠ¥å‘Š
- ğŸ›¡ï¸ é”™è¯¯å¤„ç†å’Œæ¢å¤æœºåˆ¶
- ğŸš€ é«˜æ€§èƒ½å¹¶è¡Œå¤„ç†

ç‰ˆæœ¬: 2.0.0
æ›´æ–°æ—¶é—´: 2024-08-26
å…¼å®¹ç¯å¢ƒ: VSCode + JupyterNote + ä¼˜çŸ¿API
"""

import os
import warnings
import numpy as np
import pandas as pd
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Union, Any
import json
import hashlib
import pickle
from pathlib import Path

# å¯¼å…¥æ•°æ®ç»„ä»¶
try:
    from .data_loader import DataLoader
    from .data_processor import DataProcessor
    from .feature_engineer import FeatureEngineer
    COMPONENTS_AVAILABLE = True
except ImportError:
    # å¦‚æœä½œä¸ºç‹¬ç«‹æ¨¡å—è¿è¡Œï¼Œå°è¯•ç›´æ¥å¯¼å…¥
    try:
        from data_loader import DataLoader
        from data_processor import DataProcessor
        from feature_engineer import FeatureEngineer
        COMPONENTS_AVAILABLE = True
    except ImportError:
        print("âš ï¸ æ— æ³•å¯¼å…¥æ•°æ®ç»„ä»¶ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
        COMPONENTS_AVAILABLE = False

# å¯¼å…¥é…ç½®
try:
    from config.settings import Config
except ImportError:
    # é»˜è®¤é…ç½®ç±»
    class Config:
        START_DATE = '2020-01-01'
        END_DATE = '2024-08-20'
        UNIVERSE = 'CSI300'
        INDEX_CODE = '000300'
        CACHE_DIR = './cache'
        ENABLE_CACHE = True

# æŠ‘åˆ¶è­¦å‘Š
warnings.filterwarnings('ignore')

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

print("ğŸ¯ æ•°æ®ç®¡ç†å™¨æ¨¡å—åŠ è½½ä¸­...")


class DataManager:
    """
    æ•°æ®ç®¡ç†å™¨ - ç»Ÿä¸€åè°ƒæ‰€æœ‰æ•°æ®ç›¸å…³ç»„ä»¶
    """
    
    def __init__(self, config: Optional[Dict] = None):
        """
        åˆå§‹åŒ–æ•°æ®ç®¡ç†å™¨
        
        Args:
            config: é…ç½®å­—å…¸
        """
        self.config = config or self._get_default_config()
        self.cache_dir = self.config.get('cache_dir', './cache')
        os.makedirs(self.cache_dir, exist_ok=True)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.loader = None
        self.processor = None
        self.engineer = None
        self._init_components()
        
        # ç®¡ç†å™¨çŠ¶æ€
        self.pipeline_cache = {}
        self.execution_history = []
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'pipeline_runs': 0,
            'total_processing_time': 0,
            'data_quality_scores': [],
            'cache_usage': {'hits': 0, 'misses': 0},
            'error_counts': {}
        }
        
        print("ğŸ› ï¸ æ•°æ®ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   ğŸ“ ç¼“å­˜ç›®å½•: {self.cache_dir}")
        print(f"   ğŸ”§ ç»„ä»¶çŠ¶æ€: {'âœ… å®Œæ•´' if COMPONENTS_AVAILABLE else 'âš ï¸ æ¨¡æ‹Ÿæ¨¡å¼'}")
    
    def _get_default_config(self) -> Dict:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            'cache_dir': './cache',
            'enable_cache': True,
            'cache_expire_hours': 24,
            'parallel_processing': False,
            'max_workers': 4,
            'batch_processing': True,
            'batch_size': 1000,
            'quality_threshold': 0.7,
            'auto_retry': True,
            'max_retries': 3,
        }
    
    def _init_components(self):
        """åˆå§‹åŒ–æ•°æ®ç»„ä»¶"""
        if not COMPONENTS_AVAILABLE:
            print("âš ï¸ ä½¿ç”¨æ¨¡æ‹Ÿç»„ä»¶")
            return
        
        try:
            # åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
            loader_config = Config() if 'Config' in globals() else None
            self.loader = DataLoader(loader_config)
            
            # åˆå§‹åŒ–æ•°æ®é¢„å¤„ç†å™¨
            processor_config = {
                'cache_dir': self.cache_dir,
                'enable_cache': self.config['enable_cache']
            }
            self.processor = DataProcessor(processor_config)
            
            # åˆå§‹åŒ–ç‰¹å¾å·¥ç¨‹å™¨
            engineer_config = {
                'cache_dir': self.cache_dir,
                'enable_cache': self.config['enable_cache']
            }
            self.engineer = FeatureEngineer(config=engineer_config)
            
            print("âœ… æ‰€æœ‰æ•°æ®ç»„ä»¶åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            print(f"âŒ ç»„ä»¶åˆå§‹åŒ–å¤±è´¥: {e}")
            logger.error(f"ç»„ä»¶åˆå§‹åŒ–é”™è¯¯: {e}")
    
    def _generate_pipeline_key(self, **kwargs) -> str:
        """ç”Ÿæˆæµæ°´çº¿ç¼“å­˜é”®"""
        config_str = json.dumps(kwargs, sort_keys=True, default=str)
        return hashlib.md5(config_str.encode()).hexdigest()
    
    def _load_pipeline_cache(self, cache_key: str):
        """åŠ è½½æµæ°´çº¿ç¼“å­˜"""
        if not self.config['enable_cache']:
            return None
        
        cache_path = os.path.join(self.cache_dir, f"pipeline_{cache_key}.pkl")
        
        try:
            if os.path.exists(cache_path):
                file_time = datetime.fromtimestamp(os.path.getmtime(cache_path))
                expire_time = datetime.now() - timedelta(hours=self.config['cache_expire_hours'])
                
                if file_time > expire_time:
                    with open(cache_path, 'rb') as f:
                        self.stats['cache_usage']['hits'] += 1
                        return pickle.load(f)
        except Exception as e:
            logger.warning(f"æµæ°´çº¿ç¼“å­˜åŠ è½½å¤±è´¥: {e}")
        
        self.stats['cache_usage']['misses'] += 1
        return None
    
    def _save_pipeline_cache(self, data, cache_key: str):
        """ä¿å­˜æµæ°´çº¿ç¼“å­˜"""
        if not self.config['enable_cache']:
            return
        
        cache_path = os.path.join(self.cache_dir, f"pipeline_{cache_key}.pkl")
        
        try:
            with open(cache_path, 'wb') as f:
                pickle.dump(data, f)
        except Exception as e:
            logger.warning(f"æµæ°´çº¿ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
    
    def load_data(self, data_type: str = 'price', **kwargs) -> pd.DataFrame:
        """
        åŠ è½½æ•°æ®
        
        Args:
            data_type: æ•°æ®ç±»å‹ ('price', 'stock_info', 'financial', 'complete')
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            åŠ è½½çš„æ•°æ®
        """
        print(f"ğŸ“¥ åŠ è½½æ•°æ®ç±»å‹: {data_type}")
        
        if not self.loader:
            print("âŒ æ•°æ®åŠ è½½å™¨ä¸å¯ç”¨")
            return pd.DataFrame()
        
        try:
            if data_type == 'price':
                return self.loader.get_price_data(**kwargs)
            elif data_type == 'stock_info':
                stock_list = kwargs.get('stock_list', [])
                return self.loader.get_stock_info(stock_list)
            elif data_type == 'financial':
                stock_list = kwargs.get('stock_list', [])
                return self.loader.get_financial_data(stock_list, **kwargs)
            elif data_type == 'complete':
                return self.loader.get_complete_dataset(**kwargs)
            else:
                print(f"âš ï¸ ä¸æ”¯æŒçš„æ•°æ®ç±»å‹: {data_type}")
                return pd.DataFrame()
                
        except Exception as e:
            print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
            logger.error(f"æ•°æ®åŠ è½½é”™è¯¯: {e}")
            return pd.DataFrame()
    
    def process_data(self, input_data: pd.DataFrame, 
                    processing_config: Optional[Dict] = None,
                    force_refresh: bool = False) -> pd.DataFrame:
        """
        å¤„ç†æ•°æ®
        
        Args:
            input_data: è¾“å…¥æ•°æ®
            processing_config: å¤„ç†é…ç½®
            force_refresh: å¼ºåˆ¶åˆ·æ–°
            
        Returns:
            å¤„ç†åçš„æ•°æ®
        """
        print("ğŸ§¹ å¼€å§‹æ•°æ®å¤„ç†...")
        
        if not self.processor:
            print("âŒ æ•°æ®å¤„ç†å™¨ä¸å¯ç”¨")
            return input_data
        
        if input_data.empty:
            print("âš ï¸ è¾“å…¥æ•°æ®ä¸ºç©º")
            return input_data
        
        try:
            # æ›´æ–°å¤„ç†å™¨é…ç½®
            if processing_config:
                for key, value in processing_config.items():
                    if hasattr(self.processor.config, key):
                        setattr(self.processor.config, key, value)
            
            # è¿è¡Œå¤„ç†æµæ°´çº¿
            results = self.processor.run_complete_pipeline(input_data)
            
            if 'normalized_data' in results:
                return results['normalized_data']
            elif 'data_with_returns' in results:
                return results['data_with_returns']
            elif 'clean_data' in results:
                return results['clean_data']
            else:
                return input_data
                
        except Exception as e:
            print(f"âŒ æ•°æ®å¤„ç†å¤±è´¥: {e}")
            logger.error(f"æ•°æ®å¤„ç†é”™è¯¯: {e}")
            return input_data
    
    def generate_features(self, input_data: pd.DataFrame,
                         feature_config: Optional[Dict] = None,
                         force_refresh: bool = False) -> pd.DataFrame:
        """
        ç”Ÿæˆç‰¹å¾
        
        Args:
            input_data: è¾“å…¥æ•°æ®
            feature_config: ç‰¹å¾é…ç½®
            force_refresh: å¼ºåˆ¶åˆ·æ–°
            
        Returns:
            åŒ…å«ç‰¹å¾çš„æ•°æ®
        """
        print("ğŸ”¬ å¼€å§‹ç‰¹å¾å·¥ç¨‹...")
        
        if not self.engineer:
            print("âŒ ç‰¹å¾å·¥ç¨‹å™¨ä¸å¯ç”¨")
            return input_data
        
        if input_data.empty:
            print("âš ï¸ è¾“å…¥æ•°æ®ä¸ºç©º")
            return input_data
        
        try:
            # æ›´æ–°ç‰¹å¾å·¥ç¨‹å™¨é…ç½®
            if feature_config:
                for key, value in feature_config.items():
                    if key in self.engineer.config:
                        self.engineer.config[key] = value
            
            # è®¾ç½®ä»·æ ¼æ•°æ®
            self.engineer.price_data = input_data
            
            # ç”Ÿæˆæ‰€æœ‰ç‰¹å¾
            features = self.engineer.generate_all_features(input_data)
            
            return features
            
        except Exception as e:
            print(f"âŒ ç‰¹å¾ç”Ÿæˆå¤±è´¥: {e}")
            logger.error(f"ç‰¹å¾ç”Ÿæˆé”™è¯¯: {e}")
            return input_data
    
    def run_complete_pipeline(self, 
                             data_config: Optional[Dict] = None,
                             processing_config: Optional[Dict] = None,
                             feature_config: Optional[Dict] = None,
                             force_refresh: bool = False) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´çš„æ•°æ®ç®¡é“
        
        Args:
            data_config: æ•°æ®è·å–é…ç½®
            processing_config: æ•°æ®é¢„å¤„ç†é…ç½®
            feature_config: ç‰¹å¾å·¥ç¨‹é…ç½®
            force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°æ‰€æœ‰ç¼“å­˜
            
        Returns:
            åŒ…å«æ‰€æœ‰è¾“å‡ºçš„ç»“æœå­—å…¸
        """
        print("ğŸš€ å¯åŠ¨å®Œæ•´æ•°æ®ç®¡é“")
        print("=" * 60)
        
        pipeline_start_time = datetime.now()
        results = {
            'pipeline_info': {
                'start_time': pipeline_start_time.isoformat(),
                'config': {
                    'data_config': data_config,
                    'processing_config': processing_config, 
                    'feature_config': feature_config
                }
            }
        }
        
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = self._generate_pipeline_key(
            data_config=data_config,
            processing_config=processing_config,
            feature_config=feature_config
        )
        
        # å°è¯•ä»ç¼“å­˜åŠ è½½
        if not force_refresh:
            cached_results = self._load_pipeline_cache(cache_key)
            if cached_results is not None:
                print("ğŸ“¥ ä»ç¼“å­˜åŠ è½½å®Œæ•´æµæ°´çº¿ç»“æœ")
                return cached_results
        
        try:
            # æ­¥éª¤1: æ•°æ®è·å–
            print("ğŸ“„ æ­¥éª¤1: æ•°æ®è·å–")
            raw_data = self.load_data(
                data_type='complete',
                **(data_config or {})
            )
            results['raw_data'] = raw_data
            
            if not raw_data or (isinstance(raw_data, dict) and not raw_data.get('price_data', pd.DataFrame()).empty is False):
                raise ValueError("æ•°æ®è·å–å¤±è´¥")
            
            # æå–ä»·æ ¼æ•°æ®
            if isinstance(raw_data, dict):
                price_data = raw_data.get('price_data', pd.DataFrame())
                stock_info = raw_data.get('stock_info', pd.DataFrame())
                results['stock_list'] = raw_data.get('stock_list', [])
            else:
                price_data = raw_data
                stock_info = pd.DataFrame()
            
            if price_data.empty:
                raise ValueError("ä»·æ ¼æ•°æ®ä¸ºç©º")
            
            # æ­¥éª¤2: æ•°æ®é¢„å¤„ç†
            print("\nğŸ“„ æ­¥éª¤2: æ•°æ®é¢„å¤„ç†")
            processed_data = self.process_data(
                input_data=price_data,
                processing_config=processing_config,
                force_refresh=force_refresh
            )
            results['processed_data'] = processed_data
            
            # æ­¥éª¤3: ç‰¹å¾å·¥ç¨‹
            print("\nğŸ“„ æ­¥éª¤3: ç‰¹å¾å·¥ç¨‹")
            features = self.generate_features(
                input_data=processed_data,
                feature_config=feature_config,
                force_refresh=force_refresh
            )
            results['features'] = features
            
            # æ­¥éª¤4: æ•°æ®è´¨é‡è¯„ä¼°
            print("\nğŸ“„ æ­¥éª¤4: æ•°æ®è´¨é‡è¯„ä¼°")
            quality_metrics = self._evaluate_data_quality(features)
            results['quality_metrics'] = quality_metrics
            
            # æ­¥éª¤5: ç”ŸæˆæŠ¥å‘Š
            pipeline_end_time = datetime.now()
            processing_time = (pipeline_end_time - pipeline_start_time).total_seconds()
            
            results['pipeline_info'].update({
                'end_time': pipeline_end_time.isoformat(),
                'processing_time': processing_time,
                'success': True
            })
            
            # æ›´æ–°ç»Ÿè®¡
            self.stats['pipeline_runs'] += 1
            self.stats['total_processing_time'] += processing_time
            if quality_metrics.get('overall_score'):
                self.stats['data_quality_scores'].append(quality_metrics['overall_score'])
            
            # è®°å½•æ‰§è¡Œå†å²
            self.execution_history.append({
                'timestamp': pipeline_start_time.isoformat(),
                'processing_time': processing_time,
                'data_rows': len(features) if not features.empty else 0,
                'feature_count': len(features.columns) if not features.empty else 0,
                'quality_score': quality_metrics.get('overall_score', 0),
                'success': True
            })
            
            print(f"âœ… å®Œæ•´æ•°æ®ç®¡é“æ‰§è¡Œå®Œæˆ")
            print(f"   â±ï¸ æ€»å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’")
            print(f"   ğŸ“Š æœ€ç»ˆæ•°æ®è¡Œæ•°: {len(features):,}")
            print(f"   ğŸ”¬ ç‰¹å¾æ€»æ•°: {len(features.columns)}")
            print(f"   ğŸ¯ è´¨é‡è¯„åˆ†: {quality_metrics.get('overall_score', 0):.3f}")
            
            # ä¿å­˜åˆ°ç¼“å­˜
            self._save_pipeline_cache(results, cache_key)
            
            return results
            
        except Exception as e:
            # é”™è¯¯å¤„ç†
            error_msg = str(e)
            error_type = type(e).__name__
            
            print(f"âŒ æ•°æ®ç®¡é“æ‰§è¡Œå¤±è´¥: {error_msg}")
            logger.error(f"ç®¡é“æ‰§è¡Œé”™è¯¯: {error_msg}")
            
            # æ›´æ–°é”™è¯¯ç»Ÿè®¡
            self.stats['error_counts'][error_type] = self.stats['error_counts'].get(error_type, 0) + 1
            
            # è®°å½•å¤±è´¥çš„æ‰§è¡Œå†å²
            self.execution_history.append({
                'timestamp': pipeline_start_time.isoformat(),
                'processing_time': (datetime.now() - pipeline_start_time).total_seconds(),
                'error': error_msg,
                'success': False
            })
            
            results['error'] = error_msg
            results['pipeline_info']['success'] = False
            
            return results
    
    def _evaluate_data_quality(self, data: pd.DataFrame) -> Dict[str, Any]:
        """è¯„ä¼°æ•°æ®è´¨é‡"""
        if data.empty:
            return {'overall_score': 0.0, 'metrics': {}}
        
        quality_metrics = {}
        
        try:
            # 1. å®Œæ•´æ€§æ£€æŸ¥
            total_cells = len(data) * len(data.columns)
            missing_cells = data.isnull().sum().sum()
            completeness = 1 - (missing_cells / total_cells) if total_cells > 0 else 0
            quality_metrics['completeness'] = completeness
            
            # 2. ä¸€è‡´æ€§æ£€æŸ¥ï¼ˆæ•°å€¼åˆ—çš„å¼‚å¸¸å€¼æ¯”ä¾‹ï¼‰
            numeric_cols = data.select_dtypes(include=[np.number]).columns
            outlier_ratio = 0
            if len(numeric_cols) > 0:
                outlier_count = 0
                total_numeric_values = 0
                
                for col in numeric_cols:
                    col_data = data[col].dropna()
                    if len(col_data) > 0:
                        z_scores = np.abs((col_data - col_data.mean()) / col_data.std())
                        outliers = np.sum(z_scores > 3)
                        outlier_count += outliers
                        total_numeric_values += len(col_data)
                
                outlier_ratio = outlier_count / total_numeric_values if total_numeric_values > 0 else 0
            
            consistency = max(0, 1 - outlier_ratio * 2)
            quality_metrics['consistency'] = consistency
            
            # 3. è¦†ç›–ç‡æ£€æŸ¥ï¼ˆæ—¶é—´è¿ç»­æ€§ï¼‰
            coverage = 1.0  # é»˜è®¤å€¼
            if 'tradeDate' in data.columns:
                date_col = data['tradeDate']
                if len(date_col.dropna()) > 1:
                    date_range = (date_col.max() - date_col.min()).days
                    unique_dates = date_col.nunique()
                    coverage = min(1.0, unique_dates / max(1, date_range / 7))  # å‡è®¾æ¯å‘¨5ä¸ªäº¤æ˜“æ—¥
            
            quality_metrics['coverage'] = coverage
            
            # 4. ç‰¹å¾è´¨é‡ï¼ˆéå¸¸æ•°ç‰¹å¾æ¯”ä¾‹ï¼‰
            feature_quality = 1.0
            if len(numeric_cols) > 0:
                non_constant_features = 0
                for col in numeric_cols:
                    if data[col].nunique() > 1:
                        non_constant_features += 1
                feature_quality = non_constant_features / len(numeric_cols)
            
            quality_metrics['feature_quality'] = feature_quality
            
            # 5. ç»¼åˆè¯„åˆ†
            overall_score = (
                completeness * 0.3 +
                consistency * 0.3 +
                coverage * 0.2 +
                feature_quality * 0.2
            )
            quality_metrics['overall_score'] = overall_score
            
        except Exception as e:
            logger.error(f"æ•°æ®è´¨é‡è¯„ä¼°é”™è¯¯: {e}")
            quality_metrics = {'overall_score': 0.0, 'error': str(e)}
        
        return quality_metrics
    
    def get_pipeline_status(self) -> Dict[str, Any]:
        """è·å–æµæ°´çº¿çŠ¶æ€"""
        return {
            'components': {
                'loader': self.loader is not None,
                'processor': self.processor is not None,
                'engineer': self.engineer is not None
            },
            'stats': self.stats,
            'config': self.config,
            'recent_executions': self.execution_history[-5:],  # æœ€è¿‘5æ¬¡æ‰§è¡Œ
            'cache_info': self._get_cache_info()
        }
    
    def _get_cache_info(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ä¿¡æ¯"""
        cache_info = {
            'cache_dir': self.cache_dir,
            'enabled': self.config['enable_cache'],
            'files': [],
            'total_size_mb': 0
        }
        
        try:
            cache_files = list(Path(self.cache_dir).glob("*.pkl"))
            total_size = 0
            
            for file_path in cache_files:
                size = file_path.stat().st_size
                total_size += size
                cache_info['files'].append({
                    'name': file_path.name,
                    'size_mb': size / (1024 * 1024),
                    'modified': datetime.fromtimestamp(file_path.stat().st_mtime).isoformat()
                })
            
            cache_info['file_count'] = len(cache_files)
            cache_info['total_size_mb'] = total_size / (1024 * 1024)
            
        except Exception as e:
            logger.warning(f"è·å–ç¼“å­˜ä¿¡æ¯å¤±è´¥: {e}")
        
        return cache_info
    
    def clear_all_cache(self):
        """æ¸…ç†æ‰€æœ‰ç¼“å­˜"""
        try:
            cache_files = list(Path(self.cache_dir).glob("*.pkl"))
            removed_count = 0
            
            for file_path in cache_files:
                try:
                    file_path.unlink()
                    removed_count += 1
                except Exception as e:
                    logger.warning(f"åˆ é™¤ç¼“å­˜æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            
            # æ¸…ç†ç»„ä»¶ç¼“å­˜
            if self.loader:
                self.loader.clear_cache()
            if self.processor:
                self.processor.reset_stats()
            
            print(f"ğŸ§¹ æ¸…ç†ç¼“å­˜å®Œæˆ: {removed_count} ä¸ªæ–‡ä»¶")
            
        except Exception as e:
            print(f"âŒ æ¸…ç†ç¼“å­˜å¤±è´¥: {e}")
    
    def validate_pipeline(self) -> Dict[str, bool]:
        """éªŒè¯æµæ°´çº¿å®Œæ•´æ€§"""
        validation_results = {}
        
        # æ£€æŸ¥ç»„ä»¶
        validation_results['loader_available'] = self.loader is not None
        validation_results['processor_available'] = self.processor is not None
        validation_results['engineer_available'] = self.engineer is not None
        
        # æ£€æŸ¥ç¼“å­˜ç›®å½•
        validation_results['cache_dir_exists'] = os.path.exists(self.cache_dir)
        validation_results['cache_dir_writable'] = os.access(self.cache_dir, os.W_OK)
        
        # ç»„ä»¶åŠŸèƒ½æµ‹è¯•
        if self.loader:
            try:
                # ç®€å•çš„è¿æ¥æµ‹è¯•
                validation_results['loader_functional'] = hasattr(self.loader, 'get_price_data')
            except Exception:
                validation_results['loader_functional'] = False
        else:
            validation_results['loader_functional'] = False
        
        # æ•´ä½“çŠ¶æ€
        validation_results['pipeline_ready'] = all([
            validation_results['loader_available'],
            validation_results['processor_available'],
            validation_results['engineer_available'],
            validation_results['cache_dir_exists']
        ])
        
        return validation_results
    
    def generate_pipeline_report(self) -> str:
        """ç”Ÿæˆæµæ°´çº¿è¿è¡ŒæŠ¥å‘Š"""
        status = self.get_pipeline_status()
        validation = self.validate_pipeline()
        
        report = f"""
ğŸ¯ æ•°æ®ç®¡ç†å™¨è¿è¡ŒæŠ¥å‘Š
{'='*50}
ğŸ“… ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

ğŸ“Š ç»„ä»¶çŠ¶æ€:
   ğŸ“¥ æ•°æ®åŠ è½½å™¨: {'âœ… æ­£å¸¸' if validation['loader_available'] else 'âŒ ä¸å¯ç”¨'}
   ğŸ§¹ æ•°æ®å¤„ç†å™¨: {'âœ… æ­£å¸¸' if validation['processor_available'] else 'âŒ ä¸å¯ç”¨'}
   ğŸ”¬ ç‰¹å¾å·¥ç¨‹å™¨: {'âœ… æ­£å¸¸' if validation['engineer_available'] else 'âŒ ä¸å¯ç”¨'}
   ğŸ¯ æµæ°´çº¿çŠ¶æ€: {'âœ… å°±ç»ª' if validation['pipeline_ready'] else 'âŒ æœªå°±ç»ª'}

ğŸ“ˆ è¿è¡Œç»Ÿè®¡:
   ğŸš€ æµæ°´çº¿æ‰§è¡Œ: {status['stats']['pipeline_runs']} æ¬¡
   â±ï¸ æ€»å¤„ç†æ—¶é—´: {status['stats']['total_processing_time']:.1f} ç§’
   ğŸ“Š å¹³å‡è´¨é‡åˆ†: {np.mean(status['stats']['data_quality_scores']) if status['stats']['data_quality_scores'] else 0:.3f}
   ğŸ’¾ ç¼“å­˜å‘½ä¸­ç‡: {status['stats']['cache_usage']['hits'] / (status['stats']['cache_usage']['hits'] + status['stats']['cache_usage']['misses']) * 100 if status['stats']['cache_usage']['hits'] + status['stats']['cache_usage']['misses'] > 0 else 0:.1f}%

ğŸ—‚ï¸ ç¼“å­˜ä¿¡æ¯:
   ğŸ“ ç¼“å­˜ç›®å½•: {status['cache_info']['cache_dir']}
   ğŸ“„ ç¼“å­˜æ–‡ä»¶: {status['cache_info'].get('file_count', 0)} ä¸ª
   ğŸ’¾ ç¼“å­˜å¤§å°: {status['cache_info'].get('total_size_mb', 0):.1f} MB

ğŸ’¡ å»ºè®®:
"""
        
        # æ·»åŠ å»ºè®®
        if not validation['pipeline_ready']:
            report += "   âš ï¸ æµæ°´çº¿æœªå®Œå…¨å°±ç»ªï¼Œè¯·æ£€æŸ¥ç»„ä»¶çŠ¶æ€\n"
        
        if status['stats']['pipeline_runs'] == 0:
            report += "   ğŸš€ å°šæœªè¿è¡Œæµæ°´çº¿ï¼Œå»ºè®®å…ˆæ‰§è¡Œæµ‹è¯•\n"
        
        error_count = sum(status['stats']['error_counts'].values())
        if error_count > 0:
            report += f"   ğŸ”§ å‘ç° {error_count} ä¸ªé”™è¯¯ï¼Œå»ºè®®æŸ¥çœ‹æ—¥å¿—\n"
        
        if status['stats']['data_quality_scores']:
            avg_quality = np.mean(status['stats']['data_quality_scores'])
            if avg_quality < 0.7:
                report += "   ğŸ“Š æ•°æ®è´¨é‡åä½ï¼Œå»ºè®®ä¼˜åŒ–æ•°æ®æº\n"
        
        report += "\n" + "="*50
        
        return report


# ==========================================
# ğŸ­ å·¥å‚å‡½æ•°å’Œæ¨¡å—å¯¼å‡º
# ==========================================

def create_data_manager(config: Optional[Dict] = None) -> DataManager:
    """
    åˆ›å»ºæ•°æ®ç®¡ç†å™¨å®ä¾‹çš„å·¥å‚å‡½æ•°
    
    Args:
        config: é…ç½®å‚æ•°å­—å…¸
        
    Returns:
        DataManagerå®ä¾‹
    """
    return DataManager(config)

# åˆ›å»ºé»˜è®¤å®ä¾‹
default_data_manager = DataManager()

# æ¨¡å—å¯¼å‡º
__all__ = [
    'DataManager',
    'create_data_manager',
    'default_data_manager'
]

if __name__ == "__main__":
    print("ğŸ¯ æ•°æ®ç®¡ç†å™¨ v2.0 æ¨¡å—åŠ è½½å®Œæˆ")
    print("ğŸ“˜ ä½¿ç”¨ç¤ºä¾‹:")
    print("   from data_manager import DataManager, create_data_manager")
    print("   manager = create_data_manager()")
    print("   results = manager.run_complete_pipeline()")
    print("")
    print("ğŸ’¡ åŠŸèƒ½ç‰¹æ€§:")
    print("   ğŸ¯ å®Œæ•´çš„æ•°æ®æµæ°´çº¿è‡ªåŠ¨åŒ–")
    print("   ğŸ”„ æ™ºèƒ½ç¼“å­˜å’Œå¢é‡æ›´æ–°")
    print("   ğŸ“Š æ•°æ®è´¨é‡ç›‘æ§å’ŒæŠ¥å‘Š")
    print("   ğŸ›¡ï¸ å®Œå–„çš„é”™è¯¯å¤„ç†æœºåˆ¶")
    print("   ğŸš€ é«˜æ€§èƒ½å¹¶è¡Œå¤„ç†æ”¯æŒ")
    print("   ğŸ“‹ è¯¦ç»†çš„æ‰§è¡Œå†å²å’Œç»Ÿè®¡")