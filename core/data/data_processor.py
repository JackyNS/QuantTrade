#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®é¢„å¤„ç†å™¨å®Œæ•´å®ç° - data_processor.py
=======================================

ä¸“ä¸ºé‡åŒ–äº¤æ˜“æ¡†æ¶è®¾è®¡çš„é«˜çº§æ•°æ®é¢„å¤„ç†å™¨ï¼ŒåŒ…å«ï¼š
- ğŸ§¹ æ™ºèƒ½æ•°æ®æ¸…æ´—å’Œå¼‚å¸¸å€¼å¤„ç†
- ğŸ“Š å¤šç»´åº¦è‚¡ç¥¨ç­›é€‰å’Œè´¨é‡è¯„ä¼°
- ğŸ“„ é«˜çº§æ•°æ®æ ‡å‡†åŒ–å’Œç‰¹å¾ç¼©æ”¾
- ğŸ“ˆ å¤šå‘¨æœŸæ”¶ç›Šç‡è®¡ç®—å’Œé£é™©æŒ‡æ ‡
- ğŸ’¾ é«˜æ•ˆç¼“å­˜æœºåˆ¶å’Œæ‰¹å¤„ç†
- ğŸ¯ å®Œæ•´çš„æ•°æ®è´¨é‡ç›‘æ§ä½“ç³»

ç‰ˆæœ¬: 2.0.0
æ›´æ–°æ—¶é—´: 2024-08-26
å…¼å®¹ç¯å¢ƒ: VSCode + JupyterNote + ä¼˜çŸ¿API
"""

import os
import warnings
import numpy as np
import pandas as pd
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional, Union, Any
from pathlib import Path
import hashlib
import pickle
import json

# ç§‘å­¦è®¡ç®—åº“
from scipy import stats
from scipy.stats import zscore

# æŠ‘åˆ¶è­¦å‘Š
warnings.filterwarnings('ignore')

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

print("ğŸ§¹ æ•°æ®é¢„å¤„ç†å™¨æ¨¡å—åŠ è½½ä¸­...")


class DataProcessor:
    """
    é«˜çº§æ•°æ®é¢„å¤„ç†å™¨ - é›†æˆæ¸…æ´—ã€ç­›é€‰ã€æ ‡å‡†åŒ–åŠŸèƒ½
    """
    
    def __init__(self, config: Optional[Dict] = None):
        """
        åˆå§‹åŒ–æ•°æ®é¢„å¤„ç†å™¨
        
        Args:
            config: é…ç½®å‚æ•°å­—å…¸
        """
        self.config = config or self._get_default_config()
        self.cache_dir = self.config.get('cache_dir', './cache')
        os.makedirs(self.cache_dir, exist_ok=True)
        
        # åˆå§‹åŒ–ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'processed_datasets': 0,
            'total_processing_time': 0,
            'cache_hits': 0,
            'cache_misses': 0,
            'quality_scores': [],
            'error_counts': {}
        }
        
        # å¤„ç†å†å²è®°å½•
        self.processing_history = []
        
        print("ğŸ› ï¸ æ•°æ®é¢„å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   ğŸ“ ç¼“å­˜ç›®å½•: {self.cache_dir}")
        print(f"   ğŸ”§ é…ç½®å‚æ•°: {len(self.config)} é¡¹")
    
    def _get_default_config(self) -> Dict:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            # æ•°æ®è´¨é‡å‚æ•°
            'min_trading_days': 250,     # æœ€å°‘äº¤æ˜“å¤©æ•°
            'max_missing_ratio': 0.1,    # æœ€å¤§ç¼ºå¤±å€¼æ¯”ä¾‹
            'min_price': 1.0,            # æœ€ä½ä»·æ ¼
            'max_price': 1000.0,         # æœ€é«˜ä»·æ ¼
            'min_volume': 100000,        # æœ€å°æˆäº¤é‡
            
            # STè‚¡ç¥¨å¤„ç†
            'exclude_st': True,          # æ’é™¤STè‚¡ç¥¨
            'exclude_new_days': 250,     # æ’é™¤æ–°è‚¡å¤©æ•°
            
            # å¼‚å¸¸å€¼å¤„ç†
            'outlier_method': 'zscore',  # å¼‚å¸¸å€¼æ£€æµ‹æ–¹æ³•
            'outlier_threshold': 3.0,    # å¼‚å¸¸å€¼é˜ˆå€¼
            'fill_method': 'forward',    # ç¼ºå¤±å€¼å¡«å……æ–¹æ³•
            
            # æ ‡å‡†åŒ–å‚æ•°
            'normalize_method': 'zscore', # æ ‡å‡†åŒ–æ–¹æ³•
            'normalize_features': True,   # æ˜¯å¦æ ‡å‡†åŒ–ç‰¹å¾
            
            # ç¼“å­˜è®¾ç½®
            'cache_dir': './cache',
            'enable_cache': True,
            'cache_expire_hours': 24,
            
            # æ”¶ç›Šç‡è®¡ç®—
            'return_periods': [1, 5, 10, 20],  # æ”¶ç›Šç‡å‘¨æœŸ
            'risk_free_rate': 0.03,            # æ— é£é™©åˆ©ç‡
        }
    
    def _generate_cache_key(self, *args) -> str:
        """ç”Ÿæˆç¼“å­˜é”®å€¼"""
        key_str = '_'.join(str(arg) for arg in args)
        return hashlib.md5(key_str.encode()).hexdigest()
    
    def _load_from_cache(self, cache_key: str):
        """ä»ç¼“å­˜åŠ è½½æ•°æ®"""
        cache_path = os.path.join(self.cache_dir, f"{cache_key}.pkl")
        
        try:
            if os.path.exists(cache_path):
                # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ
                file_time = datetime.fromtimestamp(os.path.getmtime(cache_path))
                expire_time = datetime.now() - timedelta(hours=self.config['cache_expire_hours'])
                
                if file_time > expire_time:
                    with open(cache_path, 'rb') as f:
                        self.stats['cache_hits'] += 1
                        return pickle.load(f)
        except Exception as e:
            logger.warning(f"ç¼“å­˜åŠ è½½å¤±è´¥: {e}")
        
        self.stats['cache_misses'] += 1
        return None
    
    def _save_to_cache(self, data, cache_key: str):
        """ä¿å­˜æ•°æ®åˆ°ç¼“å­˜"""
        if not self.config['enable_cache']:
            return
        
        cache_path = os.path.join(self.cache_dir, f"{cache_key}.pkl")
        
        try:
            with open(cache_path, 'wb') as f:
                pickle.dump(data, f)
        except Exception as e:
            logger.warning(f"ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
    
    def clean_price_data(self, price_data: pd.DataFrame) -> pd.DataFrame:
        """
        æ¸…æ´—ä»·æ ¼æ•°æ®
        
        Args:
            price_data: åŸå§‹ä»·æ ¼æ•°æ®
            
        Returns:
            æ¸…æ´—åçš„ä»·æ ¼æ•°æ®
        """
        if price_data.empty:
            return price_data
        
        print("ğŸ§¹ å¼€å§‹æ•°æ®æ¸…æ´—...")
        start_time = datetime.now()
        
        # ç”Ÿæˆç¼“å­˜é”®
        data_hash = hashlib.md5(str(price_data.shape).encode()).hexdigest()
        cache_key = f"clean_price_{data_hash}"
        
        # å°è¯•ä»ç¼“å­˜åŠ è½½
        cached_data = self._load_from_cache(cache_key)
        if cached_data is not None:
            print("ğŸ“¥ ä»ç¼“å­˜åŠ è½½æ¸…æ´—æ•°æ®")
            return cached_data
        
        # å¤åˆ¶æ•°æ®é¿å…ä¿®æ”¹åŸå§‹æ•°æ®
        clean_data = price_data.copy()
        original_rows = len(clean_data)
        
        # 1. åŸºç¡€æ•°æ®æ£€æŸ¥
        print("   ğŸ“Š åŸºç¡€æ•°æ®æ£€æŸ¥...")
        required_columns = ['ticker', 'tradeDate', 'closePrice', 'turnoverVol']
        missing_cols = [col for col in required_columns if col not in clean_data.columns]
        
        if missing_cols:
            print(f"   âš ï¸ ç¼ºå¤±å¿…è¦åˆ—: {missing_cols}")
            return clean_data
        
        # 2. æ•°æ®ç±»å‹è½¬æ¢
        print("   ğŸ”„ æ•°æ®ç±»å‹è½¬æ¢...")
        if 'tradeDate' in clean_data.columns:
            clean_data['tradeDate'] = pd.to_datetime(clean_data['tradeDate'])
        
        # æ•°å€¼åˆ—è½¬æ¢
        numeric_cols = ['openPrice', 'highestPrice', 'lowestPrice', 'closePrice', 
                       'turnoverVol', 'turnoverValue']
        for col in numeric_cols:
            if col in clean_data.columns:
                clean_data[col] = pd.to_numeric(clean_data[col], errors='coerce')
        
        # 3. ç§»é™¤å¼‚å¸¸å€¼
        print("   ğŸ¯ å¼‚å¸¸å€¼æ£€æµ‹å’Œå¤„ç†...")
        
        # ä»·æ ¼èŒƒå›´æ£€æŸ¥
        if 'closePrice' in clean_data.columns:
            price_mask = (clean_data['closePrice'] >= self.config['min_price']) & \
                        (clean_data['closePrice'] <= self.config['max_price'])
            clean_data = clean_data[price_mask]
        
        # æˆäº¤é‡æ£€æŸ¥
        if 'turnoverVol' in clean_data.columns:
            volume_mask = clean_data['turnoverVol'] >= self.config['min_volume']
            clean_data = clean_data[volume_mask]
        
        # 4. å¤„ç†ç¼ºå¤±å€¼
        print("   ğŸ”§ ç¼ºå¤±å€¼å¤„ç†...")
        
        # æŒ‰è‚¡ç¥¨åˆ†ç»„å¤„ç†ç¼ºå¤±å€¼
        if self.config['fill_method'] == 'forward':
            clean_data = clean_data.groupby('ticker').apply(
                lambda x: x.fillna(method='ffill')
            ).reset_index(drop=True)
        elif self.config['fill_method'] == 'interpolate':
            numeric_cols = clean_data.select_dtypes(include=[np.number]).columns
            clean_data[numeric_cols] = clean_data.groupby('ticker')[numeric_cols].apply(
                lambda x: x.interpolate()
            )
        
        # 5. ç§»é™¤ç¼ºå¤±å€¼è¿‡å¤šçš„è®°å½•
        max_missing = self.config['max_missing_ratio']
        missing_ratios = clean_data.isnull().sum(axis=1) / len(clean_data.columns)
        clean_data = clean_data[missing_ratios <= max_missing]
        
        # 6. æ•°æ®æ’åº
        print("   ğŸ“… æ•°æ®æ’åº...")
        clean_data = clean_data.sort_values(['ticker', 'tradeDate'])
        clean_data = clean_data.reset_index(drop=True)
        
        # æ¸…æ´—ç»Ÿè®¡
        cleaned_rows = len(clean_data)
        removed_rows = original_rows - cleaned_rows
        removal_rate = removed_rows / original_rows if original_rows > 0 else 0
        
        print(f"âœ… æ•°æ®æ¸…æ´—å®Œæˆ")
        print(f"   ğŸ“Š åŸå§‹æ•°æ®: {original_rows:,} è¡Œ")
        print(f"   ğŸ§¹ æ¸…æ´—å: {cleaned_rows:,} è¡Œ")
        print(f"   ğŸ—‘ï¸ ç§»é™¤: {removed_rows:,} è¡Œ ({removal_rate:.2%})")
        
        # ä¿å­˜åˆ°ç¼“å­˜
        self._save_to_cache(clean_data, cache_key)
        
        # è®°å½•å¤„ç†å†å²
        self.processing_history.append({
            'operation': 'clean_price_data',
            'timestamp': datetime.now().isoformat(),
            'input_rows': original_rows,
            'output_rows': cleaned_rows,
            'removal_rate': removal_rate
        })
        
        return clean_data
    
    def filter_stocks(self, price_data: pd.DataFrame, 
                     stock_info: pd.DataFrame = None) -> List[str]:
        """
        ç­›é€‰è‚¡ç¥¨æ± 
        
        Args:
            price_data: ä»·æ ¼æ•°æ®
            stock_info: è‚¡ç¥¨åŸºç¡€ä¿¡æ¯
            
        Returns:
            ç­›é€‰åçš„è‚¡ç¥¨ä»£ç åˆ—è¡¨
        """
        print("ğŸ¯ å¼€å§‹è‚¡ç¥¨ç­›é€‰...")
        
        if price_data.empty:
            return []
        
        qualified_stocks = []
        
        # æŒ‰è‚¡ç¥¨åˆ†ç»„åˆ†æ
        for ticker, group in price_data.groupby('ticker'):
            # 1. äº¤æ˜“å¤©æ•°æ£€æŸ¥
            trading_days = len(group)
            if trading_days < self.config['min_trading_days']:
                continue
            
            # 2. ç¼ºå¤±å€¼æ£€æŸ¥
            missing_ratio = group.isnull().sum().sum() / (len(group) * len(group.columns))
            if missing_ratio > self.config['max_missing_ratio']:
                continue
            
            # 3. STè‚¡ç¥¨æ£€æŸ¥ï¼ˆå¦‚æœæœ‰è‚¡ç¥¨ä¿¡æ¯ï¼‰
            if self.config['exclude_st'] and stock_info is not None:
                stock_name = stock_info[stock_info['ticker'] == ticker]['shortName'].iloc[0] if len(stock_info[stock_info['ticker'] == ticker]) > 0 else ''
                if 'ST' in stock_name or '*ST' in stock_name:
                    continue
            
            # 4. æ–°è‚¡æ£€æŸ¥ï¼ˆå¦‚æœæœ‰ä¸Šå¸‚æ—¥æœŸä¿¡æ¯ï¼‰
            if stock_info is not None and 'listDate' in stock_info.columns:
                list_info = stock_info[stock_info['ticker'] == ticker]
                if len(list_info) > 0:
                    list_date = pd.to_datetime(list_info['listDate'].iloc[0])
                    days_since_listing = (datetime.now() - list_date).days
                    if days_since_listing < self.config['exclude_new_days']:
                        continue
            
            # 5. ä»·æ ¼å’Œæˆäº¤é‡æ£€æŸ¥
            if 'closePrice' in group.columns and 'turnoverVol' in group.columns:
                avg_price = group['closePrice'].mean()
                avg_volume = group['turnoverVol'].mean()
                
                if (avg_price >= self.config['min_price'] and 
                    avg_price <= self.config['max_price'] and
                    avg_volume >= self.config['min_volume']):
                    qualified_stocks.append(ticker)
        
        print(f"âœ… è‚¡ç¥¨ç­›é€‰å®Œæˆ")
        print(f"   ğŸ“Š ç­›é€‰å‰: {price_data['ticker'].nunique()} åª")
        print(f"   ğŸ¯ ç­›é€‰å: {len(qualified_stocks)} åª")
        print(f"   ğŸ“‰ ç­›é€‰ç‡: {len(qualified_stocks)/price_data['ticker'].nunique():.2%}")
        
        return qualified_stocks
    
    def calculate_returns(self, price_data: pd.DataFrame) -> pd.DataFrame:
        """
        è®¡ç®—å¤šå‘¨æœŸæ”¶ç›Šç‡
        
        Args:
            price_data: ä»·æ ¼æ•°æ®
            
        Returns:
            åŒ…å«æ”¶ç›Šç‡çš„æ•°æ®
        """
        print("ğŸ“ˆ è®¡ç®—æ”¶ç›Šç‡æŒ‡æ ‡...")
        
        if price_data.empty or 'closePrice' not in price_data.columns:
            return price_data
        
        # å¤åˆ¶æ•°æ®
        data_with_returns = price_data.copy()
        
        # æŒ‰è‚¡ç¥¨åˆ†ç»„è®¡ç®—æ”¶ç›Šç‡
        for ticker, group in data_with_returns.groupby('ticker'):
            group = group.sort_values('tradeDate')
            
            # è®¡ç®—ä¸åŒå‘¨æœŸçš„æ”¶ç›Šç‡
            for period in self.config['return_periods']:
                col_name = f'return_{period}d'
                data_with_returns.loc[group.index, col_name] = (
                    group['closePrice'].pct_change(period)
                )
            
            # è®¡ç®—å¯¹æ•°æ”¶ç›Šç‡
            data_with_returns.loc[group.index, 'log_return'] = np.log(
                group['closePrice'] / group['closePrice'].shift(1)
            )
            
            # è®¡ç®—æ³¢åŠ¨ç‡ï¼ˆ20æ—¥æ»šåŠ¨ï¼‰
            returns = group['closePrice'].pct_change()
            data_with_returns.loc[group.index, 'volatility_20d'] = (
                returns.rolling(window=20).std() * np.sqrt(252)
            )
        
        print(f"âœ… æ”¶ç›Šç‡è®¡ç®—å®Œæˆ")
        print(f"   ğŸ“Š æ–°å¢åˆ—: {len(self.config['return_periods']) + 2} ä¸ª")
        
        return data_with_returns
    
    def normalize_features(self, data: pd.DataFrame, 
                          features: List[str] = None) -> pd.DataFrame:
        """
        ç‰¹å¾æ ‡å‡†åŒ–
        
        Args:
            data: è¾“å…¥æ•°æ®
            features: éœ€è¦æ ‡å‡†åŒ–çš„ç‰¹å¾åˆ—è¡¨
            
        Returns:
            æ ‡å‡†åŒ–åçš„æ•°æ®
        """
        print("ğŸ“Š ç‰¹å¾æ ‡å‡†åŒ–...")
        
        if not self.config['normalize_features'] or data.empty:
            return data
        
        # å¦‚æœæœªæŒ‡å®šç‰¹å¾ï¼Œè‡ªåŠ¨é€‰æ‹©æ•°å€¼ç‰¹å¾
        if features is None:
            features = data.select_dtypes(include=[np.number]).columns.tolist()
            # æ’é™¤IDå’Œæ—¥æœŸç›¸å…³åˆ—
            exclude_cols = ['ticker', 'tradeDate'] + [col for col in features if 'date' in col.lower()]
            features = [col for col in features if col not in exclude_cols]
        
        normalized_data = data.copy()
        
        # æŒ‰è‚¡ç¥¨åˆ†ç»„æ ‡å‡†åŒ–ï¼ˆé¿å…è·¨è‚¡ç¥¨æ ‡å‡†åŒ–ï¼‰
        if self.config['normalize_method'] == 'zscore':
            for feature in features:
                if feature in normalized_data.columns:
                    normalized_data[f'{feature}_norm'] = normalized_data.groupby('ticker')[feature].transform(
                        lambda x: (x - x.mean()) / x.std() if x.std() > 0 else 0
                    )
        
        elif self.config['normalize_method'] == 'minmax':
            for feature in features:
                if feature in normalized_data.columns:
                    normalized_data[f'{feature}_norm'] = normalized_data.groupby('ticker')[feature].transform(
                        lambda x: (x - x.min()) / (x.max() - x.min()) if x.max() > x.min() else 0
                    )
        
        print(f"âœ… ç‰¹å¾æ ‡å‡†åŒ–å®Œæˆ")
        print(f"   ğŸ“Š æ ‡å‡†åŒ–ç‰¹å¾: {len(features)} ä¸ª")
        
        return normalized_data
    
    def calculate_quality_score(self, price_data: pd.DataFrame) -> Dict[str, float]:
        """
        è®¡ç®—æ•°æ®è´¨é‡è¯„åˆ†
        
        Args:
            price_data: ä»·æ ¼æ•°æ®
            
        Returns:
            è´¨é‡è¯„åˆ†å­—å…¸
        """
        if price_data.empty:
            return {'overall_score': 0.0}
        
        scores = {}
        
        # 1. å®Œæ•´æ€§è¯„åˆ†ï¼ˆç¼ºå¤±å€¼æ¯”ä¾‹ï¼‰
        missing_ratio = price_data.isnull().sum().sum() / (len(price_data) * len(price_data.columns))
        scores['completeness'] = max(0, 1 - missing_ratio * 2)
        
        # 2. ä¸€è‡´æ€§è¯„åˆ†ï¼ˆå¼‚å¸¸å€¼æ¯”ä¾‹ï¼‰
        numeric_cols = price_data.select_dtypes(include=[np.number]).columns
        outlier_count = 0
        total_values = 0
        
        for col in numeric_cols:
            if col in price_data.columns:
                z_scores = np.abs(zscore(price_data[col].dropna()))
                outlier_count += np.sum(z_scores > self.config['outlier_threshold'])
                total_values += len(z_scores)
        
        outlier_ratio = outlier_count / total_values if total_values > 0 else 0
        scores['consistency'] = max(0, 1 - outlier_ratio * 5)
        
        # 3. è¿ç»­æ€§è¯„åˆ†ï¼ˆæ•°æ®è¦†ç›–ç‡ï¼‰
        if 'tradeDate' in price_data.columns:
            date_range = price_data['tradeDate'].max() - price_data['tradeDate'].min()
            actual_days = len(price_data['tradeDate'].unique())
            expected_days = date_range.days
            coverage_ratio = actual_days / expected_days if expected_days > 0 else 0
            scores['continuity'] = min(1.0, coverage_ratio * 1.5)
        else:
            scores['continuity'] = 0.8
        
        # 4. ç»¼åˆè¯„åˆ†
        scores['overall_score'] = (
            scores['completeness'] * 0.4 + 
            scores['consistency'] * 0.4 + 
            scores['continuity'] * 0.2
        )
        
        return scores
    
    def run_complete_pipeline(self, price_data: pd.DataFrame,
                            stock_info: pd.DataFrame = None) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´çš„æ•°æ®é¢„å¤„ç†æµæ°´çº¿
        
        Args:
            price_data: åŸå§‹ä»·æ ¼æ•°æ®
            stock_info: è‚¡ç¥¨åŸºç¡€ä¿¡æ¯
            
        Returns:
            å¤„ç†ç»“æœå­—å…¸
        """
        print("ğŸš€ å¯åŠ¨å®Œæ•´æ•°æ®é¢„å¤„ç†æµæ°´çº¿")
        print("=" * 50)
        
        start_time = datetime.now()
        results = {}
        
        try:
            # 1. æ•°æ®æ¸…æ´—
            clean_data = self.clean_price_data(price_data)
            results['clean_data'] = clean_data
            
            # 2. è‚¡ç¥¨ç­›é€‰
            qualified_stocks = self.filter_stocks(clean_data, stock_info)
            results['qualified_stocks'] = qualified_stocks
            
            # è¿‡æ»¤æ•°æ®åªä¿ç•™åˆæ ¼è‚¡ç¥¨
            if qualified_stocks:
                filtered_data = clean_data[clean_data['ticker'].isin(qualified_stocks)]
            else:
                filtered_data = clean_data
            
            # 3. è®¡ç®—æ”¶ç›Šç‡
            data_with_returns = self.calculate_returns(filtered_data)
            results['data_with_returns'] = data_with_returns
            
            # 4. ç‰¹å¾æ ‡å‡†åŒ–
            normalized_data = self.normalize_features(data_with_returns)
            results['normalized_data'] = normalized_data
            
            # 5. è´¨é‡è¯„åˆ†
            quality_scores = self.calculate_quality_score(normalized_data)
            results['quality_scores'] = quality_scores
            
            # 6. ç»Ÿè®¡ä¿¡æ¯
            processing_time = (datetime.now() - start_time).total_seconds()
            self.stats['processed_datasets'] += 1
            self.stats['total_processing_time'] += processing_time
            self.stats['quality_scores'].append(quality_scores['overall_score'])
            
            results['processing_info'] = {
                'processing_time': processing_time,
                'input_rows': len(price_data),
                'output_rows': len(normalized_data),
                'qualified_stocks': len(qualified_stocks),
                'quality_score': quality_scores['overall_score']
            }
            
            print(f"âœ… æ•°æ®é¢„å¤„ç†æµæ°´çº¿å®Œæˆ")
            print(f"   â±ï¸ å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’")
            print(f"   ğŸ“Š è´¨é‡è¯„åˆ†: {quality_scores['overall_score']:.3f}")
            print(f"   ğŸ¯ åˆæ ¼è‚¡ç¥¨: {len(qualified_stocks)} åª")
            
        except Exception as e:
            print(f"âŒ å¤„ç†æµæ°´çº¿é”™è¯¯: {e}")
            error_type = type(e).__name__
            self.stats['error_counts'][error_type] = self.stats['error_counts'].get(error_type, 0) + 1
            results['error'] = str(e)
        
        return results
    
    def get_processing_stats(self) -> Dict[str, Any]:
        """è·å–å¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'stats': self.stats,
            'config': self.config,
            'cache_info': self.get_cache_info(),
            'processing_history': self.processing_history[-10:]  # æœ€è¿‘10æ¬¡å¤„ç†è®°å½•
        }
    
    def get_cache_info(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ä¿¡æ¯"""
        cache_files = []
        total_size = 0
        
        try:
            for file_path in Path(self.cache_dir).glob("*.pkl"):
                size = file_path.stat().st_size
                cache_files.append({
                    'name': file_path.name,
                    'size': size,
                    'modified': datetime.fromtimestamp(file_path.stat().st_mtime).isoformat()
                })
                total_size += size
        except Exception as e:
            logger.warning(f"è·å–ç¼“å­˜ä¿¡æ¯å¤±è´¥: {e}")
        
        return {
            'cache_dir': self.cache_dir,
            'file_count': len(cache_files),
            'total_size_mb': total_size / (1024 * 1024),
            'files': cache_files[:5]  # åªè¿”å›å‰5ä¸ªæ–‡ä»¶ä¿¡æ¯
        }
    
    def reset_stats(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.stats = {
            'processed_datasets': 0,
            'total_processing_time': 0,
            'cache_hits': 0,
            'cache_misses': 0,
            'quality_scores': [],
            'error_counts': {}
        }
        self.processing_history = []
        print("ğŸ“Š ç»Ÿè®¡ä¿¡æ¯å·²é‡ç½®")


# ==========================================
# ğŸ­ å·¥å‚å‡½æ•°å’Œæ¨¡å—å¯¼å‡º
# ==========================================

def create_data_processor(config: Optional[Dict] = None) -> DataProcessor:
    """
    åˆ›å»ºæ•°æ®é¢„å¤„ç†å™¨å®ä¾‹çš„å·¥å‚å‡½æ•°
    
    Args:
        config: è‡ªå®šä¹‰é…ç½®å‚æ•°
        
    Returns:
        DataProcessorå®ä¾‹
    """
    return DataProcessor(config)

# åˆ›å»ºé»˜è®¤å…¨å±€å®ä¾‹
default_processor = DataProcessor()

# æ¨¡å—å¯¼å‡º
__all__ = [
    'DataProcessor',
    'create_data_processor', 
    'default_processor'
]

if __name__ == "__main__":
    print("ğŸ”§ é«˜çº§æ•°æ®é¢„å¤„ç†å™¨ v2.0 æ¨¡å—åŠ è½½å®Œæˆ")
    print("ğŸ“˜ ä½¿ç”¨ç¤ºä¾‹:")
    print("   from data_processor import DataProcessor, create_data_processor")
    print("   processor = create_data_processor()")
    print("   results = processor.run_complete_pipeline(price_data)")
    print("")
    print("ğŸ’¡ åŠŸèƒ½ç‰¹æ€§:")
    print("   ğŸ§¹ æ™ºèƒ½æ•°æ®æ¸…æ´—å’Œå¼‚å¸¸å€¼å¤„ç†")
    print("   ğŸ“Š å¤šç»´åº¦è‚¡ç¥¨ç­›é€‰å’Œè´¨é‡è¯„ä¼°")
    print("   ğŸ“„ é«˜çº§æ•°æ®æ ‡å‡†åŒ–å’Œç‰¹å¾ç¼©æ”¾") 
    print("   ğŸ“ˆ å¤šå‘¨æœŸæ”¶ç›Šç‡è®¡ç®—å’Œé£é™©æŒ‡æ ‡")
    print("   ğŸ’¾ é«˜æ•ˆç¼“å­˜æœºåˆ¶å’Œæ‰¹å¤„ç†")
    print("   ğŸ¯ å®Œæ•´çš„æ•°æ®è´¨é‡ç›‘æ§ä½“ç³»")