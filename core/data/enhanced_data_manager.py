#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆæ•°æ®ç®¡ç†å™¨
===============

é›†æˆæ‰€æœ‰æ•°æ®åŠŸèƒ½çš„ç»Ÿä¸€ç®¡ç†å™¨
"""

from typing import Dict, List, Optional, Union, Any, Tuple
import pandas as pd
from datetime import datetime, date, timedelta
import logging
from pathlib import Path

# å¯¼å…¥æ•°æ®ç»„ä»¶
from .adapters.data_source_manager import DataSourceManager
from .quality_checker import DataQualityChecker
from .cache_manager import SmartCacheManager

# å¯¼å…¥ä¸‹è½½å™¨
from .downloaders.a_shares_downloader import ASharesDownloader
from .downloaders.strategy_downloader import StrategyDownloader
from .downloaders.indicator_downloader import IndicatorDownloader

# å¯¼å…¥åŸæœ‰ç»„ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
try:
    from .data_processor import DataProcessor
    from .feature_engineer import FeatureEngineer
    LEGACY_COMPONENTS_AVAILABLE = True
except ImportError:
    LEGACY_COMPONENTS_AVAILABLE = False

logger = logging.getLogger(__name__)

class EnhancedDataManager:
    """å¢å¼ºç‰ˆæ•°æ®ç®¡ç†å™¨
    
    ç»Ÿä¸€ç®¡ç†æ‰€æœ‰æ•°æ®ç›¸å…³åŠŸèƒ½ï¼š
    - å¤šæ•°æ®æºç®¡ç†
    - æ™ºèƒ½ç¼“å­˜ç³»ç»Ÿ
    - æ•°æ®è´¨é‡æ£€æŸ¥
    - æ•°æ®å¤„ç†å’Œç‰¹å¾å·¥ç¨‹
    - å®Œæ•´çš„æ•°æ®æµæ°´çº¿
    """
    
    def __init__(self, config: Optional[Dict] = None):
        """åˆå§‹åŒ–å¢å¼ºç‰ˆæ•°æ®ç®¡ç†å™¨
        
        Args:
            config: é…ç½®å‚æ•°
        """
        self.config = config or {}
        
        # åˆå§‹åŒ–å„ä¸ªç»„ä»¶
        logger.info("ğŸš€ åˆå§‹åŒ–å¢å¼ºç‰ˆæ•°æ®ç®¡ç†å™¨...")
        
        # æ•°æ®æºç®¡ç†å™¨
        data_source_config = self.config.get('data_sources', {})
        self.data_source_manager = DataSourceManager(data_source_config)
        
        # ç¼“å­˜ç®¡ç†å™¨
        cache_config = self.config.get('cache', {})
        self.cache_manager = SmartCacheManager(cache_config)
        
        # æ•°æ®è´¨é‡æ£€æŸ¥å™¨
        quality_config = self.config.get('quality', {})
        self.quality_checker = DataQualityChecker(quality_config)
        
        # åˆå§‹åŒ–ä¸‹è½½å™¨
        downloader_config = {**self.config, 'data_dir': self.config.get('data_dir', './data')}
        self.a_shares_downloader = ASharesDownloader(downloader_config)
        self.strategy_downloader = StrategyDownloader(downloader_config)
        self.indicator_downloader = IndicatorDownloader(downloader_config)
        
        # æ•°æ®å¤„ç†å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        self.data_processor = None
        self.feature_engineer = None
        
        if LEGACY_COMPONENTS_AVAILABLE:
            try:
                self.data_processor = DataProcessor(self.config.get('processor', {}))
                self.feature_engineer = FeatureEngineer(self.config.get('feature_engineer', {}))
                logger.info("âœ… é—ç•™æ•°æ®ç»„ä»¶åŠ è½½æˆåŠŸ")
            except Exception as e:
                logger.warning(f"âš ï¸ é—ç•™æ•°æ®ç»„ä»¶åŠ è½½å¤±è´¥: {str(e)}")
        
        # åˆå§‹åŒ–æ•°æ®æºè¿æ¥
        self._initialize_data_sources()
        
        logger.info("âœ… å¢å¼ºç‰ˆæ•°æ®ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _initialize_data_sources(self):
        """åˆå§‹åŒ–æ•°æ®æºè¿æ¥"""
        logger.info("ğŸ”— æµ‹è¯•æ•°æ®æºè¿æ¥...")
        
        connection_results = self.data_source_manager.test_all_connections()
        available_sources = self.data_source_manager.get_available_sources()
        
        logger.info(f"âœ… æ•°æ®æºåˆå§‹åŒ–å®Œæˆï¼Œå¯ç”¨æ•°æ®æº: {available_sources}")
        
        if not available_sources:
            logger.warning("âš ï¸ æ²¡æœ‰å¯ç”¨çš„æ•°æ®æºï¼ŒæŸäº›åŠŸèƒ½å¯èƒ½å—é™")
    
    def get_stock_list(self, 
                      market: Optional[str] = None,
                      use_cache: bool = True,
                      **kwargs) -> pd.DataFrame:
        """è·å–è‚¡ç¥¨åˆ—è¡¨
        
        Args:
            market: å¸‚åœºä»£ç 
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            pd.DataFrame: è‚¡ç¥¨åˆ—è¡¨
        """
        # ç”Ÿæˆç¼“å­˜é”®
        cache_params = {'market': market, **kwargs}
        
        # å°è¯•ä»ç¼“å­˜è·å–
        if use_cache:
            cached_data = self.cache_manager.get('stock_list', cache_params)
            if cached_data is not None:
                logger.info(f"âœ… ä»ç¼“å­˜è·å–è‚¡ç¥¨åˆ—è¡¨ï¼Œå…±{len(cached_data)}åªè‚¡ç¥¨")
                return cached_data
        
        # ä»æ•°æ®æºè·å–
        logger.info(f"ğŸ“Š è·å–è‚¡ç¥¨åˆ—è¡¨ (å¸‚åœº: {market or 'å…¨éƒ¨'})")
        data = self.data_source_manager.get_stock_list(market=market, **kwargs)
        
        if not data.empty:
            # æ•°æ®è´¨é‡æ£€æŸ¥
            quality_result = self.quality_checker.check_data_types(
                data, 
                expected_types={'symbol': 'object', 'name': 'object'}
            )
            
            if quality_result.get('type_issues'):
                logger.warning(f"âš ï¸ è‚¡ç¥¨åˆ—è¡¨æ•°æ®ç±»å‹é—®é¢˜: {quality_result['type_issues']}")
            
            # ç¼“å­˜æ•°æ®
            if use_cache:
                self.cache_manager.put('stock_list', cache_params, data, expire_hours=24)
        
        return data
    
    def get_price_data(self,
                      symbols: Union[str, List[str]],
                      start_date: Union[str, date, datetime],
                      end_date: Union[str, date, datetime],
                      use_cache: bool = True,
                      quality_check: bool = True,
                      **kwargs) -> pd.DataFrame:
        """è·å–ä»·æ ¼æ•°æ®
        
        Args:
            symbols: è‚¡ç¥¨ä»£ç æˆ–ä»£ç åˆ—è¡¨
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            quality_check: æ˜¯å¦è¿›è¡Œè´¨é‡æ£€æŸ¥
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            pd.DataFrame: ä»·æ ¼æ•°æ®
        """
        # æ ‡å‡†åŒ–å‚æ•°
        if isinstance(symbols, str):
            symbols = [symbols]
        
        # ç”Ÿæˆç¼“å­˜é”®
        cache_params = {
            'symbols': sorted(symbols),
            'start_date': str(start_date),
            'end_date': str(end_date),
            **kwargs
        }
        
        # å°è¯•ä»ç¼“å­˜è·å–
        if use_cache:
            cached_data = self.cache_manager.get('price_data', cache_params)
            if cached_data is not None:
                logger.info(f"âœ… ä»ç¼“å­˜è·å–ä»·æ ¼æ•°æ®ï¼Œå…±{len(cached_data)}æ¡è®°å½•")
                return cached_data
        
        # ä»æ•°æ®æºè·å–
        logger.info(f"ğŸ“Š è·å–ä»·æ ¼æ•°æ®: {len(symbols)}åªè‚¡ç¥¨, {start_date} è‡³ {end_date}")
        data = self.data_source_manager.get_price_data(
            symbols=symbols,
            start_date=start_date,
            end_date=end_date,
            **kwargs
        )
        
        if not data.empty:
            # æ•°æ®è´¨é‡æ£€æŸ¥
            if quality_check:
                self._perform_price_data_quality_check(data)
            
            # ç¼“å­˜æ•°æ®
            if use_cache:
                # æ ¹æ®æ•°æ®é‡è®¾ç½®ç¼“å­˜è¿‡æœŸæ—¶é—´
                expire_hours = 1 if len(data) > 10000 else 6
                self.cache_manager.put('price_data', cache_params, data, expire_hours=expire_hours)
        
        return data
    
    def get_financial_data(self,
                          symbols: Union[str, List[str]],
                          start_date: Union[str, date, datetime],
                          end_date: Union[str, date, datetime],
                          report_type: str = 'annual',
                          use_cache: bool = True,
                          **kwargs) -> pd.DataFrame:
        """è·å–è´¢åŠ¡æ•°æ®
        
        Args:
            symbols: è‚¡ç¥¨ä»£ç æˆ–ä»£ç åˆ—è¡¨
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            report_type: æŠ¥å‘Šç±»å‹
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            pd.DataFrame: è´¢åŠ¡æ•°æ®
        """
        # æ ‡å‡†åŒ–å‚æ•°
        if isinstance(symbols, str):
            symbols = [symbols]
        
        # ç”Ÿæˆç¼“å­˜é”®
        cache_params = {
            'symbols': sorted(symbols),
            'start_date': str(start_date),
            'end_date': str(end_date),
            'report_type': report_type,
            **kwargs
        }
        
        # å°è¯•ä»ç¼“å­˜è·å–
        if use_cache:
            cached_data = self.cache_manager.get('financial_data', cache_params)
            if cached_data is not None:
                logger.info(f"âœ… ä»ç¼“å­˜è·å–è´¢åŠ¡æ•°æ®ï¼Œå…±{len(cached_data)}æ¡è®°å½•")
                return cached_data
        
        # ä»æ•°æ®æºè·å–
        logger.info(f"ğŸ“Š è·å–è´¢åŠ¡æ•°æ®: {len(symbols)}åªè‚¡ç¥¨, æŠ¥å‘Šç±»å‹: {report_type}")
        data = self.data_source_manager.get_financial_data(
            symbols=symbols,
            start_date=start_date,
            end_date=end_date,
            report_type=report_type,
            **kwargs
        )
        
        if not data.empty:
            # ç¼“å­˜æ•°æ® (è´¢åŠ¡æ•°æ®æ›´æ–°é¢‘ç‡è¾ƒä½ï¼Œå¯ä»¥ç¼“å­˜æ›´é•¿æ—¶é—´)
            if use_cache:
                self.cache_manager.put('financial_data', cache_params, data, expire_hours=48)
        
        return data
    
    def _perform_price_data_quality_check(self, data: pd.DataFrame):
        """æ‰§è¡Œä»·æ ¼æ•°æ®è´¨é‡æ£€æŸ¥"""
        try:
            # æ£€æŸ¥ç¼ºå¤±æ•°æ®
            missing_result = self.quality_checker.check_missing_data(
                data, 
                critical_columns=['date', 'symbol', 'close']
            )
            
            if missing_result.get('critical_issues'):
                logger.warning(f"âš ï¸ ä»·æ ¼æ•°æ®å…³é”®åˆ—ç¼ºå¤±: {missing_result['critical_issues']}")
            
            # æ£€æŸ¥ä»·æ ¼ä¸€è‡´æ€§
            consistency_result = self.quality_checker.check_price_data_consistency(data)
            
            if consistency_result.get('consistency_issues'):
                logger.warning(f"âš ï¸ ä»·æ ¼æ•°æ®ä¸€è‡´æ€§é—®é¢˜: {consistency_result['consistency_issues']}")
            
            # æ£€æŸ¥å¼‚å¸¸å€¼
            outlier_result = self.quality_checker.check_outliers(
                data,
                numeric_columns=['open', 'high', 'low', 'close', 'volume']
            )
            
            high_outlier_cols = []
            for col, result in outlier_result.get('outlier_summary', {}).items():
                if result.get('outlier_rate', 0) > 0.05:  # è¶…è¿‡5%å¼‚å¸¸å€¼
                    high_outlier_cols.append(col)
            
            if high_outlier_cols:
                logger.warning(f"âš ï¸ ä»·æ ¼æ•°æ®å¼‚å¸¸å€¼è¾ƒå¤šçš„åˆ—: {high_outlier_cols}")
                
        except Exception as e:
            logger.error(f"âŒ ä»·æ ¼æ•°æ®è´¨é‡æ£€æŸ¥å¤±è´¥: {str(e)}")
    
    def generate_features(self, 
                         price_data: pd.DataFrame,
                         feature_types: Optional[List[str]] = None,
                         use_cache: bool = True) -> pd.DataFrame:
        """ç”Ÿæˆç‰¹å¾æ•°æ®
        
        Args:
            price_data: ä»·æ ¼æ•°æ®
            feature_types: ç‰¹å¾ç±»å‹åˆ—è¡¨
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            
        Returns:
            pd.DataFrame: ç‰¹å¾æ•°æ®
        """
        if self.feature_engineer is None:
            logger.error("âŒ ç‰¹å¾å·¥ç¨‹å™¨ä¸å¯ç”¨")
            return pd.DataFrame()
        
        # ç”Ÿæˆç¼“å­˜é”®
        data_hash = pd.util.hash_pandas_object(price_data).sum()
        cache_params = {
            'data_hash': str(data_hash),
            'feature_types': sorted(feature_types or [])
        }
        
        # å°è¯•ä»ç¼“å­˜è·å–
        if use_cache:
            cached_features = self.cache_manager.get('features', cache_params)
            if cached_features is not None:
                logger.info(f"âœ… ä»ç¼“å­˜è·å–ç‰¹å¾æ•°æ®ï¼Œå…±{len(cached_features)}æ¡è®°å½•")
                return cached_features
        
        # ç”Ÿæˆç‰¹å¾
        logger.info("ğŸ”§ ç”Ÿæˆç‰¹å¾æ•°æ®...")
        
        try:
            if hasattr(self.feature_engineer, 'generate_features'):
                features = self.feature_engineer.generate_features(
                    price_data, 
                    feature_types=feature_types
                )
            else:
                # ä½¿ç”¨æ—§ç‰ˆæœ¬æ–¹æ³•
                features = self.feature_engineer.generate_all_features(price_data)
            
            if not features.empty:
                # ç¼“å­˜ç‰¹å¾æ•°æ®
                if use_cache:
                    self.cache_manager.put('features', cache_params, features, expire_hours=12)
                
                logger.info(f"âœ… ç‰¹å¾ç”Ÿæˆå®Œæˆï¼Œå…±{len(features)}æ¡è®°å½•ï¼Œ{len(features.columns)}ä¸ªç‰¹å¾")
            
            return features
            
        except Exception as e:
            logger.error(f"âŒ ç‰¹å¾ç”Ÿæˆå¤±è´¥: {str(e)}")
            return pd.DataFrame()
    
    def run_complete_pipeline(self,
                             symbols: Union[str, List[str]],
                             start_date: Union[str, date, datetime],
                             end_date: Union[str, date, datetime],
                             include_features: bool = True,
                             quality_report: bool = False) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„æ•°æ®æµæ°´çº¿
        
        Args:
            symbols: è‚¡ç¥¨ä»£ç æˆ–ä»£ç åˆ—è¡¨
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            include_features: æ˜¯å¦ç”Ÿæˆç‰¹å¾
            quality_report: æ˜¯å¦ç”Ÿæˆè´¨é‡æŠ¥å‘Š
            
        Returns:
            Dict: å®Œæ•´çš„æ•°æ®ç»“æœ
        """
        logger.info("ğŸš€ å¯åŠ¨å®Œæ•´æ•°æ®æµæ°´çº¿...")
        
        result = {
            'symbols': symbols,
            'date_range': (str(start_date), str(end_date)),
            'timestamp': datetime.now().isoformat(),
            'price_data': pd.DataFrame(),
            'features': pd.DataFrame(),
            'quality_report': None,
            'cache_stats': None,
            'data_source_status': None
        }
        
        try:
            # 1. è·å–ä»·æ ¼æ•°æ®
            logger.info("ğŸ“Š æ­¥éª¤1: è·å–ä»·æ ¼æ•°æ®")
            price_data = self.get_price_data(
                symbols=symbols,
                start_date=start_date,
                end_date=end_date,
                quality_check=True
            )
            
            if price_data.empty:
                logger.error("âŒ æœªè·å–åˆ°ä»·æ ¼æ•°æ®ï¼Œæµæ°´çº¿ç»ˆæ­¢")
                return result
            
            result['price_data'] = price_data
            logger.info(f"âœ… ä»·æ ¼æ•°æ®è·å–å®Œæˆ: {len(price_data)}æ¡è®°å½•")
            
            # 2. ç”Ÿæˆç‰¹å¾ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if include_features and self.feature_engineer is not None:
                logger.info("ğŸ”§ æ­¥éª¤2: ç”Ÿæˆç‰¹å¾æ•°æ®")
                features = self.generate_features(price_data)
                result['features'] = features
                
                if not features.empty:
                    logger.info(f"âœ… ç‰¹å¾ç”Ÿæˆå®Œæˆ: {len(features)}æ¡è®°å½•ï¼Œ{len(features.columns)}ä¸ªç‰¹å¾")
                else:
                    logger.warning("âš ï¸ ç‰¹å¾ç”Ÿæˆå¤±è´¥æˆ–æ— ç‰¹å¾æ•°æ®")
            
            # 3. ç”Ÿæˆè´¨é‡æŠ¥å‘Šï¼ˆå¦‚æœéœ€è¦ï¼‰
            if quality_report:
                logger.info("ğŸ“Š æ­¥éª¤3: ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Š")
                quality_result = self.quality_checker.generate_quality_report(
                    price_data, 
                    "ä»·æ ¼æ•°æ®è´¨é‡æŠ¥å‘Š"
                )
                result['quality_report'] = quality_result
                
                logger.info(f"âœ… è´¨é‡æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼Œæ•´ä½“å¾—åˆ†: {quality_result['overall_score']:.2f}")
            
            # 4. è·å–ç¼“å­˜ç»Ÿè®¡
            result['cache_stats'] = self.cache_manager.get_cache_stats()
            
            # 5. è·å–æ•°æ®æºçŠ¶æ€
            result['data_source_status'] = self.data_source_manager.get_status_report()
            
            logger.info("ğŸ‰ å®Œæ•´æ•°æ®æµæ°´çº¿æ‰§è¡ŒæˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®æµæ°´çº¿æ‰§è¡Œå¤±è´¥: {str(e)}")
            result['error'] = str(e)
        
        return result
    
    def get_cache_statistics(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        return self.cache_manager.get_cache_stats()
    
    def get_data_source_status(self) -> Dict[str, Any]:
        """è·å–æ•°æ®æºçŠ¶æ€"""
        return self.data_source_manager.get_status_report()
    
    def clear_cache(self, cache_type: Optional[str] = None):
        """æ¸…ç†ç¼“å­˜
        
        Args:
            cache_type: ç¼“å­˜ç±»å‹ï¼ŒNoneä¸ºæ¸…ç†å…¨éƒ¨
        """
        logger.info(f"ğŸ§¹ æ¸…ç†ç¼“å­˜ (ç±»å‹: {cache_type or 'å…¨éƒ¨'})")
        self.cache_manager.clear_cache(cache_type)
    
    def cleanup_expired_cache(self):
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        logger.info("ğŸ§¹ æ¸…ç†è¿‡æœŸç¼“å­˜")
        self.cache_manager.cleanup_expired()
    
    def validate_data_pipeline(self) -> Dict[str, Any]:
        """éªŒè¯æ•°æ®æµæ°´çº¿çŠ¶æ€
        
        Returns:
            Dict: éªŒè¯ç»“æœ
        """
        logger.info("ğŸ” éªŒè¯æ•°æ®æµæ°´çº¿çŠ¶æ€...")
        
        validation_result = {
            'timestamp': datetime.now().isoformat(),
            'overall_status': 'unknown',
            'components': {},
            'recommendations': []
        }
        
        # æ£€æŸ¥æ•°æ®æºç®¡ç†å™¨
        available_sources = self.data_source_manager.get_available_sources()
        validation_result['components']['data_sources'] = {
            'status': 'ok' if available_sources else 'error',
            'available_count': len(available_sources),
            'available_sources': available_sources
        }
        
        if not available_sources:
            validation_result['recommendations'].append("æ²¡æœ‰å¯ç”¨çš„æ•°æ®æºï¼Œè¯·æ£€æŸ¥APIé…ç½®")
        
        # æ£€æŸ¥ç¼“å­˜ç®¡ç†å™¨
        cache_stats = self.cache_manager.get_cache_stats()
        validation_result['components']['cache'] = {
            'status': 'ok',
            'hit_rate': cache_stats['statistics']['hit_rate'],
            'memory_usage_mb': cache_stats['memory_cache']['usage_mb']
        }
        
        # æ£€æŸ¥è´¨é‡æ£€æŸ¥å™¨
        validation_result['components']['quality_checker'] = {
            'status': 'ok' if self.quality_checker else 'error'
        }
        
        # æ£€æŸ¥ç‰¹å¾å·¥ç¨‹å™¨
        validation_result['components']['feature_engineer'] = {
            'status': 'ok' if self.feature_engineer else 'warning',
            'message': 'ç‰¹å¾å·¥ç¨‹å™¨ä¸å¯ç”¨' if not self.feature_engineer else 'æ­£å¸¸'
        }
        
        # æ•´ä½“çŠ¶æ€è¯„ä¼°
        component_statuses = [comp['status'] for comp in validation_result['components'].values()]
        if 'error' in component_statuses:
            validation_result['overall_status'] = 'error'
        elif 'warning' in component_statuses:
            validation_result['overall_status'] = 'warning'
        else:
            validation_result['overall_status'] = 'ok'
        
        logger.info(f"âœ… æ•°æ®æµæ°´çº¿éªŒè¯å®Œæˆï¼ŒçŠ¶æ€: {validation_result['overall_status']}")
        return validation_result
    
    # ===================================
    # æ•°æ®ä¸‹è½½åŠŸèƒ½
    # ===================================
    
    def download_a_shares_data(self, 
                              symbols: Optional[List[str]] = None,
                              market: Optional[str] = None,
                              start_date: Union[str, datetime] = None,
                              end_date: Union[str, datetime] = None,
                              resume: bool = True) -> Dict[str, int]:
        """ä¸‹è½½Aè‚¡æ•°æ®
        
        Args:
            symbols: è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™ä¸‹è½½æ‰€æœ‰è‚¡ç¥¨
            market: å¸‚åœºä»£ç 
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            resume: æ˜¯å¦æ–­ç‚¹ç»­ä¼ 
            
        Returns:
            ä¸‹è½½ç»Ÿè®¡ç»“æœ
        """
        logger.info("ğŸš€ å¼€å§‹ä¸‹è½½Aè‚¡æ•°æ®...")
        
        if symbols is not None:
            return self.a_shares_downloader.download_batch(symbols, start_date, end_date, resume)
        else:
            return self.a_shares_downloader.download_all(market, start_date, end_date, resume)
    
    def download_strategy_data(self,
                             symbols: Optional[List[str]] = None,
                             data_types: List[str] = None) -> Dict[str, Dict[str, int]]:
        """ä¸‹è½½ç­–ç•¥æ•°æ®
        
        Args:
            symbols: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            data_types: æ•°æ®ç±»å‹åˆ—è¡¨ ['capital_flow', 'market_sentiment', etc.]
            
        Returns:
            å„ç±»å‹æ•°æ®çš„ä¸‹è½½ç»Ÿè®¡ç»“æœ
        """
        logger.info("ğŸš€ å¼€å§‹ä¸‹è½½ç­–ç•¥æ•°æ®...")
        return self.strategy_downloader.download_all_strategy_data(symbols, data_types)
    
    def download_indicators_data(self,
                               symbols: List[str],
                               indicators: List[str] = None) -> Dict[str, int]:
        """ä¸‹è½½/è®¡ç®—æŠ€æœ¯æŒ‡æ ‡æ•°æ®
        
        Args:
            symbols: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            indicators: æŠ€æœ¯æŒ‡æ ‡åˆ—è¡¨
            
        Returns:
            ä¸‹è½½ç»Ÿè®¡ç»“æœ
        """
        logger.info("ğŸš€ å¼€å§‹è®¡ç®—æŠ€æœ¯æŒ‡æ ‡æ•°æ®...")
        return self.indicator_downloader.download_indicators_batch(symbols, indicators)
    
    def get_download_status(self) -> Dict[str, Any]:
        """è·å–å…¨éƒ¨ä¸‹è½½çŠ¶æ€æŠ¥å‘Š"""
        return {
            'a_shares': self.a_shares_downloader.get_download_status(),
            'strategy': self.strategy_downloader.get_download_status(),
            'indicators': self.indicator_downloader.get_supported_indicators_info(),
            'data_sources': self.data_source_manager.get_status_report(),
            'cache': self.cache_manager.get_cache_stats()
        }
    
    def cleanup_all_cache(self):
        """æ¸…ç†æ‰€æœ‰ç¼“å­˜"""
        logger.info("ğŸ§¹ æ¸…ç†æ‰€æœ‰ç¼“å­˜...")
        self.cache_manager.clear_cache()
        self.a_shares_downloader.cleanup_cache()
        self.strategy_downloader.cleanup_cache()
        self.indicator_downloader.cleanup_cache()
        logger.info("âœ… æ‰€æœ‰ç¼“å­˜æ¸…ç†å®Œæˆ")
    
    def __enter__(self):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        # æ¸…ç†èµ„æº
        if hasattr(self, 'data_source_manager'):
            self.data_source_manager.cleanup()
        
        logger.info("âœ… å¢å¼ºç‰ˆæ•°æ®ç®¡ç†å™¨èµ„æºæ¸…ç†å®Œæˆ")