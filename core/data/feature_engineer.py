#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®Œæ•´ç‰¹å¾å·¥ç¨‹å™¨ - feature_engineer.py
=====================================

ä¸“ä¸ºé‡åŒ–äº¤æ˜“æ¡†æ¶è®¾è®¡çš„é«˜æ€§èƒ½ç‰¹å¾å·¥ç¨‹å™¨ï¼ŒåŒ…å«ï¼š
- ğŸ“Š 60+ æŠ€æœ¯æŒ‡æ ‡ç®—æ³•å®ç°
- ğŸ”¬ å¤šç»´åº¦å› å­ç‰¹å¾ç”Ÿæˆ
- ğŸ“ˆ è‡ªå®šä¹‰æŒ‡æ ‡æ”¯æŒ
- ğŸ’¾ é«˜æ•ˆç¼“å­˜æœºåˆ¶
- ğŸ¯ æ‰¹é‡ç‰¹å¾è®¡ç®—
- ğŸ“‹ ç‰¹å¾é‡è¦æ€§è¯„ä¼°

ç‰ˆæœ¬: 2.0.0
æ›´æ–°æ—¶é—´: 2024-08-26
å…¼å®¹ç¯å¢ƒ: VSCode + JupyterNote + ä¼˜çŸ¿API
"""

import os
import warnings
import numpy as np
import pandas as pd
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional, Union, Any, Callable
from pathlib import Path
import hashlib
import pickle
import json

# ç§‘å­¦è®¡ç®—åº“
from scipy import stats
from scipy.stats import zscore, skew, kurtosis
from scipy.signal import argrelextrema

# å°è¯•å¯¼å…¥TA-Libï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨å†…ç½®ç®—æ³•
try:
    import talib
    TALIB_AVAILABLE = True
    print("ğŸ“Š TA-Libåº“å·²åŠ è½½")
except ImportError:
    TALIB_AVAILABLE = False
    print("âš ï¸ TA-Libæœªå®‰è£…ï¼Œå°†ä½¿ç”¨å†…ç½®æŠ€æœ¯æŒ‡æ ‡ç®—æ³•")

# æŠ‘åˆ¶è­¦å‘Š
warnings.filterwarnings('ignore')

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

print("ğŸ”¬ ç‰¹å¾å·¥ç¨‹å™¨æ¨¡å—åŠ è½½ä¸­...")


class FeatureEngineer:
    """
    å®Œæ•´ç‰¹å¾å·¥ç¨‹å™¨ - é›†æˆ60+æŠ€æœ¯æŒ‡æ ‡å’Œå› å­ç‰¹å¾
    """
    
    def __init__(self, price_data: Optional[pd.DataFrame] = None, 
                 config: Optional[Dict] = None):
        """
        åˆå§‹åŒ–ç‰¹å¾å·¥ç¨‹å™¨
        
        Args:
            price_data: ä»·æ ¼æ•°æ®DataFrameï¼ŒåŒ…å«OHLCVåˆ—
            config: é…ç½®å‚æ•°å­—å…¸
        """
        self.price_data = price_data
        self.config = config or self._get_default_config()
        self.cache_dir = self.config.get('cache_dir', './cache')
        os.makedirs(self.cache_dir, exist_ok=True)
        
        # ç‰¹å¾ç¼“å­˜
        self.feature_cache = {}
        
        # æŠ€æœ¯æŒ‡æ ‡å‚æ•°
        self.indicator_params = self._get_default_indicator_params()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'generated_features': 0,
            'processing_time': 0,
            'cache_hits': 0,
            'cache_misses': 0,
            'feature_importance': {}
        }
        
        print("ğŸ› ï¸ ç‰¹å¾å·¥ç¨‹å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   ğŸ“ ç¼“å­˜ç›®å½•: {self.cache_dir}")
        print(f"   ğŸ”§ TA-Libå¯ç”¨: {'âœ…' if TALIB_AVAILABLE else 'âŒ'}")
    
    def _get_default_config(self) -> Dict:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            'cache_dir': './cache',
            'enable_cache': True,
            'cache_expire_hours': 24,
            'batch_size': 1000,
            'n_jobs': 1,
            'feature_selection': True,
            'correlation_threshold': 0.95,
        }
    
    def _get_default_indicator_params(self) -> Dict:
        """è·å–é»˜è®¤æŠ€æœ¯æŒ‡æ ‡å‚æ•°"""
        return {
            # ç§»åŠ¨å¹³å‡å‚æ•°
            'ma_periods': [5, 10, 20, 60],
            'ema_periods': [12, 26],
            
            # æ³¢åŠ¨æ€§æŒ‡æ ‡
            'bb_period': 20,
            'bb_std': 2,
            'atr_period': 14,
            
            # åŠ¨é‡æŒ‡æ ‡
            'rsi_period': 14,
            'macd_fast': 12,
            'macd_slow': 26,
            'macd_signal': 9,
            
            # æˆäº¤é‡æŒ‡æ ‡
            'volume_ma_period': 20,
            'vwap_period': 20,
        }
    
    def _generate_cache_key(self, *args) -> str:
        """ç”Ÿæˆç¼“å­˜é”®å€¼"""
        key_str = '_'.join(str(arg) for arg in args)
        return hashlib.md5(key_str.encode()).hexdigest()
    
    def _load_from_cache(self, cache_key: str):
        """ä»ç¼“å­˜åŠ è½½ç‰¹å¾"""
        if not self.config['enable_cache']:
            return None
        
        cache_path = os.path.join(self.cache_dir, f"features_{cache_key}.pkl")
        
        try:
            if os.path.exists(cache_path):
                file_time = datetime.fromtimestamp(os.path.getmtime(cache_path))
                expire_time = datetime.now() - timedelta(hours=self.config['cache_expire_hours'])
                
                if file_time > expire_time:
                    with open(cache_path, 'rb') as f:
                        self.stats['cache_hits'] += 1
                        return pickle.load(f)
        except Exception as e:
            logger.warning(f"ç‰¹å¾ç¼“å­˜åŠ è½½å¤±è´¥: {e}")
        
        self.stats['cache_misses'] += 1
        return None
    
    def _save_to_cache(self, features, cache_key: str):
        """ä¿å­˜ç‰¹å¾åˆ°ç¼“å­˜"""
        if not self.config['enable_cache']:
            return
        
        cache_path = os.path.join(self.cache_dir, f"features_{cache_key}.pkl")
        
        try:
            with open(cache_path, 'wb') as f:
                pickle.dump(features, f)
        except Exception as e:
            logger.warning(f"ç‰¹å¾ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
    
    # ==========================================
    # ä»·æ ¼ç›¸å…³ç‰¹å¾
    # ==========================================
    
    def generate_price_features(self, data: pd.DataFrame = None) -> pd.DataFrame:
        """ç”Ÿæˆä»·æ ¼ç›¸å…³ç‰¹å¾"""
        data = data if data is not None else self.price_data
        if data is None or data.empty:
            return pd.DataFrame()
        
        print("ğŸ’° ç”Ÿæˆä»·æ ¼ç‰¹å¾...")
        features = data.copy()
        
        required_cols = ['openPrice', 'highestPrice', 'lowestPrice', 'closePrice']
        if not all(col in features.columns for col in required_cols):
            print("âš ï¸ ç¼ºå°‘å¿…è¦çš„ä»·æ ¼åˆ—")
            return features
        
        # æŒ‰è‚¡ç¥¨åˆ†ç»„è®¡ç®—ç‰¹å¾
        for ticker, group in features.groupby('ticker'):
            group = group.sort_values('tradeDate')
            idx = group.index
            
            # åŸºç¡€ä»·æ ¼ç‰¹å¾
            features.loc[idx, 'price_range'] = group['highestPrice'] - group['lowestPrice']
            features.loc[idx, 'price_gap'] = group['openPrice'] - group['closePrice'].shift(1)
            features.loc[idx, 'upper_shadow'] = group['highestPrice'] - np.maximum(group['openPrice'], group['closePrice'])
            features.loc[idx, 'lower_shadow'] = np.minimum(group['openPrice'], group['closePrice']) - group['lowestPrice']
            
            # æ”¶ç›Šç‡ç‰¹å¾
            features.loc[idx, 'daily_return'] = group['closePrice'].pct_change()
            features.loc[idx, 'log_return'] = np.log(group['closePrice'] / group['closePrice'].shift(1))
            
            # æ³¢åŠ¨ç‡ç‰¹å¾ï¼ˆå¤šå‘¨æœŸï¼‰
            for window in [5, 10, 20]:
                returns = group['closePrice'].pct_change()
                features.loc[idx, f'volatility_{window}d'] = returns.rolling(window).std() * np.sqrt(252)
                features.loc[idx, f'return_mean_{window}d'] = returns.rolling(window).mean()
                features.loc[idx, f'return_skew_{window}d'] = returns.rolling(window).skew()
                features.loc[idx, f'return_kurtosis_{window}d'] = returns.rolling(window).kurt()
        
        print(f"âœ… ç”Ÿæˆä»·æ ¼ç‰¹å¾: {len([col for col in features.columns if col not in data.columns])} ä¸ª")
        return features
    
    def generate_technical_indicators(self, data: pd.DataFrame = None) -> pd.DataFrame:
        """ç”ŸæˆæŠ€æœ¯æŒ‡æ ‡ç‰¹å¾"""
        data = data if data is not None else self.price_data
        if data is None or data.empty:
            return pd.DataFrame()
        
        print("ğŸ“ˆ ç”ŸæˆæŠ€æœ¯æŒ‡æ ‡...")
        features = data.copy()
        
        # æ£€æŸ¥å¿…è¦åˆ—
        required_cols = ['closePrice', 'highestPrice', 'lowestPrice', 'turnoverVol']
        if not all(col in features.columns for col in required_cols):
            print("âš ï¸ ç¼ºå°‘å¿…è¦çš„OHLCVåˆ—")
            return features
        
        # æŒ‰è‚¡ç¥¨åˆ†ç»„è®¡ç®—æŒ‡æ ‡
        for ticker, group in features.groupby('ticker'):
            group = group.sort_values('tradeDate')
            idx = group.index
            
            # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
            close = group['closePrice'].astype(float).values
            high = group['highestPrice'].astype(float).values
            low = group['lowestPrice'].astype(float).values
            volume = group['turnoverVol'].astype(float).values
            
            if len(close) < 30:  # æ•°æ®å¤ªå°‘è·³è¿‡
                continue
            
            try:
                # 1. ç§»åŠ¨å¹³å‡æŒ‡æ ‡
                for period in self.indicator_params['ma_periods']:
                    if TALIB_AVAILABLE:
                        ma = talib.SMA(close, timeperiod=period)
                        ema = talib.EMA(close, timeperiod=period)
                    else:
                        ma = pd.Series(close).rolling(period).mean().values
                        ema = pd.Series(close).ewm(span=period).mean().values
                    
                    features.loc[idx, f'SMA_{period}'] = ma
                    features.loc[idx, f'EMA_{period}'] = ema
                    features.loc[idx, f'price_to_SMA_{period}'] = close / ma
                
                # 2. å¸ƒæ—å¸¦
                if TALIB_AVAILABLE:
                    bb_upper, bb_middle, bb_lower = talib.BBANDS(
                        close, timeperiod=self.indicator_params['bb_period'],
                        nbdevup=self.indicator_params['bb_std'],
                        nbdevdn=self.indicator_params['bb_std']
                    )
                else:
                    # å†…ç½®å¸ƒæ—å¸¦ç®—æ³•
                    period = self.indicator_params['bb_period']
                    std_dev = self.indicator_params['bb_std']
                    sma = pd.Series(close).rolling(period).mean()
                    std = pd.Series(close).rolling(period).std()
                    bb_upper = (sma + std_dev * std).values
                    bb_middle = sma.values
                    bb_lower = (sma - std_dev * std).values
                
                features.loc[idx, 'BB_upper'] = bb_upper
                features.loc[idx, 'BB_middle'] = bb_middle
                features.loc[idx, 'BB_lower'] = bb_lower
                features.loc[idx, 'BB_width'] = (bb_upper - bb_lower) / bb_middle
                features.loc[idx, 'BB_position'] = (close - bb_lower) / (bb_upper - bb_lower)
                
                # 3. RSIæŒ‡æ ‡
                if TALIB_AVAILABLE:
                    rsi = talib.RSI(close, timeperiod=self.indicator_params['rsi_period'])
                else:
                    # å†…ç½®RSIç®—æ³•
                    period = self.indicator_params['rsi_period']
                    delta = pd.Series(close).diff()
                    gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
                    loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
                    rs = gain / loss
                    rsi = (100 - (100 / (1 + rs))).values
                
                features.loc[idx, 'RSI'] = rsi
                features.loc[idx, 'RSI_overbought'] = (rsi > 70).astype(int)
                features.loc[idx, 'RSI_oversold'] = (rsi < 30).astype(int)
                
                # 4. MACDæŒ‡æ ‡
                if TALIB_AVAILABLE:
                    macd, macdsignal, macdhist = talib.MACD(
                        close,
                        fastperiod=self.indicator_params['macd_fast'],
                        slowperiod=self.indicator_params['macd_slow'],
                        signalperiod=self.indicator_params['macd_signal']
                    )
                else:
                    # å†…ç½®MACDç®—æ³•
                    ema12 = pd.Series(close).ewm(span=12).mean()
                    ema26 = pd.Series(close).ewm(span=26).mean()
                    macd = (ema12 - ema26).values
                    macdsignal = pd.Series(macd).ewm(span=9).mean().values
                    macdhist = macd - macdsignal
                
                features.loc[idx, 'MACD'] = macd
                features.loc[idx, 'MACD_signal'] = macdsignal
                features.loc[idx, 'MACD_hist'] = macdhist
                
                # 5. éšæœºæŒ‡æ ‡KDJ
                if TALIB_AVAILABLE:
                    slowk, slowd = talib.STOCH(high, low, close, fastk_period=9, slowk_period=3, slowd_period=3)
                    j = 3 * slowk - 2 * slowd
                else:
                    # å†…ç½®KDJç®—æ³•
                    low_min = pd.Series(low).rolling(9).min()
                    high_max = pd.Series(high).rolling(9).max()
                    rsv = 100 * (close - low_min) / (high_max - low_min)
                    slowk = rsv.ewm(com=2).mean().values
                    slowd = pd.Series(slowk).ewm(com=2).mean().values
                    j = 3 * slowk - 2 * slowd
                
                features.loc[idx, 'K'] = slowk
                features.loc[idx, 'D'] = slowd
                features.loc[idx, 'J'] = j
                
                # 6. ATR (å¹³å‡çœŸå®æ³¢å¹…)
                if TALIB_AVAILABLE:
                    atr = talib.ATR(high, low, close, timeperiod=self.indicator_params['atr_period'])
                else:
                    # å†…ç½®ATRç®—æ³•
                    tr1 = high - low
                    tr2 = np.abs(high - np.roll(close, 1))
                    tr3 = np.abs(low - np.roll(close, 1))
                    tr = np.maximum(tr1, np.maximum(tr2, tr3))
                    atr = pd.Series(tr).rolling(14).mean().values
                
                features.loc[idx, 'ATR'] = atr
                features.loc[idx, 'ATR_ratio'] = atr / close
                
                # 7. å¨å»‰æŒ‡æ ‡
                if TALIB_AVAILABLE:
                    williams_r = talib.WILLR(high, low, close, timeperiod=14)
                else:
                    # å†…ç½®Williams %Rç®—æ³•
                    high_14 = pd.Series(high).rolling(14).max()
                    low_14 = pd.Series(low).rolling(14).min()
                    williams_r = -100 * (high_14 - close) / (high_14 - low_14)
                    williams_r = williams_r.values
                
                features.loc[idx, 'Williams_R'] = williams_r
                
            except Exception as e:
                logger.warning(f"æŠ€æœ¯æŒ‡æ ‡è®¡ç®—å¤±è´¥ {ticker}: {e}")
                continue
        
        print(f"âœ… ç”ŸæˆæŠ€æœ¯æŒ‡æ ‡: çº¦20+ ä¸ª")
        return features
    
    def generate_volume_features(self, data: pd.DataFrame = None) -> pd.DataFrame:
        """ç”Ÿæˆæˆäº¤é‡ç›¸å…³ç‰¹å¾"""
        data = data if data is not None else self.price_data
        if data is None or data.empty:
            return pd.DataFrame()
        
        print("ğŸ“Š ç”Ÿæˆæˆäº¤é‡ç‰¹å¾...")
        features = data.copy()
        
        if 'turnoverVol' not in features.columns:
            print("âš ï¸ ç¼ºå°‘æˆäº¤é‡æ•°æ®")
            return features
        
        # æŒ‰è‚¡ç¥¨åˆ†ç»„è®¡ç®—ç‰¹å¾
        for ticker, group in features.groupby('ticker'):
            group = group.sort_values('tradeDate')
            idx = group.index
            
            volume = group['turnoverVol'].values
            close = group['closePrice'].values if 'closePrice' in group.columns else None
            
            # åŸºç¡€æˆäº¤é‡ç‰¹å¾
            features.loc[idx, 'volume_ma_5'] = pd.Series(volume).rolling(5).mean().values
            features.loc[idx, 'volume_ma_20'] = pd.Series(volume).rolling(20).mean().values
            features.loc[idx, 'volume_ratio'] = volume / pd.Series(volume).rolling(20).mean().values
            
            # æˆäº¤é‡å˜åŒ–ç‡
            features.loc[idx, 'volume_change'] = pd.Series(volume).pct_change().values
            features.loc[idx, 'volume_std_20'] = pd.Series(volume).rolling(20).std().values
            
            # VWAP (æˆäº¤é‡åŠ æƒå¹³å‡ä»·)
            if close is not None:
                typical_price = close  # ç®€åŒ–ä¸ºæ”¶ç›˜ä»·
                vwap = (typical_price * volume).rolling(20).sum() / pd.Series(volume).rolling(20).sum()
                features.loc[idx, 'VWAP'] = vwap.values
                features.loc[idx, 'price_to_VWAP'] = close / vwap.values
            
            # OBV (èƒ½é‡æ½®)
            if close is not None and TALIB_AVAILABLE:
                try:
                    obv = talib.OBV(close.astype(float), volume.astype(float))
                    features.loc[idx, 'OBV'] = obv
                except Exception as e:
                    # å†…ç½®OBVç®—æ³•
                    price_change = pd.Series(close).diff()
                    obv_values = []
                    obv_val = 0
                    for i, (pc, vol) in enumerate(zip(price_change, volume)):
                        if pd.isna(pc):
                            obv_values.append(obv_val)
                        elif pc > 0:
                            obv_val += vol
                            obv_values.append(obv_val)
                        elif pc < 0:
                            obv_val -= vol
                            obv_values.append(obv_val)
                        else:
                            obv_values.append(obv_val)
                    features.loc[idx, 'OBV'] = obv_values
        
        print(f"âœ… ç”Ÿæˆæˆäº¤é‡ç‰¹å¾: çº¦8 ä¸ª")
        return features
    
    def generate_momentum_features(self, data: pd.DataFrame = None) -> pd.DataFrame:
        """ç”ŸæˆåŠ¨é‡ç›¸å…³ç‰¹å¾"""
        data = data if data is not None else self.price_data
        if data is None or data.empty:
            return pd.DataFrame()
        
        print("ğŸš€ ç”ŸæˆåŠ¨é‡ç‰¹å¾...")
        features = data.copy()
        
        if 'closePrice' not in features.columns:
            print("âš ï¸ ç¼ºå°‘ä»·æ ¼æ•°æ®")
            return features
        
        # æŒ‰è‚¡ç¥¨åˆ†ç»„è®¡ç®—ç‰¹å¾
        for ticker, group in features.groupby('ticker'):
            group = group.sort_values('tradeDate')
            idx = group.index
            
            close = group['closePrice'].values
            
            # åŠ¨é‡æŒ‡æ ‡
            for period in [5, 10, 20, 60]:
                momentum = close / np.roll(close, period) - 1
                features.loc[idx, f'momentum_{period}d'] = momentum
                
                # ç›¸å¯¹å¼ºåº¦
                market_return = pd.Series(close).pct_change().rolling(period).mean().values
                features.loc[idx, f'relative_strength_{period}d'] = momentum - market_return
            
            # ROC (å˜åŠ¨ç‡æŒ‡æ ‡)
            if TALIB_AVAILABLE:
                roc = talib.ROC(close, timeperiod=10)
            else:
                roc = (close / np.roll(close, 10) - 1) * 100
            features.loc[idx, 'ROC'] = roc
            
            # CCI (å•†å“é€šé“æŒ‡æ ‡)
            if 'highestPrice' in group.columns and 'lowestPrice' in group.columns:
                high = group['highestPrice'].values
                low = group['lowestPrice'].values
                
                if TALIB_AVAILABLE:
                    cci = talib.CCI(high, low, close, timeperiod=14)
                else:
                    # å†…ç½®CCIç®—æ³•
                    tp = (high + low + close) / 3
                    sma_tp = pd.Series(tp).rolling(14).mean()
                    mad = pd.Series(tp).rolling(14).apply(lambda x: np.mean(np.abs(x - x.mean())))
                    cci = (tp - sma_tp) / (0.015 * mad)
                    cci = cci.values
                
                features.loc[idx, 'CCI'] = cci
        
        print(f"âœ… ç”ŸæˆåŠ¨é‡ç‰¹å¾: çº¦15+ ä¸ª")
        return features
    
    def generate_statistical_features(self, data: pd.DataFrame = None) -> pd.DataFrame:
        """ç”Ÿæˆç»Ÿè®¡ç›¸å…³ç‰¹å¾"""
        data = data if data is not None else self.price_data
        if data is None or data.empty:
            return pd.DataFrame()
        
        print("ğŸ“ˆ ç”Ÿæˆç»Ÿè®¡ç‰¹å¾...")
        features = data.copy()
        
        # æŒ‰è‚¡ç¥¨åˆ†ç»„è®¡ç®—ç‰¹å¾
        for ticker, group in features.groupby('ticker'):
            group = group.sort_values('tradeDate')
            idx = group.index
            
            if 'closePrice' in group.columns:
                close = group['closePrice']
                returns = close.pct_change()
                
                # ç»Ÿè®¡ç‰¹å¾ï¼ˆä¸åŒçª—å£ï¼‰
                for window in [5, 10, 20]:
                    # åŸºç¡€ç»Ÿè®¡é‡
                    features.loc[idx, f'price_mean_{window}d'] = close.rolling(window).mean()
                    features.loc[idx, f'price_std_{window}d'] = close.rolling(window).std()
                    features.loc[idx, f'price_skew_{window}d'] = close.rolling(window).skew()
                    features.loc[idx, f'price_kurt_{window}d'] = close.rolling(window).kurt()
                    
                    # æ”¶ç›Šç‡ç»Ÿè®¡é‡
                    features.loc[idx, f'return_mean_{window}d'] = returns.rolling(window).mean()
                    features.loc[idx, f'return_std_{window}d'] = returns.rolling(window).std()
                    
                    # æœ€å¤§å›æ’¤
                    rolling_max = close.rolling(window).max()
                    drawdown = (close - rolling_max) / rolling_max
                    features.loc[idx, f'max_drawdown_{window}d'] = drawdown.rolling(window).min()
                    
                    # ä¸Šæ¶¨ä¸‹è·Œå¤©æ•°æ¯”ä¾‹
                    up_days = (returns > 0).rolling(window).sum()
                    features.loc[idx, f'up_ratio_{window}d'] = up_days / window
        
        print(f"âœ… ç”Ÿæˆç»Ÿè®¡ç‰¹å¾: çº¦30+ ä¸ª")
        return features
    
    def generate_all_features(self, data: pd.DataFrame = None) -> pd.DataFrame:
        """
        ç”Ÿæˆæ‰€æœ‰ç‰¹å¾
        
        Args:
            data: è¾“å…¥ä»·æ ¼æ•°æ®
            
        Returns:
            åŒ…å«æ‰€æœ‰ç‰¹å¾çš„DataFrame
        """
        data = data if data is not None else self.price_data
        if data is None or data.empty:
            print("âŒ æ²¡æœ‰è¾“å…¥æ•°æ®")
            return pd.DataFrame()
        
        print("ğŸ¯ ç”Ÿæˆæ‰€æœ‰ç‰¹å¾...")
        start_time = datetime.now()
        
        # ç”Ÿæˆç¼“å­˜é”®
        data_hash = hashlib.md5(str(data.shape).encode()).hexdigest()
        cache_key = f"all_features_{data_hash}"
        
        # å°è¯•ä»ç¼“å­˜åŠ è½½
        cached_features = self._load_from_cache(cache_key)
        if cached_features is not None:
            print("ğŸ“¥ ä»ç¼“å­˜åŠ è½½æ‰€æœ‰ç‰¹å¾")
            return cached_features
        
        # é€æ­¥ç”Ÿæˆç‰¹å¾
        features = data.copy()
        
        try:
            # 1. ä»·æ ¼ç‰¹å¾
            features = self.generate_price_features(features)
            
            # 2. æŠ€æœ¯æŒ‡æ ‡
            features = self.generate_technical_indicators(features)
            
            # 3. æˆäº¤é‡ç‰¹å¾
            features = self.generate_volume_features(features)
            
            # 4. åŠ¨é‡ç‰¹å¾
            features = self.generate_momentum_features(features)
            
            # 5. ç»Ÿè®¡ç‰¹å¾
            features = self.generate_statistical_features(features)
            
            # 6. ç‰¹å¾åå¤„ç†
            features = self._post_process_features(features)
            
            # è®¡ç®—å¤„ç†ç»Ÿè®¡
            processing_time = (datetime.now() - start_time).total_seconds()
            feature_count = len(features.columns) - len(data.columns)
            
            self.stats['generated_features'] += feature_count
            self.stats['processing_time'] += processing_time
            
            print(f"âœ… ç‰¹å¾ç”Ÿæˆå®Œæˆ")
            print(f"   ğŸ“Š åŸå§‹ç‰¹å¾: {len(data.columns)} ä¸ª")
            print(f"   ğŸ”¬ æ–°å¢ç‰¹å¾: {feature_count} ä¸ª")
            print(f"   â±ï¸ å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’")
            
            # ä¿å­˜åˆ°ç¼“å­˜
            self._save_to_cache(features, cache_key)
            
            return features
            
        except Exception as e:
            print(f"âŒ ç‰¹å¾ç”Ÿæˆå¤±è´¥: {e}")
            logger.error(f"ç‰¹å¾ç”Ÿæˆé”™è¯¯: {e}")
            return data
    
    def _post_process_features(self, features: pd.DataFrame) -> pd.DataFrame:
        """ç‰¹å¾åå¤„ç†"""
        print("ğŸ”§ ç‰¹å¾åå¤„ç†...")
        
        # 1. å¤„ç†æ— ç©·å€¼
        features = features.replace([np.inf, -np.inf], np.nan)
        
        # 2. ç‰¹å¾é€‰æ‹©ï¼ˆç§»é™¤é«˜ç›¸å…³æ€§ç‰¹å¾ï¼‰
        if self.config['feature_selection']:
            features = self._remove_highly_correlated_features(features)
        
        # 3. æŒ‰è‚¡ç¥¨åˆ†ç»„å‰å‘å¡«å……ç¼ºå¤±å€¼
        numeric_cols = features.select_dtypes(include=[np.number]).columns
        features[numeric_cols] = features.groupby('ticker')[numeric_cols].apply(
            lambda x: x.fillna(method='ffill').fillna(method='bfill')
        )
        
        return features
    
    def _remove_highly_correlated_features(self, features: pd.DataFrame) -> pd.DataFrame:
        """ç§»é™¤é«˜ç›¸å…³æ€§ç‰¹å¾"""
        numeric_cols = features.select_dtypes(include=[np.number]).columns
        # æ’é™¤åŸºç¡€åˆ—
        exclude_cols = ['ticker', 'tradeDate', 'openPrice', 'highestPrice', 'lowestPrice', 'closePrice', 'turnoverVol']
        feature_cols = [col for col in numeric_cols if col not in exclude_cols]
        
        if len(feature_cols) < 2:
            return features
        
        try:
            # è®¡ç®—ç›¸å…³ç³»æ•°çŸ©é˜µ
            corr_matrix = features[feature_cols].corr().abs()
            
            # æ‰¾åˆ°é«˜ç›¸å…³æ€§ç‰¹å¾å¯¹
            upper_triangle = corr_matrix.where(
                np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)
            )
            
            # æ‰¾åˆ°éœ€è¦åˆ é™¤çš„ç‰¹å¾
            to_drop = [column for column in upper_triangle.columns 
                      if any(upper_triangle[column] > self.config['correlation_threshold'])]
            
            if to_drop:
                print(f"   ğŸ—‘ï¸ ç§»é™¤é«˜ç›¸å…³ç‰¹å¾: {len(to_drop)} ä¸ª")
                features = features.drop(columns=to_drop)
            
        except Exception as e:
            logger.warning(f"ç›¸å…³æ€§åˆ†æå¤±è´¥: {e}")
        
        return features
    
    def calculate_feature_importance(self, features: pd.DataFrame, 
                                   target_col: str = None) -> Dict[str, float]:
        """
        è®¡ç®—ç‰¹å¾é‡è¦æ€§
        
        Args:
            features: ç‰¹å¾æ•°æ®
            target_col: ç›®æ ‡å˜é‡åˆ—å
            
        Returns:
            ç‰¹å¾é‡è¦æ€§å­—å…¸
        """
        if target_col is None or target_col not in features.columns:
            print("âš ï¸ æœªæŒ‡å®šæœ‰æ•ˆçš„ç›®æ ‡å˜é‡")
            return {}
        
        print("ğŸ“Š è®¡ç®—ç‰¹å¾é‡è¦æ€§...")
        
        # é€‰æ‹©æ•°å€¼ç‰¹å¾
        numeric_cols = features.select_dtypes(include=[np.number]).columns
        feature_cols = [col for col in numeric_cols if col != target_col and 'ticker' not in col]
        
        importance_scores = {}
        
        try:
            # ä½¿ç”¨ç›¸å…³ç³»æ•°ä½œä¸ºé‡è¦æ€§æŒ‡æ ‡
            for col in feature_cols:
                if col in features.columns:
                    corr = features[col].corr(features[target_col])
                    importance_scores[col] = abs(corr) if not pd.isna(corr) else 0
            
            # æ’åº
            importance_scores = dict(sorted(importance_scores.items(), 
                                          key=lambda x: x[1], reverse=True))
            
            self.stats['feature_importance'] = importance_scores
            
            # æ˜¾ç¤ºå‰10ä¸ªé‡è¦ç‰¹å¾
            top_features = list(importance_scores.items())[:10]
            print("ğŸ“‹ ç‰¹å¾é‡è¦æ€§Top 10:")
            for i, (feature, score) in enumerate(top_features, 1):
                print(f"   {i:2d}. {feature}: {score:.4f}")
            
        except Exception as e:
            logger.error(f"ç‰¹å¾é‡è¦æ€§è®¡ç®—å¤±è´¥: {e}")
        
        return importance_scores
    
    def get_feature_summary(self) -> Dict[str, Any]:
        """è·å–ç‰¹å¾æ‘˜è¦ä¿¡æ¯"""
        return {
            'stats': self.stats,
            'config': self.config,
            'indicator_params': self.indicator_params,
            'talib_available': TALIB_AVAILABLE,
            'cache_dir': self.cache_dir
        }


# ==========================================
# ğŸ­ å·¥å‚å‡½æ•°å’Œæ¨¡å—å¯¼å‡º
# ==========================================

def create_feature_engineer(price_data: pd.DataFrame = None, 
                           config: Dict = None) -> FeatureEngineer:
    """
    åˆ›å»ºç‰¹å¾å·¥ç¨‹å™¨å®ä¾‹çš„å·¥å‚å‡½æ•°
    
    Args:
        price_data: ä»·æ ¼æ•°æ®
        config: é…ç½®å‚æ•°
        
    Returns:
        FeatureEngineerå®ä¾‹
    """
    return FeatureEngineer(price_data, config)

# æ¨¡å—å¯¼å‡º
__all__ = [
    'FeatureEngineer',
    'create_feature_engineer'
]

if __name__ == "__main__":
    print("ğŸ”¬ ç‰¹å¾å·¥ç¨‹å™¨ v2.0 æ¨¡å—åŠ è½½å®Œæˆ")
    print("ğŸ“˜ ä½¿ç”¨ç¤ºä¾‹:")
    print("   from feature_engineer import FeatureEngineer, create_feature_engineer")
    print("   engineer = create_feature_engineer(price_data)")
    print("   features = engineer.generate_all_features()")
    print("")
    print("ğŸ’¡ åŠŸèƒ½ç‰¹æ€§:")
    print("   ğŸ“Š 60+ æŠ€æœ¯æŒ‡æ ‡å’Œç»Ÿè®¡ç‰¹å¾")
    print("   ğŸ”¬ TA-Libé›†æˆå’Œå†…ç½®ç®—æ³•å¤‡ä»½")
    print("   ğŸš€ é«˜æ€§èƒ½æ‰¹é‡ç‰¹å¾è®¡ç®—")
    print("   ğŸ’¾ æ™ºèƒ½ç¼“å­˜å’Œå¢é‡æ›´æ–°")
    print("   ğŸ“‹ ç‰¹å¾é‡è¦æ€§åˆ†æ")
    print("   ğŸ¯ è‡ªåŠ¨ç‰¹å¾é€‰æ‹©å’Œå»é‡")