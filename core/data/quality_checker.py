#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®è´¨é‡æ£€æŸ¥å™¨
=============

å…¨é¢çš„æ•°æ®è´¨é‡æ£€æŸ¥å’ŒéªŒè¯ç³»ç»Ÿ
"""

from typing import Dict, List, Optional, Union, Any, Tuple
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta
import logging
from pathlib import Path
import json
from scipy import stats
import warnings

logger = logging.getLogger(__name__)

class DataQualityChecker:
    """æ•°æ®è´¨é‡æ£€æŸ¥å™¨
    
    æä¾›å…¨é¢çš„æ•°æ®è´¨é‡æ£€æŸ¥åŠŸèƒ½ï¼š
    - ç¼ºå¤±æ•°æ®æ£€æµ‹
    - å¼‚å¸¸å€¼æ£€æµ‹
    - æ•°æ®ç±»å‹éªŒè¯
    - ä¸€è‡´æ€§æ£€æŸ¥
    - å®Œæ•´æ€§éªŒè¯
    - è´¨é‡æŠ¥å‘Šç”Ÿæˆ
    """
    
    def __init__(self, config: Optional[Dict] = None):
        """åˆå§‹åŒ–æ•°æ®è´¨é‡æ£€æŸ¥å™¨
        
        Args:
            config: é…ç½®å‚æ•°
        """
        self.config = config or {}
        
        # é»˜è®¤é˜ˆå€¼é…ç½®
        self.thresholds = {
            'missing_rate': 0.1,      # ç¼ºå¤±ç‡é˜ˆå€¼ 10%
            'outlier_zscore': 3.0,    # Z-scoreå¼‚å¸¸å€¼é˜ˆå€¼
            'outlier_iqr': 1.5,       # IQRå¼‚å¸¸å€¼é˜ˆå€¼
            'price_change': 0.5,      # ä»·æ ¼å˜åŒ–é˜ˆå€¼ 50%
            'volume_change': 10.0,    # æˆäº¤é‡å˜åŒ–é˜ˆå€¼ 1000%
            'trading_days': 245       # å¹´äº¤æ˜“æ—¥æ•°é‡
        }
        
        # æ›´æ–°ç”¨æˆ·é…ç½®çš„é˜ˆå€¼
        if 'thresholds' in self.config:
            self.thresholds.update(self.config['thresholds'])
        
        # æ£€æŸ¥ç»“æœå­˜å‚¨
        self.check_results = {}
        
    def check_missing_data(self, 
                          df: pd.DataFrame,
                          critical_columns: Optional[List[str]] = None) -> Dict[str, Any]:
        """æ£€æŸ¥ç¼ºå¤±æ•°æ®
        
        Args:
            df: å¾…æ£€æŸ¥çš„DataFrame
            critical_columns: å…³é”®åˆ—åˆ—è¡¨
            
        Returns:
            Dict: æ£€æŸ¥ç»“æœ
        """
        logger.info("ğŸ” æ£€æŸ¥ç¼ºå¤±æ•°æ®...")
        
        if df.empty:
            return {'error': 'æ•°æ®ä¸ºç©º'}
        
        results = {
            'total_rows': len(df),
            'columns_checked': list(df.columns),
            'missing_summary': {},
            'critical_issues': [],
            'recommendations': []
        }
        
        # è®¡ç®—æ¯åˆ—ç¼ºå¤±ç‡
        missing_counts = df.isnull().sum()
        missing_rates = missing_counts / len(df)
        
        for col in df.columns:
            missing_count = missing_counts[col]
            missing_rate = missing_rates[col]
            
            results['missing_summary'][col] = {
                'missing_count': int(missing_count),
                'missing_rate': float(missing_rate),
                'is_critical': col in (critical_columns or []),
                'severity': self._get_missing_severity(missing_rate)
            }
            
            # æ£€æŸ¥å…³é”®åˆ—
            if critical_columns and col in critical_columns:
                if missing_rate > 0:
                    results['critical_issues'].append(
                        f"å…³é”®åˆ— '{col}' å­˜åœ¨ {missing_count} ä¸ªç¼ºå¤±å€¼ ({missing_rate:.2%})"
                    )
            
            # é«˜ç¼ºå¤±ç‡è­¦å‘Š
            if missing_rate > self.thresholds['missing_rate']:
                results['recommendations'].append(
                    f"åˆ— '{col}' ç¼ºå¤±ç‡è¿‡é«˜ ({missing_rate:.2%})ï¼Œå»ºè®®æ£€æŸ¥æ•°æ®æºæˆ–è¿›è¡Œæ’å€¼å¤„ç†"
                )
        
        # æ•´ä½“è¯„ä¼°
        total_missing = missing_counts.sum()
        total_cells = len(df) * len(df.columns)
        overall_missing_rate = total_missing / total_cells
        
        results['overall'] = {
            'total_missing': int(total_missing),
            'total_cells': int(total_cells),
            'missing_rate': float(overall_missing_rate),
            'quality_score': 1.0 - overall_missing_rate
        }
        
        logger.info(f"âœ… ç¼ºå¤±æ•°æ®æ£€æŸ¥å®Œæˆï¼Œæ•´ä½“ç¼ºå¤±ç‡: {overall_missing_rate:.2%}")
        return results
    
    def check_outliers(self, 
                      df: pd.DataFrame,
                      numeric_columns: Optional[List[str]] = None,
                      method: str = 'both') -> Dict[str, Any]:
        """æ£€æŸ¥å¼‚å¸¸å€¼
        
        Args:
            df: å¾…æ£€æŸ¥çš„DataFrame
            numeric_columns: æ•°å€¼åˆ—åˆ—è¡¨
            method: æ£€æµ‹æ–¹æ³• ('zscore', 'iqr', 'both')
            
        Returns:
            Dict: æ£€æŸ¥ç»“æœ
        """
        logger.info("ğŸ” æ£€æŸ¥å¼‚å¸¸å€¼...")
        
        if df.empty:
            return {'error': 'æ•°æ®ä¸ºç©º'}
        
        # è‡ªåŠ¨è¯†åˆ«æ•°å€¼åˆ—
        if numeric_columns is None:
            numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()
        
        results = {
            'columns_checked': numeric_columns,
            'outlier_summary': {},
            'outlier_details': {},
            'recommendations': []
        }
        
        for col in numeric_columns:
            if col not in df.columns:
                continue
                
            series = df[col].dropna()
            if series.empty:
                continue
            
            col_results = {
                'total_values': len(series),
                'zscore_outliers': 0,
                'iqr_outliers': 0,
                'outlier_indices': []
            }
            
            # Z-Scoreæ–¹æ³•
            if method in ['zscore', 'both']:
                z_scores = np.abs(stats.zscore(series))
                zscore_outliers = series[z_scores > self.thresholds['outlier_zscore']]
                col_results['zscore_outliers'] = len(zscore_outliers)
                col_results['outlier_indices'].extend(zscore_outliers.index.tolist())
            
            # IQRæ–¹æ³•
            if method in ['iqr', 'both']:
                Q1 = series.quantile(0.25)
                Q3 = series.quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - self.thresholds['outlier_iqr'] * IQR
                upper_bound = Q3 + self.thresholds['outlier_iqr'] * IQR
                
                iqr_outliers = series[(series < lower_bound) | (series > upper_bound)]
                col_results['iqr_outliers'] = len(iqr_outliers)
                col_results['outlier_indices'].extend(iqr_outliers.index.tolist())
            
            # å»é‡å¼‚å¸¸å€¼ç´¢å¼•
            col_results['outlier_indices'] = list(set(col_results['outlier_indices']))
            col_results['outlier_rate'] = len(col_results['outlier_indices']) / len(series)
            
            results['outlier_summary'][col] = col_results
            
            # è¯¦ç»†å¼‚å¸¸å€¼ä¿¡æ¯
            if col_results['outlier_indices']:
                outlier_values = series.loc[col_results['outlier_indices']]
                results['outlier_details'][col] = {
                    'min_outlier': float(outlier_values.min()),
                    'max_outlier': float(outlier_values.max()),
                    'outlier_values': outlier_values.tolist()[:10]  # æœ€å¤šæ˜¾ç¤º10ä¸ª
                }
            
            # å»ºè®®
            if col_results['outlier_rate'] > 0.05:  # 5%å¼‚å¸¸å€¼é˜ˆå€¼
                results['recommendations'].append(
                    f"åˆ— '{col}' å¼‚å¸¸å€¼æ¯”ä¾‹è¾ƒé«˜ ({col_results['outlier_rate']:.2%})ï¼Œå»ºè®®è¿›ä¸€æ­¥åˆ†æ"
                )
        
        logger.info(f"âœ… å¼‚å¸¸å€¼æ£€æŸ¥å®Œæˆï¼Œæ£€æŸ¥äº† {len(numeric_columns)} ä¸ªæ•°å€¼åˆ—")
        return results
    
    def check_data_types(self, 
                        df: pd.DataFrame,
                        expected_types: Optional[Dict[str, str]] = None) -> Dict[str, Any]:
        """æ£€æŸ¥æ•°æ®ç±»å‹
        
        Args:
            df: å¾…æ£€æŸ¥çš„DataFrame  
            expected_types: æœŸæœ›çš„æ•°æ®ç±»å‹å­—å…¸ {column: dtype}
            
        Returns:
            Dict: æ£€æŸ¥ç»“æœ
        """
        logger.info("ğŸ” æ£€æŸ¥æ•°æ®ç±»å‹...")
        
        if df.empty:
            return {'error': 'æ•°æ®ä¸ºç©º'}
        
        results = {
            'type_summary': {},
            'type_issues': [],
            'recommendations': []
        }
        
        # é»˜è®¤æœŸæœ›ç±»å‹ï¼ˆé’ˆå¯¹é‡‘èæ•°æ®ï¼‰
        if expected_types is None:
            expected_types = {
                'date': 'datetime64',
                'symbol': 'object',
                'open': 'float64',
                'high': 'float64', 
                'low': 'float64',
                'close': 'float64',
                'volume': 'int64',
                'amount': 'float64'
            }
        
        for col in df.columns:
            current_type = str(df[col].dtype)
            expected_type = expected_types.get(col, 'auto')
            
            col_info = {
                'current_type': current_type,
                'expected_type': expected_type,
                'is_correct': True,
                'null_count': int(df[col].isnull().sum())
            }
            
            # æ£€æŸ¥ç±»å‹åŒ¹é…
            if expected_type != 'auto':
                if expected_type == 'datetime64' and not pd.api.types.is_datetime64_any_dtype(df[col]):
                    col_info['is_correct'] = False
                    results['type_issues'].append(f"åˆ— '{col}' åº”ä¸ºæ—¥æœŸç±»å‹ï¼Œå½“å‰ä¸º {current_type}")
                elif expected_type.startswith('float') and not pd.api.types.is_numeric_dtype(df[col]):
                    col_info['is_correct'] = False  
                    results['type_issues'].append(f"åˆ— '{col}' åº”ä¸ºæµ®ç‚¹æ•°ç±»å‹ï¼Œå½“å‰ä¸º {current_type}")
                elif expected_type.startswith('int') and not pd.api.types.is_integer_dtype(df[col]):
                    col_info['is_correct'] = False
                    results['type_issues'].append(f"åˆ— '{col}' åº”ä¸ºæ•´æ•°ç±»å‹ï¼Œå½“å‰ä¸º {current_type}")
                elif expected_type == 'object' and df[col].dtype != 'object':
                    col_info['is_correct'] = False
                    results['type_issues'].append(f"åˆ— '{col}' åº”ä¸ºå­—ç¬¦ä¸²ç±»å‹ï¼Œå½“å‰ä¸º {current_type}")
            
            results['type_summary'][col] = col_info
        
        # ç”Ÿæˆå»ºè®®
        for issue in results['type_issues']:
            results['recommendations'].append(f"æ•°æ®ç±»å‹è½¬æ¢: {issue}")
        
        logger.info(f"âœ… æ•°æ®ç±»å‹æ£€æŸ¥å®Œæˆï¼Œå‘ç° {len(results['type_issues'])} ä¸ªé—®é¢˜")
        return results
    
    def check_price_data_consistency(self, df: pd.DataFrame) -> Dict[str, Any]:
        """æ£€æŸ¥ä»·æ ¼æ•°æ®ä¸€è‡´æ€§
        
        Args:
            df: ä»·æ ¼æ•°æ®DataFrame
            
        Returns:
            Dict: æ£€æŸ¥ç»“æœ
        """
        logger.info("ğŸ” æ£€æŸ¥ä»·æ ¼æ•°æ®ä¸€è‡´æ€§...")
        
        if df.empty:
            return {'error': 'æ•°æ®ä¸ºç©º'}
        
        required_cols = ['open', 'high', 'low', 'close']
        missing_cols = [col for col in required_cols if col not in df.columns]
        
        if missing_cols:
            return {'error': f'ç¼ºå°‘å¿…éœ€çš„ä»·æ ¼åˆ—: {missing_cols}'}
        
        results = {
            'consistency_issues': [],
            'price_relationships': {},
            'extreme_changes': [],
            'recommendations': []
        }
        
        # æ£€æŸ¥ä»·æ ¼å…³ç³» (high >= low, high >= open, high >= close, low <= open, low <= close)
        invalid_high_low = df['high'] < df['low']
        invalid_high_open = df['high'] < df['open'] 
        invalid_high_close = df['high'] < df['close']
        invalid_low_open = df['low'] > df['open']
        invalid_low_close = df['low'] > df['close']
        
        issue_counts = {
            'high_less_than_low': invalid_high_low.sum(),
            'high_less_than_open': invalid_high_open.sum(),
            'high_less_than_close': invalid_high_close.sum(),
            'low_greater_than_open': invalid_low_open.sum(),
            'low_greater_than_close': invalid_low_close.sum()
        }
        
        results['price_relationships'] = issue_counts
        
        # è®°å½•ä¸€è‡´æ€§é—®é¢˜
        for issue_type, count in issue_counts.items():
            if count > 0:
                results['consistency_issues'].append(f"{issue_type}: {count} æ¡è®°å½•")
        
        # æ£€æŸ¥æç«¯ä»·æ ¼å˜åŒ–
        if 'symbol' in df.columns and len(df) > 1:
            df_sorted = df.sort_values(['symbol', 'date'] if 'date' in df.columns else ['symbol'])
            
            for symbol in df_sorted['symbol'].unique():
                symbol_data = df_sorted[df_sorted['symbol'] == symbol]
                if len(symbol_data) < 2:
                    continue
                
                # è®¡ç®—ä»·æ ¼å˜åŒ–ç‡
                price_changes = symbol_data['close'].pct_change().abs()
                extreme_changes = price_changes > self.thresholds['price_change']
                
                if extreme_changes.any():
                    extreme_indices = symbol_data[extreme_changes].index.tolist()
                    results['extreme_changes'].extend([
                        {
                            'symbol': symbol,
                            'index': idx,
                            'change_rate': float(price_changes.loc[idx])
                        }
                        for idx in extreme_indices
                    ])
        
        # ç”Ÿæˆå»ºè®®
        if results['consistency_issues']:
            results['recommendations'].append("å­˜åœ¨ä»·æ ¼å…³ç³»ä¸ä¸€è‡´çš„è®°å½•ï¼Œå»ºè®®æ£€æŸ¥æ•°æ®æºè´¨é‡")
        
        if results['extreme_changes']:
            results['recommendations'].append(
                f"å‘ç° {len(results['extreme_changes'])} ä¸ªæç«¯ä»·æ ¼å˜åŒ–ï¼Œå»ºè®®è¿›ä¸€æ­¥æ ¸å®"
            )
        
        logger.info(f"âœ… ä»·æ ¼æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥å®Œæˆ")
        return results
    
    def check_completeness(self, 
                          df: pd.DataFrame,
                          date_column: str = 'date',
                          symbol_column: str = 'symbol') -> Dict[str, Any]:
        """æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
        
        Args:
            df: å¾…æ£€æŸ¥çš„DataFrame
            date_column: æ—¥æœŸåˆ—å
            symbol_column: è‚¡ç¥¨ä»£ç åˆ—å
            
        Returns:
            Dict: æ£€æŸ¥ç»“æœ
        """
        logger.info("ğŸ” æ£€æŸ¥æ•°æ®å®Œæ•´æ€§...")
        
        if df.empty:
            return {'error': 'æ•°æ®ä¸ºç©º'}
        
        if date_column not in df.columns:
            return {'error': f'ç¼ºå°‘æ—¥æœŸåˆ—: {date_column}'}
        
        if symbol_column not in df.columns:
            return {'error': f'ç¼ºå°‘è‚¡ç¥¨ä»£ç åˆ—: {symbol_column}'}
        
        results = {
            'date_range': {},
            'symbol_coverage': {},
            'missing_dates': {},
            'recommendations': []
        }
        
        # æ—¥æœŸèŒƒå›´åˆ†æ
        df[date_column] = pd.to_datetime(df[date_column])
        min_date = df[date_column].min()
        max_date = df[date_column].max()
        date_range = (max_date - min_date).days
        
        results['date_range'] = {
            'start_date': min_date.strftime('%Y-%m-%d'),
            'end_date': max_date.strftime('%Y-%m-%d'),
            'total_days': date_range,
            'unique_dates': df[date_column].nunique()
        }
        
        # è‚¡ç¥¨è¦†ç›–åˆ†æ
        symbols = df[symbol_column].unique()
        results['symbol_coverage'] = {
            'total_symbols': len(symbols),
            'symbols_list': symbols.tolist()[:20],  # æœ€å¤šæ˜¾ç¤º20ä¸ª
            'avg_records_per_symbol': len(df) / len(symbols)
        }
        
        # æ£€æŸ¥ç¼ºå¤±çš„äº¤æ˜“æ—¥
        trading_dates = pd.date_range(start=min_date, end=max_date, freq='D')
        trading_dates = trading_dates[trading_dates.weekday < 5]  # å»é™¤å‘¨æœ«
        
        existing_dates = set(df[date_column].dt.date)
        expected_dates = set(trading_dates.date)
        missing_dates = expected_dates - existing_dates
        
        if missing_dates:
            missing_list = sorted(list(missing_dates))[:10]  # æœ€å¤šæ˜¾ç¤º10ä¸ª
            results['missing_dates'] = {
                'count': len(missing_dates),
                'sample_dates': [d.strftime('%Y-%m-%d') for d in missing_list]
            }
            
            results['recommendations'].append(
                f"å‘ç° {len(missing_dates)} ä¸ªç¼ºå¤±çš„äº¤æ˜“æ—¥ï¼Œå»ºè®®æ£€æŸ¥æ˜¯å¦ä¸ºèŠ‚å‡æ—¥æˆ–æ•°æ®æºé—®é¢˜"
            )
        
        # æ£€æŸ¥æ¯ä¸ªè‚¡ç¥¨çš„æ•°æ®å®Œæ•´æ€§
        incomplete_symbols = []
        expected_records = results['date_range']['unique_dates']
        
        for symbol in symbols:
            symbol_records = len(df[df[symbol_column] == symbol])
            completeness = symbol_records / expected_records
            
            if completeness < 0.9:  # å®Œæ•´åº¦ä½äº90%
                incomplete_symbols.append({
                    'symbol': symbol,
                    'records': symbol_records,
                    'expected': expected_records,
                    'completeness': completeness
                })
        
        if incomplete_symbols:
            results['incomplete_symbols'] = incomplete_symbols[:10]  # æœ€å¤šæ˜¾ç¤º10ä¸ª
            results['recommendations'].append(
                f"{len(incomplete_symbols)} ä¸ªè‚¡ç¥¨çš„æ•°æ®ä¸å®Œæ•´ï¼Œå»ºè®®è¡¥å……ç¼ºå¤±æ•°æ®"
            )
        
        logger.info(f"âœ… æ•°æ®å®Œæ•´æ€§æ£€æŸ¥å®Œæˆ")
        return results
    
    def generate_quality_report(self, 
                               df: pd.DataFrame,
                               report_name: str = "æ•°æ®è´¨é‡æŠ¥å‘Š") -> Dict[str, Any]:
        """ç”Ÿæˆç»¼åˆè´¨é‡æŠ¥å‘Š
        
        Args:
            df: å¾…æ£€æŸ¥çš„DataFrame
            report_name: æŠ¥å‘Šåç§°
            
        Returns:
            Dict: ç»¼åˆè´¨é‡æŠ¥å‘Š
        """
        logger.info("ğŸ“Š ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Š...")
        
        report = {
            'report_name': report_name,
            'timestamp': datetime.now().isoformat(),
            'data_info': {
                'rows': len(df),
                'columns': len(df.columns),
                'memory_usage_mb': df.memory_usage(deep=True).sum() / 1024 / 1024
            },
            'quality_checks': {},
            'overall_score': 0.0,
            'recommendations': []
        }
        
        # æ‰§è¡Œå„é¡¹æ£€æŸ¥
        checks = [
            ('missing_data', self.check_missing_data),
            ('outliers', self.check_outliers), 
            ('data_types', self.check_data_types),
            ('completeness', self.check_completeness)
        ]
        
        # å¦‚æœæ˜¯ä»·æ ¼æ•°æ®ï¼Œæ·»åŠ ä¸€è‡´æ€§æ£€æŸ¥
        price_cols = ['open', 'high', 'low', 'close']
        if all(col in df.columns for col in price_cols):
            checks.append(('price_consistency', self.check_price_data_consistency))
        
        scores = []
        all_recommendations = []
        
        for check_name, check_func in checks:
            try:
                result = check_func(df)
                if 'error' not in result:
                    report['quality_checks'][check_name] = result
                    
                    # è®¡ç®—å¾—åˆ†
                    score = self._calculate_check_score(check_name, result)
                    scores.append(score)
                    
                    # æ”¶é›†å»ºè®®
                    if 'recommendations' in result:
                        all_recommendations.extend(result['recommendations'])
                else:
                    logger.warning(f"âš ï¸ {check_name} æ£€æŸ¥å¤±è´¥: {result['error']}")
                    
            except Exception as e:
                logger.error(f"âŒ {check_name} æ£€æŸ¥å‡ºé”™: {str(e)}")
        
        # è®¡ç®—æ•´ä½“å¾—åˆ†
        if scores:
            report['overall_score'] = sum(scores) / len(scores)
        
        # åˆå¹¶å»ºè®®
        report['recommendations'] = list(set(all_recommendations))
        
        # è´¨é‡è¯„çº§
        score = report['overall_score']
        if score >= 0.9:
            report['quality_grade'] = 'A (ä¼˜ç§€)'
        elif score >= 0.8:
            report['quality_grade'] = 'B (è‰¯å¥½)'
        elif score >= 0.7:
            report['quality_grade'] = 'C (ä¸€èˆ¬)'
        elif score >= 0.6:
            report['quality_grade'] = 'D (è¾ƒå·®)'
        else:
            report['quality_grade'] = 'F (æå·®)'
        
        logger.info(f"âœ… æ•°æ®è´¨é‡æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼Œæ•´ä½“å¾—åˆ†: {score:.2f}")
        return report
    
    def _calculate_check_score(self, check_name: str, result: Dict[str, Any]) -> float:
        """è®¡ç®—å•é¡¹æ£€æŸ¥å¾—åˆ†"""
        if check_name == 'missing_data':
            if 'overall' in result:
                return result['overall'].get('quality_score', 0.8)
        elif check_name == 'outliers':
            # åŸºäºå¼‚å¸¸å€¼æ¯”ä¾‹è®¡ç®—å¾—åˆ†
            outlier_rates = []
            for col_result in result.get('outlier_summary', {}).values():
                outlier_rates.append(col_result.get('outlier_rate', 0))
            if outlier_rates:
                avg_outlier_rate = sum(outlier_rates) / len(outlier_rates)
                return max(0.0, 1.0 - avg_outlier_rate * 5)  # å¼‚å¸¸å€¼ç‡*5ä½œä¸ºæ‰£åˆ†
        elif check_name == 'data_types':
            total_cols = len(result.get('type_summary', {}))
            issues = len(result.get('type_issues', []))
            if total_cols > 0:
                return max(0.0, (total_cols - issues) / total_cols)
        elif check_name == 'completeness':
            # åŸºäºæ•°æ®å®Œæ•´æ€§è¯„åˆ†
            incomplete_count = len(result.get('incomplete_symbols', []))
            total_symbols = result.get('symbol_coverage', {}).get('total_symbols', 1)
            return max(0.0, (total_symbols - incomplete_count) / total_symbols)
        elif check_name == 'price_consistency':
            # åŸºäºä¸€è‡´æ€§é—®é¢˜æ•°é‡è¯„åˆ†
            issues = len(result.get('consistency_issues', []))
            return max(0.0, 1.0 - issues * 0.1)
        
        return 0.8  # é»˜è®¤å¾—åˆ†
    
    def _get_missing_severity(self, missing_rate: float) -> str:
        """è·å–ç¼ºå¤±æ•°æ®ä¸¥é‡ç¨‹åº¦"""
        if missing_rate == 0:
            return 'æ— '
        elif missing_rate <= 0.05:
            return 'è½»å¾®'
        elif missing_rate <= 0.15:
            return 'ä¸­ç­‰'
        elif missing_rate <= 0.30:
            return 'ä¸¥é‡'
        else:
            return 'æä¸¥é‡'
    
    def save_report(self, report: Dict[str, Any], file_path: Optional[str] = None) -> str:
        """ä¿å­˜è´¨é‡æŠ¥å‘Šåˆ°æ–‡ä»¶
        
        Args:
            report: è´¨é‡æŠ¥å‘Š
            file_path: ä¿å­˜è·¯å¾„
            
        Returns:
            str: ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        if file_path is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            file_path = f"data_quality_report_{timestamp}.json"
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        Path(file_path).parent.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜JSONæŠ¥å‘Š
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False, default=str)
        
        logger.info(f"âœ… è´¨é‡æŠ¥å‘Šå·²ä¿å­˜è‡³: {file_path}")
        return file_path