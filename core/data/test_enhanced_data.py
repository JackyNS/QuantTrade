#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆæ•°æ®æ¨¡å—æµ‹è¯•
=================

æµ‹è¯•æ‰€æœ‰æ–°å¢çš„æ•°æ®åŠŸèƒ½
"""

import sys
import os
from pathlib import Path
import pandas as pd
from datetime import datetime, timedelta
import logging

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from core.data.enhanced_data_manager import EnhancedDataManager
from core.data.adapters.data_source_manager import DataSourceManager
from core.data.quality_checker import DataQualityChecker
from core.data.cache_manager import SmartCacheManager

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def test_data_source_manager():
    """æµ‹è¯•æ•°æ®æºç®¡ç†å™¨"""
    logger.info("ğŸ§ª æµ‹è¯•æ•°æ®æºç®¡ç†å™¨...")
    
    try:
        # åˆ›å»ºæ•°æ®æºç®¡ç†å™¨
        dsm = DataSourceManager()
        
        # æµ‹è¯•è¿æ¥
        connection_results = dsm.test_all_connections()
        logger.info(f"è¿æ¥æµ‹è¯•ç»“æœ: {connection_results}")
        
        # è·å–å¯ç”¨æ•°æ®æº
        available_sources = dsm.get_available_sources()
        logger.info(f"å¯ç”¨æ•°æ®æº: {available_sources}")
        
        # æµ‹è¯•è·å–è‚¡ç¥¨åˆ—è¡¨ï¼ˆå¦‚æœæœ‰å¯ç”¨æ•°æ®æºï¼‰
        if available_sources:
            stock_list = dsm.get_stock_list()
            if not stock_list.empty:
                logger.info(f"âœ… è·å–è‚¡ç¥¨åˆ—è¡¨æˆåŠŸ: {len(stock_list)}åªè‚¡ç¥¨")
            else:
                logger.warning("âš ï¸ è‚¡ç¥¨åˆ—è¡¨ä¸ºç©º")
        
        # è·å–çŠ¶æ€æŠ¥å‘Š
        status_report = dsm.get_status_report()
        logger.info(f"æ•°æ®æºçŠ¶æ€æŠ¥å‘Š: {status_report}")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ æ•°æ®æºç®¡ç†å™¨æµ‹è¯•å¤±è´¥: {str(e)}")
        return False

def test_quality_checker():
    """æµ‹è¯•æ•°æ®è´¨é‡æ£€æŸ¥å™¨"""
    logger.info("ğŸ§ª æµ‹è¯•æ•°æ®è´¨é‡æ£€æŸ¥å™¨...")
    
    try:
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_data = pd.DataFrame({
            'date': pd.date_range('2023-01-01', periods=100),
            'symbol': ['TEST'] * 100,
            'open': [100 + i * 0.1 + (i % 10 - 5) * 0.5 for i in range(100)],
            'high': [100 + i * 0.1 + (i % 10 - 5) * 0.5 + 0.5 for i in range(100)],
            'low': [100 + i * 0.1 + (i % 10 - 5) * 0.5 - 0.5 for i in range(100)],
            'close': [100 + i * 0.1 + (i % 10 - 5) * 0.5 for i in range(100)],
            'volume': [1000000 + i * 10000 for i in range(100)]
        })
        
        # æ·»åŠ ä¸€äº›ç¼ºå¤±å€¼å’Œå¼‚å¸¸å€¼
        test_data.loc[10:15, 'volume'] = None
        test_data.loc[50, 'high'] = 1000  # å¼‚å¸¸å€¼
        
        # åˆ›å»ºè´¨é‡æ£€æŸ¥å™¨
        qc = DataQualityChecker()
        
        # æµ‹è¯•å„ç§æ£€æŸ¥åŠŸèƒ½
        missing_result = qc.check_missing_data(test_data, critical_columns=['date', 'symbol', 'close'])
        logger.info(f"âœ… ç¼ºå¤±æ•°æ®æ£€æŸ¥å®Œæˆ: {missing_result['overall']['missing_rate']:.2%}")
        
        outlier_result = qc.check_outliers(test_data)
        logger.info(f"âœ… å¼‚å¸¸å€¼æ£€æŸ¥å®Œæˆ")
        
        type_result = qc.check_data_types(test_data)
        logger.info(f"âœ… æ•°æ®ç±»å‹æ£€æŸ¥å®Œæˆ")
        
        consistency_result = qc.check_price_data_consistency(test_data)
        logger.info(f"âœ… ä»·æ ¼ä¸€è‡´æ€§æ£€æŸ¥å®Œæˆ")
        
        completeness_result = qc.check_completeness(test_data)
        logger.info(f"âœ… æ•°æ®å®Œæ•´æ€§æ£€æŸ¥å®Œæˆ")
        
        # ç”Ÿæˆç»¼åˆè´¨é‡æŠ¥å‘Š
        quality_report = qc.generate_quality_report(test_data, "æµ‹è¯•æ•°æ®è´¨é‡æŠ¥å‘Š")
        logger.info(f"âœ… è´¨é‡æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼Œæ•´ä½“å¾—åˆ†: {quality_report['overall_score']:.2f}")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ æ•°æ®è´¨é‡æ£€æŸ¥å™¨æµ‹è¯•å¤±è´¥: {str(e)}")
        return False

def test_cache_manager():
    """æµ‹è¯•ç¼“å­˜ç®¡ç†å™¨"""
    logger.info("ğŸ§ª æµ‹è¯•ç¼“å­˜ç®¡ç†å™¨...")
    
    try:
        # åˆ›å»ºç¼“å­˜ç®¡ç†å™¨
        cache_config = {
            'cache_dir': './test_cache',
            'max_memory_size': 10 * 1024 * 1024,  # 10MB
            'default_expire_hours': 1
        }
        cm = SmartCacheManager(cache_config)
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_data = pd.DataFrame({
            'col1': range(1000),
            'col2': [f'value_{i}' for i in range(1000)]
        })
        
        # æµ‹è¯•ç¼“å­˜å­˜å‚¨
        data_type = 'test_data'
        params = {'param1': 'value1', 'param2': 123}
        
        cm.put(data_type, params, test_data, expire_hours=2)
        logger.info("âœ… æ•°æ®ç¼“å­˜å­˜å‚¨æˆåŠŸ")
        
        # æµ‹è¯•ç¼“å­˜è·å–
        cached_data = cm.get(data_type, params)
        if cached_data is not None and len(cached_data) == len(test_data):
            logger.info("âœ… æ•°æ®ç¼“å­˜è·å–æˆåŠŸ")
        else:
            logger.error("âŒ æ•°æ®ç¼“å­˜è·å–å¤±è´¥")
        
        # æµ‹è¯•ç¼“å­˜ç»Ÿè®¡
        cache_stats = cm.get_cache_stats()
        logger.info(f"âœ… ç¼“å­˜ç»Ÿè®¡: å‘½ä¸­ç‡ {cache_stats['statistics']['hit_rate']:.2%}")
        
        # æ¸…ç†æµ‹è¯•ç¼“å­˜
        cm.clear_cache()
        logger.info("âœ… æµ‹è¯•ç¼“å­˜æ¸…ç†å®Œæˆ")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ ç¼“å­˜ç®¡ç†å™¨æµ‹è¯•å¤±è´¥: {str(e)}")
        return False

def test_enhanced_data_manager():
    """æµ‹è¯•å¢å¼ºç‰ˆæ•°æ®ç®¡ç†å™¨"""
    logger.info("ğŸ§ª æµ‹è¯•å¢å¼ºç‰ˆæ•°æ®ç®¡ç†å™¨...")
    
    try:
        # åˆ›å»ºå¢å¼ºç‰ˆæ•°æ®ç®¡ç†å™¨
        config = {
            'cache': {
                'cache_dir': './test_cache',
                'max_memory_size': 50 * 1024 * 1024,  # 50MB
                'default_expire_hours': 2
            },
            'quality': {
                'thresholds': {
                    'missing_rate': 0.1,
                    'outlier_zscore': 3.0
                }
            }
        }
        
        edm = EnhancedDataManager(config)
        
        # éªŒè¯æ•°æ®æµæ°´çº¿
        validation_result = edm.validate_data_pipeline()
        logger.info(f"âœ… æ•°æ®æµæ°´çº¿éªŒè¯: {validation_result['overall_status']}")
        
        # æµ‹è¯•è·å–è‚¡ç¥¨åˆ—è¡¨
        stock_list = edm.get_stock_list()
        if not stock_list.empty:
            logger.info(f"âœ… è·å–è‚¡ç¥¨åˆ—è¡¨æˆåŠŸ: {len(stock_list)}åªè‚¡ç¥¨")
            
            # é€‰æ‹©å‡ åªè‚¡ç¥¨æµ‹è¯•ä»·æ ¼æ•°æ®è·å–
            test_symbols = stock_list['symbol'].head(3).tolist()
            start_date = (datetime.now() - timedelta(days=30)).date()
            end_date = datetime.now().date()
            
            price_data = edm.get_price_data(
                symbols=test_symbols,
                start_date=start_date,
                end_date=end_date,
                use_cache=True,
                quality_check=True
            )
            
            if not price_data.empty:
                logger.info(f"âœ… è·å–ä»·æ ¼æ•°æ®æˆåŠŸ: {len(price_data)}æ¡è®°å½•")
                
                # æµ‹è¯•ç‰¹å¾ç”Ÿæˆ
                features = edm.generate_features(price_data)
                if not features.empty:
                    logger.info(f"âœ… ç‰¹å¾ç”ŸæˆæˆåŠŸ: {len(features.columns)}ä¸ªç‰¹å¾")
                else:
                    logger.warning("âš ï¸ ç‰¹å¾ç”Ÿæˆå¤±è´¥æˆ–æ— å¯ç”¨ç‰¹å¾å·¥ç¨‹å™¨")
            else:
                logger.warning("âš ï¸ ä»·æ ¼æ•°æ®ä¸ºç©º")
        else:
            logger.warning("âš ï¸ è‚¡ç¥¨åˆ—è¡¨ä¸ºç©ºï¼Œè·³è¿‡ä»·æ ¼æ•°æ®æµ‹è¯•")
        
        # è·å–ç¼“å­˜ç»Ÿè®¡
        cache_stats = edm.get_cache_statistics()
        logger.info(f"âœ… ç¼“å­˜ç»Ÿè®¡è·å–æˆåŠŸ")
        
        # è·å–æ•°æ®æºçŠ¶æ€
        data_source_status = edm.get_data_source_status()
        logger.info(f"âœ… æ•°æ®æºçŠ¶æ€è·å–æˆåŠŸ")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ å¢å¼ºç‰ˆæ•°æ®ç®¡ç†å™¨æµ‹è¯•å¤±è´¥: {str(e)}")
        return False

def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    logger.info("ğŸš€ å¼€å§‹è¿è¡Œå¢å¼ºç‰ˆæ•°æ®æ¨¡å—æµ‹è¯•...")
    
    tests = [
        ("æ•°æ®æºç®¡ç†å™¨", test_data_source_manager),
        ("æ•°æ®è´¨é‡æ£€æŸ¥å™¨", test_quality_checker),
        ("ç¼“å­˜ç®¡ç†å™¨", test_cache_manager),
        ("å¢å¼ºç‰ˆæ•°æ®ç®¡ç†å™¨", test_enhanced_data_manager)
    ]
    
    results = []
    
    for test_name, test_func in tests:
        logger.info(f"\n{'='*50}")
        logger.info(f"æµ‹è¯• {test_name}")
        logger.info('='*50)
        
        try:
            result = test_func()
            results.append((test_name, result))
            
            if result:
                logger.info(f"âœ… {test_name} æµ‹è¯•é€šè¿‡")
            else:
                logger.error(f"âŒ {test_name} æµ‹è¯•å¤±è´¥")
                
        except Exception as e:
            logger.error(f"âŒ {test_name} æµ‹è¯•å¼‚å¸¸: {str(e)}")
            results.append((test_name, False))
    
    # è¾“å‡ºæµ‹è¯•æ€»ç»“
    logger.info(f"\n{'='*50}")
    logger.info("æµ‹è¯•æ€»ç»“")
    logger.info('='*50)
    
    passed_count = sum(1 for _, result in results if result)
    total_count = len(results)
    
    for test_name, result in results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        logger.info(f"{test_name}: {status}")
    
    logger.info(f"\næ€»æµ‹è¯•æ•°: {total_count}")
    logger.info(f"é€šè¿‡æ•°: {passed_count}")
    logger.info(f"å¤±è´¥æ•°: {total_count - passed_count}")
    logger.info(f"é€šè¿‡ç‡: {passed_count/total_count*100:.1f}%")
    
    if passed_count == total_count:
        logger.info("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
    else:
        logger.error("ğŸ’¥ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼")
    
    return passed_count == total_count

if __name__ == "__main__":
    success = run_all_tests()
    sys.exit(0 if success else 1)