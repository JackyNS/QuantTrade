#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¾…åŠ©å‡½æ•°æ¨¡å— - utils/helpers.py
================================

æä¾›å¸¸ç”¨çš„è¾…åŠ©å‡½æ•°ï¼Œç®€åŒ–æ—¥å¸¸å¼€å‘ä»»åŠ¡ã€‚

ä¸»è¦åŠŸèƒ½:
- ğŸ“ æ–‡ä»¶æ“ä½œ
- ğŸ”¢ æ•°å­—æ ¼å¼åŒ–
- ğŸ“… æ—¥æœŸå¤„ç†
- ğŸ“Š æ•°æ®å¤„ç†
- ğŸ”§ é€šç”¨å·¥å…·

ä½¿ç”¨ç¤ºä¾‹:
```python
from core.utils import format_number, create_dirs, calculate_trading_days

# æ ¼å¼åŒ–æ•°å­—
print(format_number(1234567.89))  # "1,234,567.89"

# åˆ›å»ºç›®å½•
create_dirs(['data/raw', 'data/processed'])

# è®¡ç®—äº¤æ˜“æ—¥
days = calculate_trading_days('2024-01-01', '2024-12-31')
```

ä½œè€…: QuantTrader Team
ç‰ˆæœ¬: 1.0.0
æ›´æ–°: 2025-08-29
"""

import os
import re
import json
import pickle
import hashlib
import logging
from pathlib import Path
from typing import Any, Dict, List, Optional, Union, Callable, Iterable
from datetime import datetime, date, timedelta
import pandas as pd
import numpy as np
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
from functools import partial
import warnings

# è·å–æ—¥å¿—å™¨
logger = logging.getLogger(__name__)

# ==========================================
# æ–‡ä»¶å’Œç›®å½•æ“ä½œ
# ==========================================

def create_dirs(paths: Union[str, List[str]], exist_ok: bool = True) -> List[Path]:
    """
    åˆ›å»ºç›®å½•ï¼ˆæ”¯æŒæ‰¹é‡åˆ›å»ºï¼‰
    
    Args:
        paths: ç›®å½•è·¯å¾„æˆ–è·¯å¾„åˆ—è¡¨
        exist_ok: å¦‚æœç›®å½•å­˜åœ¨æ˜¯å¦æŠ¥é”™
    
    Returns:
        åˆ›å»ºçš„ç›®å½•Pathå¯¹è±¡åˆ—è¡¨
    """
    if isinstance(paths, str):
        paths = [paths]
    
    created_paths = []
    for path_str in paths:
        path = Path(path_str)
        try:
            path.mkdir(parents=True, exist_ok=exist_ok)
            created_paths.append(path)
            logger.debug(f"åˆ›å»ºç›®å½•: {path}")
        except Exception as e:
            logger.error(f"åˆ›å»ºç›®å½•å¤±è´¥ {path}: {e}")
    
    return created_paths

def clean_old_files(directory: str, 
                   days: int = 30,
                   pattern: str = '*',
                   dry_run: bool = False) -> List[str]:
    """
    æ¸…ç†æ—§æ–‡ä»¶
    
    Args:
        directory: ç›®å½•è·¯å¾„
        days: ä¿ç•™å¤©æ•°
        pattern: æ–‡ä»¶åŒ¹é…æ¨¡å¼
        dry_run: æ˜¯å¦ä»…æ¨¡æ‹Ÿè¿è¡Œ
    
    Returns:
        åˆ é™¤çš„æ–‡ä»¶åˆ—è¡¨
    """
    dir_path = Path(directory)
    if not dir_path.exists():
        logger.warning(f"ç›®å½•ä¸å­˜åœ¨: {directory}")
        return []
    
    cutoff_time = datetime.now().timestamp() - (days * 24 * 60 * 60)
    deleted_files = []
    
    for file_path in dir_path.glob(pattern):
        if file_path.is_file():
            if file_path.stat().st_mtime < cutoff_time:
                if not dry_run:
                    try:
                        file_path.unlink()
                        logger.info(f"åˆ é™¤æ—§æ–‡ä»¶: {file_path}")
                    except Exception as e:
                        logger.error(f"åˆ é™¤æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
                        continue
                deleted_files.append(str(file_path))
    
    return deleted_files

def save_json(data: Any, filepath: str, indent: int = 2, ensure_ascii: bool = False):
    """
    ä¿å­˜JSONæ–‡ä»¶
    
    Args:
        data: è¦ä¿å­˜çš„æ•°æ®
        filepath: æ–‡ä»¶è·¯å¾„
        indent: ç¼©è¿›ç©ºæ ¼æ•°
        ensure_ascii: æ˜¯å¦ç¡®ä¿ASCIIç¼–ç 
    """
    filepath = Path(filepath)
    filepath.parent.mkdir(parents=True, exist_ok=True)
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=indent, ensure_ascii=ensure_ascii, default=str)
    
    logger.debug(f"ä¿å­˜JSONæ–‡ä»¶: {filepath}")

def load_json(filepath: str, default: Any = None) -> Any:
    """
    åŠ è½½JSONæ–‡ä»¶
    
    Args:
        filepath: æ–‡ä»¶è·¯å¾„
        default: é»˜è®¤å€¼ï¼ˆæ–‡ä»¶ä¸å­˜åœ¨æ—¶è¿”å›ï¼‰
    
    Returns:
        åŠ è½½çš„æ•°æ®
    """
    filepath = Path(filepath)
    if not filepath.exists():
        logger.warning(f"JSONæ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
        return default
    
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        logger.error(f"åŠ è½½JSONæ–‡ä»¶å¤±è´¥ {filepath}: {e}")
        return default

def save_pickle(data: Any, filepath: str):
    """ä¿å­˜Pickleæ–‡ä»¶"""
    filepath = Path(filepath)
    filepath.parent.mkdir(parents=True, exist_ok=True)
    
    with open(filepath, 'wb') as f:
        pickle.dump(data, f)
    
    logger.debug(f"ä¿å­˜Pickleæ–‡ä»¶: {filepath}")

def load_pickle(filepath: str, default: Any = None) -> Any:
    """åŠ è½½Pickleæ–‡ä»¶"""
    filepath = Path(filepath)
    if not filepath.exists():
        logger.warning(f"Pickleæ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
        return default
    
    try:
        with open(filepath, 'rb') as f:
            return pickle.load(f)
    except Exception as e:
        logger.error(f"åŠ è½½Pickleæ–‡ä»¶å¤±è´¥ {filepath}: {e}")
        return default

# ==========================================
# æ•°å­—æ ¼å¼åŒ–
# ==========================================

def format_number(value: float, 
                 decimal_places: int = 2,
                 use_commas: bool = True,
                 prefix: str = '',
                 suffix: str = '') -> str:
    """
    æ ¼å¼åŒ–æ•°å­—
    
    Args:
        value: æ•°å€¼
        decimal_places: å°æ•°ä½æ•°
        use_commas: æ˜¯å¦ä½¿ç”¨åƒåˆ†ä½åˆ†éš”ç¬¦
        prefix: å‰ç¼€
        suffix: åç¼€
    
    Returns:
        æ ¼å¼åŒ–çš„å­—ç¬¦ä¸²
    """
    if pd.isna(value):
        return 'N/A'
    
    if use_commas:
        formatted = f"{value:,.{decimal_places}f}"
    else:
        formatted = f"{value:.{decimal_places}f}"
    
    return f"{prefix}{formatted}{suffix}"

def format_percentage(value: float, 
                     decimal_places: int = 2,
                     multiply: bool = True) -> str:
    """
    æ ¼å¼åŒ–ç™¾åˆ†æ¯”
    
    Args:
        value: æ•°å€¼
        decimal_places: å°æ•°ä½æ•°
        multiply: æ˜¯å¦ä¹˜ä»¥100
    
    Returns:
        æ ¼å¼åŒ–çš„ç™¾åˆ†æ¯”å­—ç¬¦ä¸²
    """
    if pd.isna(value):
        return 'N/A'
    
    if multiply:
        value = value * 100
    
    return f"{value:.{decimal_places}f}%"

def format_large_number(value: float, precision: int = 2) -> str:
    """
    æ ¼å¼åŒ–å¤§æ•°å­—ï¼ˆä½¿ç”¨K/M/Bç­‰å•ä½ï¼‰
    
    Args:
        value: æ•°å€¼
        precision: ç²¾åº¦
    
    Returns:
        æ ¼å¼åŒ–çš„å­—ç¬¦ä¸²
    """
    if pd.isna(value) or value == 0:
        return '0'
    
    units = ['', 'K', 'M', 'B', 'T']
    unit_index = 0
    abs_value = abs(value)
    
    while abs_value >= 1000 and unit_index < len(units) - 1:
        abs_value /= 1000
        unit_index += 1
    
    if value < 0:
        abs_value = -abs_value
    
    return f"{abs_value:.{precision}f}{units[unit_index]}"

# ==========================================
# æ—¥æœŸå¤„ç†
# ==========================================

def convert_to_datetime(date_input: Union[str, date, datetime],
                       format: Optional[str] = None) -> datetime:
    """
    è½¬æ¢ä¸ºdatetimeå¯¹è±¡
    
    Args:
        date_input: æ—¥æœŸè¾“å…¥
        format: æ—¥æœŸæ ¼å¼
    
    Returns:
        datetimeå¯¹è±¡
    """
    if isinstance(date_input, datetime):
        return date_input
    elif isinstance(date_input, date):
        return datetime.combine(date_input, datetime.min.time())
    elif isinstance(date_input, str):
        if format:
            return datetime.strptime(date_input, format)
        else:
            return pd.to_datetime(date_input)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ—¥æœŸç±»å‹: {type(date_input)}")

def calculate_trading_days(start_date: Union[str, datetime],
                          end_date: Union[str, datetime],
                          holidays: Optional[List[str]] = None) -> int:
    """
    è®¡ç®—äº¤æ˜“æ—¥æ•°é‡ï¼ˆç®€åŒ–ç‰ˆï¼‰
    
    Args:
        start_date: å¼€å§‹æ—¥æœŸ
        end_date: ç»“æŸæ—¥æœŸ
        holidays: èŠ‚å‡æ—¥åˆ—è¡¨
    
    Returns:
        äº¤æ˜“æ—¥æ•°é‡
    """
    start = convert_to_datetime(start_date)
    end = convert_to_datetime(end_date)
    
    # ç”Ÿæˆæ—¥æœŸèŒƒå›´
    date_range = pd.date_range(start, end, freq='B')  # B = business days
    
    # æ’é™¤èŠ‚å‡æ—¥
    if holidays:
        holiday_dates = pd.to_datetime(holidays)
        date_range = date_range.difference(holiday_dates)
    
    return len(date_range)

def get_previous_trading_day(date: Union[str, datetime],
                            holidays: Optional[List[str]] = None) -> datetime:
    """è·å–ä¸Šä¸€ä¸ªäº¤æ˜“æ—¥"""
    current = convert_to_datetime(date)
    
    while True:
        current = current - timedelta(days=1)
        # è·³è¿‡å‘¨æœ«
        if current.weekday() < 5:  # Monday = 0, Friday = 4
            # æ£€æŸ¥æ˜¯å¦ä¸ºèŠ‚å‡æ—¥
            if holidays and current.strftime('%Y-%m-%d') in holidays:
                continue
            return current

def get_next_trading_day(date: Union[str, datetime],
                         holidays: Optional[List[str]] = None) -> datetime:
    """è·å–ä¸‹ä¸€ä¸ªäº¤æ˜“æ—¥"""
    current = convert_to_datetime(date)
    
    while True:
        current = current + timedelta(days=1)
        # è·³è¿‡å‘¨æœ«
        if current.weekday() < 5:
            # æ£€æŸ¥æ˜¯å¦ä¸ºèŠ‚å‡æ—¥
            if holidays and current.strftime('%Y-%m-%d') in holidays:
                continue
            return current

# ==========================================
# è‚¡ç¥¨ç›¸å…³
# ==========================================

def get_stock_name(code: str) -> str:
    """
    è·å–è‚¡ç¥¨åç§°ï¼ˆç¤ºä¾‹å‡½æ•°ï¼Œå®é™…éœ€è¦ä»æ•°æ®æºè·å–ï¼‰
    
    Args:
        code: è‚¡ç¥¨ä»£ç 
    
    Returns:
        è‚¡ç¥¨åç§°
    """
    # è¿™é‡Œåªæ˜¯ç¤ºä¾‹ï¼Œå®é™…åº”è¯¥ä»æ•°æ®åº“æˆ–APIè·å–
    stock_names = {
        '000001.SZ': 'å¹³å®‰é“¶è¡Œ',
        '000002.SZ': 'ä¸‡ç§‘A',
        '600000.SH': 'æµ¦å‘é“¶è¡Œ',
        '600036.SH': 'æ‹›å•†é“¶è¡Œ',
        '000858.SZ': 'äº”ç²®æ¶²',
    }
    
    return stock_names.get(code, code)

def parse_stock_code(code: str) -> Dict[str, str]:
    """
    è§£æè‚¡ç¥¨ä»£ç 
    
    Args:
        code: è‚¡ç¥¨ä»£ç ï¼ˆå¦‚ '000001.SZ'ï¼‰
    
    Returns:
        åŒ…å«ä»£ç å’Œå¸‚åœºä¿¡æ¯çš„å­—å…¸
    """
    match = re.match(r'^(\d{6})\.([A-Z]{2})$', code)
    if match:
        return {
            'code': match.group(1),
            'market': match.group(2),
            'full_code': code
        }
    return {'code': code, 'market': '', 'full_code': code}

# ==========================================
# æ•°æ®å¤„ç†
# ==========================================

def chunk_list(lst: List[Any], chunk_size: int) -> List[List[Any]]:
    """
    å°†åˆ—è¡¨åˆ†å—
    
    Args:
        lst: åŸå§‹åˆ—è¡¨
        chunk_size: å—å¤§å°
    
    Returns:
        åˆ†å—åçš„åˆ—è¡¨
    """
    return [lst[i:i + chunk_size] for i in range(0, len(lst), chunk_size)]

def parallel_process(func: Callable,
                    items: Iterable,
                    max_workers: Optional[int] = None,
                    use_process: bool = False,
                    show_progress: bool = False) -> List[Any]:
    """
    å¹¶è¡Œå¤„ç†æ•°æ®
    
    Args:
        func: å¤„ç†å‡½æ•°
        items: å¾…å¤„ç†é¡¹
        max_workers: æœ€å¤§å·¥ä½œçº¿ç¨‹/è¿›ç¨‹æ•°
        use_process: æ˜¯å¦ä½¿ç”¨è¿›ç¨‹æ± ï¼ˆé»˜è®¤çº¿ç¨‹æ± ï¼‰
        show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦
    
    Returns:
        å¤„ç†ç»“æœåˆ—è¡¨
    """
    executor_class = ProcessPoolExecutor if use_process else ThreadPoolExecutor
    
    results = []
    with executor_class(max_workers=max_workers) as executor:
        futures = [executor.submit(func, item) for item in items]
        
        for i, future in enumerate(futures):
            try:
                result = future.result()
                results.append(result)
                
                if show_progress and (i + 1) % 10 == 0:
                    logger.info(f"å¤„ç†è¿›åº¦: {i + 1}/{len(futures)}")
                    
            except Exception as e:
                logger.error(f"å¹¶è¡Œå¤„ç†é”™è¯¯: {e}")
                results.append(None)
    
    return results

def safe_divide(numerator: float, 
               denominator: float,
               default: float = 0.0) -> float:
    """
    å®‰å…¨é™¤æ³•
    
    Args:
        numerator: åˆ†å­
        denominator: åˆ†æ¯
        default: é»˜è®¤å€¼ï¼ˆåˆ†æ¯ä¸º0æ—¶è¿”å›ï¼‰
    
    Returns:
        é™¤æ³•ç»“æœ
    """
    if denominator == 0 or pd.isna(denominator):
        return default
    return numerator / denominator

def moving_average(data: Union[List[float], pd.Series],
                  window: int,
                  min_periods: Optional[int] = None) -> pd.Series:
    """
    è®¡ç®—ç§»åŠ¨å¹³å‡
    
    Args:
        data: æ•°æ®
        window: çª—å£å¤§å°
        min_periods: æœ€å°å‘¨æœŸæ•°
    
    Returns:
        ç§»åŠ¨å¹³å‡åºåˆ—
    """
    if not isinstance(data, pd.Series):
        data = pd.Series(data)
    
    return data.rolling(window=window, min_periods=min_periods).mean()

def exponential_smoothing(data: Union[List[float], pd.Series],
                         alpha: float = 0.3) -> pd.Series:
    """
    æŒ‡æ•°å¹³æ»‘
    
    Args:
        data: æ•°æ®
        alpha: å¹³æ»‘ç³»æ•°ï¼ˆ0-1ï¼‰
    
    Returns:
        å¹³æ»‘åçš„åºåˆ—
    """
    if not isinstance(data, pd.Series):
        data = pd.Series(data)
    
    return data.ewm(alpha=alpha, adjust=False).mean()

# ==========================================
# å…¶ä»–å·¥å…·å‡½æ•°
# ==========================================

def generate_hash(data: Any, algorithm: str = 'md5') -> str:
    """
    ç”Ÿæˆæ•°æ®å“ˆå¸Œå€¼
    
    Args:
        data: æ•°æ®
        algorithm: å“ˆå¸Œç®—æ³•
    
    Returns:
        å“ˆå¸Œå­—ç¬¦ä¸²
    """
    if algorithm == 'md5':
        hasher = hashlib.md5()
    elif algorithm == 'sha256':
        hasher = hashlib.sha256()
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„å“ˆå¸Œç®—æ³•: {algorithm}")
    
    # è½¬æ¢ä¸ºå­—èŠ‚
    if isinstance(data, str):
        data_bytes = data.encode('utf-8')
    else:
        data_bytes = pickle.dumps(data)
    
    hasher.update(data_bytes)
    return hasher.hexdigest()

def retry_on_exception(func: Callable,
                      max_retries: int = 3,
                      delay: float = 1.0,
                      exceptions: tuple = (Exception,)) -> Any:
    """
    å¼‚å¸¸é‡è¯•æ‰§è¡Œ
    
    Args:
        func: è¦æ‰§è¡Œçš„å‡½æ•°
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        delay: é‡è¯•å»¶è¿Ÿ
        exceptions: è¦æ•è·çš„å¼‚å¸¸ç±»å‹
    
    Returns:
        å‡½æ•°æ‰§è¡Œç»“æœ
    """
    import time
    
    for attempt in range(max_retries):
        try:
            return func()
        except exceptions as e:
            if attempt == max_retries - 1:
                raise
            logger.warning(f"æ‰§è¡Œå¤±è´¥ï¼Œ{delay}ç§’åé‡è¯•: {e}")
            time.sleep(delay)

def flatten_dict(d: Dict[str, Any], 
                 parent_key: str = '',
                 sep: str = '.') -> Dict[str, Any]:
    """
    å±•å¹³åµŒå¥—å­—å…¸
    
    Args:
        d: åµŒå¥—å­—å…¸
        parent_key: çˆ¶é”®
        sep: åˆ†éš”ç¬¦
    
    Returns:
        å±•å¹³åçš„å­—å…¸
    """
    items = []
    for k, v in d.items():
        new_key = f"{parent_key}{sep}{k}" if parent_key else k
        if isinstance(v, dict):
            items.extend(flatten_dict(v, new_key, sep=sep).items())
        else:
            items.append((new_key, v))
    return dict(items)

# ==========================================
# å¯¼å‡ºæ¥å£
# ==========================================

__all__ = [
    # æ–‡ä»¶æ“ä½œ
    'create_dirs',
    'clean_old_files',
    'save_json',
    'load_json',
    'save_pickle',
    'load_pickle',
    
    # æ•°å­—æ ¼å¼åŒ–
    'format_number',
    'format_percentage',
    'format_large_number',
    
    # æ—¥æœŸå¤„ç†
    'convert_to_datetime',
    'calculate_trading_days',
    'get_previous_trading_day',
    'get_next_trading_day',
    
    # è‚¡ç¥¨ç›¸å…³
    'get_stock_name',
    'parse_stock_code',
    
    # æ•°æ®å¤„ç†
    'chunk_list',
    'parallel_process',
    'safe_divide',
    'moving_average',
    'exponential_smoothing',
    
    # å…¶ä»–å·¥å…·
    'generate_hash',
    'retry_on_exception',
    'flatten_dict'
]