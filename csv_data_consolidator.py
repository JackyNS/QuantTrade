#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CSVæ•°æ®æ•´åˆå™¨ - æ•´åˆæ‰€æœ‰CSVæ•°æ®ç›®å½•å¹¶æ£€æŸ¥APIå®Œæ•´æ€§
"""

import pandas as pd
from pathlib import Path
import json
import logging
from datetime import datetime
from collections import defaultdict
import shutil

class CSVDataConsolidator:
    """CSVæ•°æ®æ•´åˆå™¨"""
    
    def __init__(self):
        # å„ä¸ªæ•°æ®æºç›®å½•
        self.data_sources = {
            "final_comprehensive_download": {
                "path": Path("data/final_comprehensive_download"),
                "description": "å®Œæ•´ä¸‹è½½æ•°æ®",
                "size_gb": 204,
                "priority": 1  # æœ€é«˜ä¼˜å…ˆçº§
            },
            "optimized_data": {
                "path": Path("data/optimized_data"),  
                "description": "ä¼˜åŒ–CSVæ•°æ®",
                "size_gb": 5.5,
                "priority": 2
            },
            "priority_download": {
                "path": Path("data/priority_download"),
                "description": "ä¼˜å…ˆä¸‹è½½æ•°æ®", 
                "size_gb": 3.0,
                "priority": 3
            },
            "historical_download": {
                "path": Path("data/historical_download"),
                "description": "å†å²åŸºç¡€æ•°æ®",
                "size_gb": 1.4,
                "priority": 4
            },
            "smart_download": {
                "path": Path("data/smart_download"),
                "description": "æ™ºèƒ½ä¸‹è½½æ•°æ®",
                "size_gb": 1.3,
                "priority": 5
            }
        }
        
        # å·²çŸ¥çš„60+ä¸ªAPIåˆ—è¡¨
        self.expected_apis = {
            "basic_info": [
                "equget", "secidget", "equindustryget", "equipoget", 
                "equdivget", "equsplitsget", "mktidxdget"
            ],
            "financial": [
                "fdmtbsalllatestget", "fdmtbsbankalllatestget", "fdmtbsindualllatestget",
                "fdmtisbankalllatestget", "fdmtisindualllatestget", "fdmtisalllatestget",
                "fdmtcfalllatestget", "fdmtcfbankalllatestget", "fdmtcfindualllatestget",
                "fdmtindipsget", "fdmtindigrowthget", "fdmtderget"
            ],
            "special_trading": [
                "mktlimitget", "mktblockdget", "fstdetailget", "fsttotalget",
                "mktconsbondpremiumget", "mktrankinstr", "mktrankdivyieldget",
                "equisactivityget", "equisparticipantqaget", "sechaltget",
                "mktequdstatsget", "mktipocontraddaysget", "mktranklistsalesget",
                "mktrankliststocksget", "vfsttargetget", "equmarginsecget",
                "mktrankinsttrget", "fdmtee"
            ],
            "governance": [
                "equshareholdernumget", "equshtenget", "equfloatshtenget",
                "equpledgeget", "equshareholdersmeetingget", "equsharesexcitget",
                "equsharesfloatget", "equexecsholdingsget", "equmschangesget",
                "equrelatedtransactionget", "equiposharefloatget", "equreformsharefloatget",
                "equactualcontrollerget", "equmanagersget", "equpartynatureget",
                "equallotget", "equallotmentsubscriptionresultsget", "equchangeplanget",
                "equoldshofferget", "equspoget", "equspopubresultget",
                "equstockpledgeget"
            ]
        }
        
        self.consolidated_dir = Path("data/consolidated_csv")
        self.setup_logging()
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        log_file = Path("csv_consolidation.log")
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
    
    def scan_all_data_sources(self):
        """æ‰«ææ‰€æœ‰æ•°æ®æº"""
        logging.info("ğŸ” æ‰«ææ‰€æœ‰CSVæ•°æ®æº...")
        
        all_apis = defaultdict(lambda: defaultdict(list))
        source_stats = {}
        
        for source_name, source_info in self.data_sources.items():
            source_path = source_info["path"]
            
            if not source_path.exists():
                logging.warning(f"âš ï¸ æ•°æ®æºä¸å­˜åœ¨: {source_name} - {source_path}")
                continue
            
            logging.info(f"ğŸ“‚ æ‰«æ {source_name}...")
            
            stats = {
                "categories": 0,
                "apis": 0,
                "files": 0,
                "size_mb": 0
            }
            
            # æ‰«æç±»åˆ«
            for category_dir in source_path.iterdir():
                if not category_dir.is_dir():
                    continue
                
                category_name = category_dir.name
                stats["categories"] += 1
                
                # æ‰«æAPI
                for api_dir in category_dir.iterdir():
                    if not api_dir.is_dir():
                        continue
                    
                    api_name = api_dir.name
                    csv_files = list(api_dir.glob("*.csv"))
                    
                    if csv_files:
                        stats["apis"] += 1
                        stats["files"] += len(csv_files)
                        
                        # è®°å½•APIä¿¡æ¯
                        all_apis[category_name][api_name].append({
                            "source": source_name,
                            "path": api_dir,
                            "files": len(csv_files),
                            "file_list": [f.name for f in csv_files],
                            "size_mb": sum(f.stat().st_size for f in csv_files) / 1024 / 1024,
                            "priority": source_info["priority"]
                        })
                        
                        stats["size_mb"] += sum(f.stat().st_size for f in csv_files) / 1024 / 1024
            
            source_stats[source_name] = stats
            logging.info(f"  âœ… {stats['categories']} ç±»åˆ«, {stats['apis']} API, {stats['files']} æ–‡ä»¶, {stats['size_mb']/1024:.1f}GB")
        
        return all_apis, source_stats
    
    def check_api_completeness(self, all_apis):
        """æ£€æŸ¥APIå®Œæ•´æ€§"""
        logging.info("ğŸ” æ£€æŸ¥60+ä¸ªAPIä¸‹è½½å®Œæ•´æ€§...")
        
        completeness_report = {
            "expected_total": 0,
            "found_total": 0,
            "categories": {},
            "missing_apis": [],
            "extra_apis": []
        }
        
        for category, expected_apis in self.expected_apis.items():
            found_apis = list(all_apis.get(category, {}).keys())
            
            missing = set(expected_apis) - set(found_apis)
            extra = set(found_apis) - set(expected_apis)
            
            category_report = {
                "expected": len(expected_apis),
                "found": len(found_apis),
                "missing": list(missing),
                "extra": list(extra),
                "completeness": (len(expected_apis) - len(missing)) / len(expected_apis) * 100
            }
            
            completeness_report["categories"][category] = category_report
            completeness_report["expected_total"] += len(expected_apis)
            completeness_report["found_total"] += len(found_apis) - len(extra)  # åªè®¡ç®—æœŸæœ›çš„API
            
            if missing:
                completeness_report["missing_apis"].extend([f"{category}/{api}" for api in missing])
            
            if extra:
                completeness_report["extra_apis"].extend([f"{category}/{api}" for api in extra])
            
            # è¾“å‡ºç±»åˆ«æŠ¥å‘Š
            status = "âœ…" if len(missing) == 0 else "âš ï¸"
            logging.info(f"{status} {category}: {len(found_apis)}/{len(expected_apis)} ({category_report['completeness']:.1f}%)")
            
            if missing:
                logging.warning(f"   ç¼ºå¤±API: {missing}")
            
            if extra:
                logging.info(f"   é¢å¤–API: {extra}")
        
        # æ€»ä½“å®Œæ•´æ€§
        overall_completeness = completeness_report["found_total"] / completeness_report["expected_total"] * 100
        completeness_report["overall_completeness"] = overall_completeness
        
        logging.info("=" * 60)
        logging.info(f"ğŸ“Š **APIå®Œæ•´æ€§æ€»ç»“**:")
        logging.info(f"  æœŸæœ›APIæ€»æ•°: {completeness_report['expected_total']}")
        logging.info(f"  æ‰¾åˆ°APIæ€»æ•°: {completeness_report['found_total']}")
        logging.info(f"  æ€»ä½“å®Œæ•´æ€§: {overall_completeness:.1f}%")
        
        if completeness_report["missing_apis"]:
            logging.warning(f"  ç¼ºå¤±API: {len(completeness_report['missing_apis'])} ä¸ª")
        
        if completeness_report["extra_apis"]:
            logging.info(f"  é¢å¤–API: {len(completeness_report['extra_apis'])} ä¸ª")
        
        return completeness_report
    
    def analyze_data_overlap(self, all_apis):
        """åˆ†ææ•°æ®é‡å å’Œä¼˜å…ˆçº§"""
        logging.info("ğŸ” åˆ†ææ•°æ®é‡å å’Œæ¥æºä¼˜å…ˆçº§...")
        
        overlap_report = {
            "multiple_sources": {},
            "single_source": {},
            "recommended_consolidation": {}
        }
        
        for category, apis in all_apis.items():
            for api_name, sources in apis.items():
                if len(sources) > 1:
                    # å¤šä¸ªæ¥æºï¼Œéœ€è¦é€‰æ‹©
                    sources_sorted = sorted(sources, key=lambda x: x["priority"])
                    best_source = sources_sorted[0]  # ä¼˜å…ˆçº§æœ€é«˜çš„
                    
                    overlap_report["multiple_sources"][f"{category}/{api_name}"] = {
                        "sources": [s["source"] for s in sources],
                        "recommended": best_source["source"],
                        "files": best_source["files"],
                        "size_mb": best_source["size_mb"]
                    }
                    
                    logging.info(f"ğŸ”„ {category}/{api_name}: {len(sources)} ä¸ªæ¥æº")
                    logging.info(f"   æ¨è: {best_source['source']} ({best_source['files']} æ–‡ä»¶, {best_source['size_mb']:.1f}MB)")
                    
                else:
                    # å•ä¸€æ¥æº
                    source = sources[0]
                    overlap_report["single_source"][f"{category}/{api_name}"] = {
                        "source": source["source"],
                        "files": source["files"],
                        "size_mb": source["size_mb"]
                    }
        
        logging.info(f"ğŸ“Š æ•°æ®æºé‡å åˆ†æ:")
        logging.info(f"  å•ä¸€æ¥æºAPI: {len(overlap_report['single_source'])}")
        logging.info(f"  å¤šæ¥æºAPI: {len(overlap_report['multiple_sources'])}")
        
        return overlap_report
    
    def create_consolidation_plan(self, all_apis, overlap_report):
        """åˆ›å»ºæ•°æ®æ•´åˆè®¡åˆ’"""
        logging.info("ğŸ“‹ åˆ›å»ºæ•°æ®æ•´åˆè®¡åˆ’...")
        
        consolidation_plan = {
            "target_structure": {},
            "copy_operations": [],
            "total_files": 0,
            "total_size_gb": 0
        }
        
        for category, apis in all_apis.items():
            consolidation_plan["target_structure"][category] = {}
            
            for api_name, sources in apis.items():
                # é€‰æ‹©æœ€ä½³æ•°æ®æº
                if len(sources) > 1:
                    # å¤šæ¥æºé€‰æ‹©ä¼˜å…ˆçº§æœ€é«˜çš„
                    best_source = sorted(sources, key=lambda x: x["priority"])[0]
                else:
                    best_source = sources[0]
                
                source_path = best_source["path"]
                target_path = self.consolidated_dir / category / api_name
                
                consolidation_plan["target_structure"][category][api_name] = {
                    "source_path": str(source_path),
                    "target_path": str(target_path),
                    "files": best_source["files"],
                    "size_mb": best_source["size_mb"],
                    "source_name": best_source["source"]
                }
                
                consolidation_plan["copy_operations"].append({
                    "source": str(source_path),
                    "target": str(target_path),
                    "files": best_source["files"],
                    "size_mb": best_source["size_mb"]
                })
                
                consolidation_plan["total_files"] += best_source["files"]
                consolidation_plan["total_size_gb"] += best_source["size_mb"] / 1024
        
        logging.info(f"ğŸ“Š æ•´åˆè®¡åˆ’ç»Ÿè®¡:")
        logging.info(f"  ç›®æ ‡æ–‡ä»¶æ•°: {consolidation_plan['total_files']}")
        logging.info(f"  ç›®æ ‡å¤§å°: {consolidation_plan['total_size_gb']:.1f} GB")
        logging.info(f"  å¤åˆ¶æ“ä½œ: {len(consolidation_plan['copy_operations'])}")
        
        return consolidation_plan
    
    def execute_consolidation(self, consolidation_plan):
        """æ‰§è¡Œæ•°æ®æ•´åˆ"""
        logging.info("ğŸš€ å¼€å§‹æ‰§è¡Œæ•°æ®æ•´åˆ...")
        
        # åˆ›å»ºç›®æ ‡ç›®å½•
        if self.consolidated_dir.exists():
            logging.warning("ç›®æ ‡ç›®å½•å·²å­˜åœ¨ï¼Œæ˜¯å¦ç»§ç»­ï¼Ÿ(è¿™å°†è¦†ç›–ç°æœ‰æ•°æ®)")
            return False
        
        self.consolidated_dir.mkdir(exist_ok=True)
        
        copied_files = 0
        copied_size = 0
        
        for i, operation in enumerate(consolidation_plan["copy_operations"], 1):
            source_path = Path(operation["source"])
            target_path = Path(operation["target"])
            
            logging.info(f"ğŸ“ [{i}/{len(consolidation_plan['copy_operations'])}] å¤åˆ¶ {source_path.name}")
            logging.info(f"   ä»: {source_path}")
            logging.info(f"   åˆ°: {target_path}")
            
            try:
                # åˆ›å»ºç›®æ ‡ç›®å½•
                target_path.mkdir(parents=True, exist_ok=True)
                
                # å¤åˆ¶æ‰€æœ‰CSVæ–‡ä»¶
                for csv_file in source_path.glob("*.csv"):
                    target_file = target_path / csv_file.name
                    shutil.copy2(csv_file, target_file)
                    copied_files += 1
                    copied_size += csv_file.stat().st_size
                
                logging.info(f"   âœ… å®Œæˆ ({operation['files']} æ–‡ä»¶)")
                
            except Exception as e:
                logging.error(f"   âŒ å¤±è´¥: {e}")
                return False
        
        logging.info("ğŸŠ æ•°æ®æ•´åˆå®Œæˆ!")
        logging.info(f"ğŸ“Š æ•´åˆç»“æœ:")
        logging.info(f"  å¤åˆ¶æ–‡ä»¶: {copied_files}")
        logging.info(f"  å¤åˆ¶å¤§å°: {copied_size/1024/1024/1024:.1f} GB")
        logging.info(f"  ç›®æ ‡ç›®å½•: {self.consolidated_dir}")
        
        return True
    
    def generate_comprehensive_report(self):
        """ç”Ÿæˆç»¼åˆæŠ¥å‘Š"""
        logging.info("ğŸ“‹ ç”ŸæˆCSVæ•°æ®ç»¼åˆåˆ†ææŠ¥å‘Š...")
        
        # 1. æ‰«ææ‰€æœ‰æ•°æ®æº
        all_apis, source_stats = self.scan_all_data_sources()
        
        # 2. æ£€æŸ¥APIå®Œæ•´æ€§
        completeness_report = self.check_api_completeness(all_apis)
        
        # 3. åˆ†ææ•°æ®é‡å 
        overlap_report = self.analyze_data_overlap(all_apis)
        
        # 4. åˆ›å»ºæ•´åˆè®¡åˆ’
        consolidation_plan = self.create_consolidation_plan(all_apis, overlap_report)
        
        # 5. ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        comprehensive_report = {
            "analysis_time": datetime.now().isoformat(),
            "data_sources": {
                "scanned": list(self.data_sources.keys()),
                "stats": source_stats
            },
            "api_completeness": completeness_report,
            "overlap_analysis": overlap_report,
            "consolidation_plan": consolidation_plan,
            "recommendations": []
        }
        
        # ç”Ÿæˆå»ºè®®
        if completeness_report["overall_completeness"] >= 95:
            comprehensive_report["recommendations"].append("âœ… APIä¸‹è½½å®Œæ•´æ€§è‰¯å¥½ï¼Œæ•°æ®æ”¶é›†åŸºæœ¬å®Œæˆ")
        else:
            comprehensive_report["recommendations"].append(f"âš ï¸ æœ‰ {len(completeness_report['missing_apis'])} ä¸ªAPIç¼ºå¤±ï¼Œå»ºè®®è¡¥å……ä¸‹è½½")
        
        if len(overlap_report["multiple_sources"]) > 0:
            comprehensive_report["recommendations"].append(f"ğŸ”„ å‘ç° {len(overlap_report['multiple_sources'])} ä¸ªAPIæœ‰å¤šä¸ªæ•°æ®æºï¼Œå»ºè®®æ•´åˆ")
        
        comprehensive_report["recommendations"].append(f"ğŸ’¾ å»ºè®®åˆ›å»ºç»Ÿä¸€æ•°æ®ç›®å½•ï¼Œæ•´åˆåå¤§å°çº¦ {consolidation_plan['total_size_gb']:.1f} GB")
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = Path("csv_data_comprehensive_report.json")
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(comprehensive_report, f, indent=2, ensure_ascii=False)
        
        logging.info(f"ğŸ“„ ç»¼åˆæŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        
        # è¾“å‡ºæœ€ç»ˆå»ºè®®
        logging.info("ğŸ¯ **æœ€ç»ˆå»ºè®®**:")
        for recommendation in comprehensive_report["recommendations"]:
            logging.info(recommendation)
        
        return comprehensive_report

if __name__ == "__main__":
    consolidator = CSVDataConsolidator()
    report = consolidator.generate_comprehensive_report()