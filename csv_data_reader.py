#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CSVæ•°æ®è¯»å–å·¥å…· - çº¯CSVæ ¼å¼çš„æ•°æ®è®¿é—®æ¥å£
"""

import pandas as pd
from pathlib import Path
import json
from datetime import datetime
import logging

class CSVDataReader:
    """CSVæ•°æ®è¯»å–å™¨"""
    
    def __init__(self):
        # ä¸»è¦æ•°æ®æºç›®å½•
        self.main_data_dir = Path("data/final_comprehensive_download")
        self.optimized_data_dir = Path("data/optimized_data")  
        self.setup_logging()
        
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
    
    def get_available_datasets(self):
        """è·å–æ‰€æœ‰å¯ç”¨çš„æ•°æ®é›†"""
        datasets = {}
        
        # æ‰«æä¸»è¦æ•°æ®æº
        if self.main_data_dir.exists():
            datasets["comprehensive"] = self._scan_directory(
                self.main_data_dir, "å®Œæ•´ä¸‹è½½æ•°æ® (204GB)"
            )
        
        # æ‰«æä¼˜åŒ–æ•°æ®æº
        if self.optimized_data_dir.exists():
            datasets["optimized"] = self._scan_directory(
                self.optimized_data_dir, "ä¼˜åŒ–æ•°æ® (5.5GB)"
            )
        
        return datasets
    
    def _scan_directory(self, directory, description):
        """æ‰«æç›®å½•è·å–æ•°æ®é›†ä¿¡æ¯"""
        info = {
            "description": description,
            "categories": {},
            "total_files": 0,
            "total_size_gb": 0
        }
        
        for category_dir in directory.iterdir():
            if not category_dir.is_dir():
                continue
                
            category_info = {
                "apis": {},
                "file_count": 0,
                "size_gb": 0
            }
            
            for api_dir in category_dir.iterdir():
                if not api_dir.is_dir():
                    continue
                    
                csv_files = list(api_dir.glob("*.csv"))
                if not csv_files:
                    continue
                    
                api_size = sum(f.stat().st_size for f in csv_files) / 1024 / 1024 / 1024
                
                category_info["apis"][api_dir.name] = {
                    "files": len(csv_files),
                    "size_gb": round(api_size, 2),
                    "files_list": [f.name for f in csv_files[:5]]  # åªæ˜¾ç¤ºå‰5ä¸ª
                }
                category_info["file_count"] += len(csv_files)
                category_info["size_gb"] += api_size
            
            if category_info["file_count"] > 0:
                info["categories"][category_dir.name] = category_info
                info["total_files"] += category_info["file_count"]
                info["total_size_gb"] += category_info["size_gb"]
        
        return info
    
    def read_data(self, dataset="comprehensive", category=None, api=None, 
                  filename=None, max_rows=None, use_chunks=False, chunk_size=10000):
        """è¯»å–CSVæ•°æ®"""
        
        logging.info(f"ğŸ“Š æŸ¥è¯¢å‚æ•°: dataset={dataset}, category={category}, api={api}, file={filename}")
        
        # ç¡®å®šæ•°æ®ç›®å½•
        if dataset == "comprehensive":
            base_dir = self.main_data_dir
        elif dataset == "optimized":
            base_dir = self.optimized_data_dir
        else:
            logging.error("âŒ æ— æ•ˆçš„æ•°æ®é›†åç§°ï¼Œè¯·ä½¿ç”¨ 'comprehensive' æˆ– 'optimized'")
            return None
        
        if not base_dir.exists():
            logging.error(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {base_dir}")
            return None
        
        # æ„å»ºæ–‡ä»¶è·¯å¾„
        if not (category and api):
            logging.error("âŒ è¯·æä¾›categoryå’Œapiå‚æ•°")
            return None
            
        api_dir = base_dir / category / api
        
        if not api_dir.exists():
            logging.error(f"âŒ APIç›®å½•ä¸å­˜åœ¨: {api_dir}")
            return None
            
        if filename:
            file_path = api_dir / filename
            if not file_path.suffix:
                file_path = file_path.with_suffix('.csv')
        else:
            # åˆ—å‡ºAPIä¸‹çš„æ‰€æœ‰æ–‡ä»¶
            csv_files = list(api_dir.glob("*.csv"))
            if csv_files:
                file_path = csv_files[0]  # å–ç¬¬ä¸€ä¸ªæ–‡ä»¶
                logging.info(f"ğŸ“ æ‰¾åˆ° {len(csv_files)} ä¸ªæ–‡ä»¶ï¼ŒåŠ è½½: {file_path.name}")
            else:
                logging.error(f"âŒ æœªæ‰¾åˆ°CSVæ–‡ä»¶: {api_dir}")
                return None
        
        # è¯»å–æ•°æ®
        try:
            if not file_path.exists():
                logging.error(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                return None
            
            logging.info(f"ğŸ“– è¯»å–æ–‡ä»¶: {file_path}")
            
            # è·å–æ–‡ä»¶ä¿¡æ¯
            file_size = file_path.stat().st_size / 1024 / 1024
            logging.info(f"ğŸ“Š æ–‡ä»¶å¤§å°: {file_size:.2f} MB")
            
            # æ ¹æ®æ–‡ä»¶å¤§å°é€‰æ‹©è¯»å–ç­–ç•¥
            if use_chunks or file_size > 500:  # å¤§äº500MBä½¿ç”¨åˆ†å—è¯»å–
                logging.info(f"ğŸ“‹ ä½¿ç”¨åˆ†å—è¯»å– (chunk_size={chunk_size:,})")
                
                chunk_list = []
                total_rows = 0
                
                for i, chunk in enumerate(pd.read_csv(file_path, chunksize=chunk_size, low_memory=False)):
                    chunk_list.append(chunk)
                    total_rows += len(chunk)
                    
                    if max_rows and total_rows >= max_rows:
                        # æˆªå–åˆ°æŒ‡å®šè¡Œæ•°
                        if len(chunk_list) > 1:
                            df = pd.concat(chunk_list, ignore_index=True)
                        else:
                            df = chunk_list[0]
                        df = df.head(max_rows)
                        logging.info(f"ğŸ“‹ å·²åŠ è½½ {len(df):,} è¡Œæ•°æ® (åˆ†å—è¯»å–)")
                        break
                        
                    if i >= 10:  # æœ€å¤šè¯»å–10ä¸ªchunk
                        logging.info(f"ğŸ“‹ å·²è¯»å–10ä¸ªchunkï¼Œç»§ç»­åˆå¹¶...")
                        break
                
                if not chunk_list:
                    return None
                    
                if len(chunk_list) == 1:
                    df = chunk_list[0]
                else:
                    df = pd.concat(chunk_list, ignore_index=True)
                    
            else:
                # æ™®é€šè¯»å–
                df = pd.read_csv(file_path, low_memory=False)
                
                if max_rows and len(df) > max_rows:
                    df = df.head(max_rows)
                    logging.info(f"ğŸ“‹ å·²åŠ è½½å‰ {max_rows:,} è¡Œæ•°æ® (æ€»è®¡ {pd.read_csv(file_path, low_memory=False).shape[0]:,} è¡Œ)")
                else:
                    logging.info(f"ğŸ“‹ å·²åŠ è½½å…¨éƒ¨ {len(df):,} è¡Œæ•°æ®")
            
            if not df.empty:
                logging.info(f"ğŸ” æ•°æ®ç»´åº¦: {df.shape}")
                logging.info(f"ğŸ“ åˆ—å: {list(df.columns)[:10]}{'...' if len(df.columns) > 10 else ''}")
                
                # æ˜¾ç¤ºæ•°æ®é¢„è§ˆ
                print("\nğŸ“„ **æ•°æ®é¢„è§ˆ**:")
                print(df.head().to_string())
                
                # æ˜¾ç¤ºæ•°æ®ç±»å‹ä¿¡æ¯
                print(f"\nğŸ“Š **æ•°æ®ç±»å‹**:")
                for col, dtype in df.dtypes.head(10).items():
                    print(f"  {col}: {dtype}")
                if len(df.dtypes) > 10:
                    print(f"  ... è¿˜æœ‰ {len(df.dtypes) - 10} ä¸ªå­—æ®µ")
            
            return df
            
        except Exception as e:
            logging.error(f"âŒ è¯»å–æ•°æ®å¤±è´¥: {e}")
            return None
    
    def list_categories(self, dataset="comprehensive"):
        """åˆ—å‡ºå¯ç”¨çš„æ•°æ®ç±»åˆ«"""
        datasets = self.get_available_datasets()
        
        if dataset not in datasets:
            logging.error(f"âŒ æ•°æ®é›† '{dataset}' ä¸å­˜åœ¨")
            return []
        
        return list(datasets[dataset]["categories"].keys())
    
    def list_apis(self, dataset="comprehensive", category=None):
        """åˆ—å‡ºæŒ‡å®šç±»åˆ«ä¸‹çš„API"""
        datasets = self.get_available_datasets()
        
        if dataset not in datasets:
            logging.error(f"âŒ æ•°æ®é›† '{dataset}' ä¸å­˜åœ¨")
            return []
            
        if category not in datasets[dataset]["categories"]:
            logging.error(f"âŒ ç±»åˆ« '{category}' ä¸å­˜åœ¨")
            return []
        
        return list(datasets[dataset]["categories"][category]["apis"].keys())
    
    def list_files(self, dataset="comprehensive", category=None, api=None):
        """åˆ—å‡ºæŒ‡å®šAPIä¸‹çš„æ–‡ä»¶"""
        if dataset == "comprehensive":
            base_dir = self.main_data_dir
        elif dataset == "optimized":
            base_dir = self.optimized_data_dir
        else:
            logging.error("âŒ æ— æ•ˆçš„æ•°æ®é›†åç§°")
            return []
        
        if not (category and api):
            logging.error("âŒ è¯·æä¾›categoryå’Œapiå‚æ•°")
            return []
            
        api_dir = base_dir / category / api
        
        if not api_dir.exists():
            return []
            
        return [f.name for f in api_dir.glob("*.csv")]
    
    def show_catalog(self):
        """æ˜¾ç¤ºæ•°æ®ç›®å½•"""
        print("ğŸ“š **CSVæ•°æ®ç›®å½•**")
        print("=" * 60)
        
        datasets = self.get_available_datasets()
        
        total_files = 0
        total_size = 0
        
        for dataset_name, dataset_info in datasets.items():
            print(f"\nğŸ“¦ **{dataset_info['description']}** ({dataset_name})")
            print(f"  ğŸ“ æ€»æ–‡ä»¶: {dataset_info['total_files']:,}")
            print(f"  ğŸ’¾ æ€»å¤§å°: {dataset_info['total_size_gb']:.1f} GB")
            
            for category, category_info in dataset_info["categories"].items():
                print(f"\n  ğŸ“‚ {category} ({category_info['file_count']} æ–‡ä»¶, {category_info['size_gb']:.1f} GB)")
                
                # æ˜¾ç¤ºå‰å‡ ä¸ªAPI
                api_items = list(category_info["apis"].items())[:5]
                for api_name, api_info in api_items:
                    print(f"    ğŸ”Œ {api_name}: {api_info['files']} æ–‡ä»¶, {api_info['size_gb']:.1f} GB")
                
                if len(category_info["apis"]) > 5:
                    print(f"    ... è¿˜æœ‰ {len(category_info['apis']) - 5} ä¸ªAPI")
            
            total_files += dataset_info['total_files']
            total_size += dataset_info['total_size_gb']
        
        print(f"\nğŸ¯ **æ€»è®¡**: {total_files:,} ä¸ªæ–‡ä»¶, {total_size:.1f} GB")

def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºCSVæ•°æ®è¯»å–åŠŸèƒ½"""
    reader = CSVDataReader()
    
    print("ğŸ” **CSVæ•°æ®è¯»å–å·¥å…·æ¼”ç¤º**\n")
    
    # 1. æ˜¾ç¤ºæ•°æ®ç›®å½•
    reader.show_catalog()
    
    # 2. æŸ¥è¯¢æ ·æœ¬æ•°æ®
    print("\n" + "="*60)
    print("ğŸ§ª **æ ·æœ¬æ•°æ®æŸ¥è¯¢æ¼”ç¤º**:")
    
    sample_df = reader.read_data(
        dataset="comprehensive",
        category="financial", 
        api="fdmtindipsget",
        max_rows=10
    )
    
    print("\nğŸŠ CSVæ•°æ®æŸ¥è¯¢ç³»ç»Ÿæ¼”ç¤ºå®Œæˆï¼")
    print("ğŸ’¡ æç¤º: ä½¿ç”¨CSVæ ¼å¼å¯ä»¥ç¡®ä¿æ•°æ®çš„åŸå§‹å®Œæ•´æ€§å’Œå…¼å®¹æ€§")

if __name__ == "__main__":
    main()