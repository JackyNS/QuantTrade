#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®æ•´åˆæ£€æŸ¥å™¨
=============

æ£€æŸ¥é‡å¤æ•°æ®æ–‡ä»¶ï¼Œè¯†åˆ«éœ€è¦åˆå¹¶çš„æ•°æ®é›†
"""

import pandas as pd
from pathlib import Path
from datetime import datetime
import hashlib

def check_data_consolidation():
    """æ£€æŸ¥æ•°æ®æ•´åˆæƒ…å†µ"""
    
    print("ğŸ” QuantTradeæ•°æ®æ•´åˆæ£€æŸ¥å™¨")
    print("=" * 60)
    print(f"â° æ£€æŸ¥æ—¶é—´: {datetime.now()}")
    print()
    
    data_path = Path("/Users/jackstudio/QuantTrade/data")
    
    # é‡ç‚¹æ£€æŸ¥çš„ç›®å½•
    key_dirs = {
        "daily": "æ—¥è¡Œæƒ…æ•°æ®",
        "adjustment": "å¤æƒå› å­",
        "dividend": "è‚¡ç¥¨åˆ†çº¢", 
        "basic_info": "è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯",
        "capital_flow": "èµ„é‡‘æµå‘",
        "financial": "è´¢åŠ¡æ•°æ®"
    }
    
    consolidation_issues = []
    
    print("ğŸ“Š å„ç›®å½•æ•°æ®é‡å¤æ£€æŸ¥:")
    print("-" * 50)
    
    for dir_name, desc in key_dirs.items():
        dir_path = data_path / dir_name
        
        print(f"\nğŸ” æ£€æŸ¥ {desc} ({dir_name}):")
        
        if not dir_path.exists():
            print(f"   âŒ ç›®å½•ä¸å­˜åœ¨")
            continue
            
        csv_files = list(dir_path.glob("*.csv"))
        
        if not csv_files:
            print(f"   ğŸ“‚ ç›®å½•ä¸ºç©º")
            continue
            
        # æŒ‰ç±»å‹åˆ†ç»„æ–‡ä»¶
        modern_files = [f for f in csv_files if "2000_2009" not in f.name]
        historical_files = [f for f in csv_files if "2000_2009" in f.name]
        
        print(f"   ğŸ“ˆ ç°ä»£æ•°æ®æ–‡ä»¶: {len(modern_files)} ä¸ª")
        print(f"   ğŸ“œ å†å²æ•°æ®æ–‡ä»¶: {len(historical_files)} ä¸ª")
        
        # æ£€æŸ¥ç°ä»£æ•°æ®é‡å¤
        if len(modern_files) > 1:
            print(f"   ğŸ” æ£€æŸ¥ç°ä»£æ•°æ®é‡å¤...")
            
            # å¯¹äºæ—¥è¡Œæƒ…æ•°æ®ï¼Œæ£€æŸ¥å¹´åº¦æ–‡ä»¶æ˜¯å¦æœ‰é‡å 
            if dir_name == "daily":
                year_files = [f for f in modern_files if f.name.startswith("daily_")]
                core_files = [f for f in modern_files if "core" in f.name]
                
                if year_files and core_files:
                    print(f"   âš ï¸ å‘ç°å¯èƒ½é‡å¤: {len(year_files)}ä¸ªå¹´åº¦æ–‡ä»¶ + {len(core_files)}ä¸ªæ ¸å¿ƒæ–‡ä»¶")
                    consolidation_issues.append({
                        'dir': dir_name,
                        'type': 'daily_overlap',
                        'files': [f.name for f in year_files + core_files]
                    })
                    
                    # æ£€æŸ¥æ—¶é—´é‡å 
                    for year_file in year_files:
                        try:
                            df_year = pd.read_csv(year_file)
                            if 'tradeDate' in df_year.columns:
                                year_range = f"{df_year['tradeDate'].min()} ~ {df_year['tradeDate'].max()}"
                                print(f"      ğŸ“„ {year_file.name}: {year_range}")
                        except:
                            pass
            
            # æ£€æŸ¥æ–‡ä»¶å†…å®¹ç›¸ä¼¼æ€§
            for i, file1 in enumerate(modern_files):
                for file2 in modern_files[i+1:]:
                    try:
                        # æ¯”è¾ƒæ–‡ä»¶å¤§å°
                        size1 = file1.stat().st_size
                        size2 = file2.stat().st_size
                        
                        if abs(size1 - size2) < size1 * 0.1:  # å¤§å°ç›¸å·®ä¸åˆ°10%
                            print(f"   âš ï¸ ç–‘ä¼¼é‡å¤æ–‡ä»¶: {file1.name} vs {file2.name} (å¤§å°ç›¸è¿‘)")
                            
                    except Exception as e:
                        pass
        
        # æ£€æŸ¥æ˜¯å¦ç¼ºå°‘å†å²æ•°æ®
        if modern_files and not historical_files:
            print(f"   â³ ç¼ºå°‘å†å²æ•°æ® (2000-2009)")
            consolidation_issues.append({
                'dir': dir_name,
                'type': 'missing_historical',
                'files': []
            })
        
        # è®¡ç®—æ€»è®°å½•æ•°å’Œå¤§å°
        total_records = 0
        total_size = 0
        
        for file in csv_files:
            try:
                size = file.stat().st_size / (1024*1024)
                total_size += size
                
                df = pd.read_csv(file)
                total_records += len(df)
                
            except Exception as e:
                pass
                
        print(f"   ğŸ“Š æ€»è®¡: {total_records:,} æ¡è®°å½•, {total_size:.1f}MB")
    
    # æ£€æŸ¥è·¨ç›®å½•çš„æ½œåœ¨é—®é¢˜
    print(f"\nğŸ” è·¨ç›®å½•æ•°æ®æ£€æŸ¥:")
    print("-" * 30)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰åˆ†æ•£çš„ç›¸åŒæ•°æ®
    test_dir = data_path / "test_api_download"
    if test_dir.exists():
        test_files = list(test_dir.glob("*.csv"))
        if test_files:
            print(f"   âš ï¸ å‘ç°æµ‹è¯•æ•°æ®ç›®å½•: {len(test_files)} ä¸ªæµ‹è¯•æ–‡ä»¶")
            consolidation_issues.append({
                'dir': 'test_api_download',
                'type': 'test_data_cleanup',
                'files': [f.name for f in test_files]
            })
    
    # æ±‡æ€»æ•´åˆå»ºè®®
    print(f"\nğŸ“‹ æ•°æ®æ•´åˆå»ºè®®:")
    print("=" * 40)
    
    if consolidation_issues:
        print(f"ğŸ”§ å‘ç° {len(consolidation_issues)} ä¸ªéœ€è¦å¤„ç†çš„é—®é¢˜:")
        
        for issue in consolidation_issues:
            if issue['type'] == 'daily_overlap':
                print(f"   âš ï¸ æ—¥è¡Œæƒ…æ•°æ®é‡å : å»ºè®®åˆå¹¶å¹´åº¦æ–‡ä»¶ï¼Œç§»é™¤coreæ–‡ä»¶")
            elif issue['type'] == 'missing_historical':
                print(f"   â³ {issue['dir']}: ç­‰å¾…å†å²æ•°æ®ä¸‹è½½å®Œæˆ")
            elif issue['type'] == 'test_data_cleanup':
                print(f"   ğŸ§¹ æµ‹è¯•æ•°æ®æ¸…ç†: å»ºè®®ç§»é™¤æˆ–ç§»åŠ¨åˆ°å¤‡ä»½ç›®å½•")
        
        print(f"\nğŸ¯ ä¼˜åŒ–åé¢„æœŸæ•ˆæœ:")
        print(f"   ğŸ“ å‡å°‘é‡å¤æ–‡ä»¶")
        print(f"   ğŸ”„ ç»Ÿä¸€æ•°æ®æ ¼å¼") 
        print(f"   ğŸ“Š æé«˜æŸ¥è¯¢æ•ˆç‡")
        print(f"   ğŸ’¾ ä¼˜åŒ–å­˜å‚¨ç©ºé—´")
        
    else:
        print(f"âœ… æœªå‘ç°æ˜æ˜¾çš„æ•°æ®é‡å¤é—®é¢˜")
        print(f"ğŸ“Š å½“å‰æ•°æ®ç»„ç»‡è‰¯å¥½")
    
    # ç­‰å¾…çŠ¶æ€æ£€æŸ¥
    print(f"\nâ³ ç­‰å¾…ä¸­çš„ä¸‹è½½ä»»åŠ¡:")
    print("-" * 30)
    
    # æ£€æŸ¥2000-2009å†å²æ•°æ®å®Œæˆæƒ…å†µ
    historical_dirs = ['calendar', 'financial', 'capital_flow', 'limit_info', 'rank_list']
    pending_historical = []
    
    for dir_name in historical_dirs:
        dir_path = data_path / dir_name
        if dir_path.exists():
            historical_files = list(dir_path.glob("*2000_2009.csv"))
            if not historical_files:
                pending_historical.append(dir_name)
    
    if pending_historical:
        print(f"   ğŸ“œ ç­‰å¾…å†å²æ•°æ®: {', '.join(pending_historical)}")
    else:
        print(f"   âœ… å†å²æ•°æ®å·²å®Œæˆ")
    
    return consolidation_issues

if __name__ == "__main__":
    check_data_consolidation()