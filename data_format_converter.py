#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®æ ¼å¼è½¬æ¢å™¨ - å°†ä¼˜åŒ–åçš„CSVæ•°æ®è½¬æ¢ä¸ºParquetæ ¼å¼
ä¿æŒä¸å†å²æ•°æ®æ ¼å¼çš„ä¸€è‡´æ€§
"""

import pandas as pd
import pyarrow.parquet as pq
import pyarrow as pa
from pathlib import Path
import json
import logging
import sqlite3
from datetime import datetime
import os

class DataFormatConverter:
    """æ•°æ®æ ¼å¼è½¬æ¢å™¨"""
    
    def __init__(self):
        self.optimized_dir = Path("data/optimized_data")
        self.parquet_dir = Path("data/optimized_parquet")
        self.parquet_dir.mkdir(exist_ok=True)
        self.setup_logging()
        
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        log_file = self.parquet_dir / "format_conversion.log"
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )
    
    def convert_csv_to_parquet(self, csv_file, parquet_file):
        """è½¬æ¢å•ä¸ªCSVæ–‡ä»¶ä¸ºParquet"""
        try:
            # è¯»å–CSVæ–‡ä»¶
            df = pd.read_csv(csv_file)
            
            if df.empty:
                logging.warning(f"è·³è¿‡ç©ºæ–‡ä»¶: {csv_file.name}")
                return False
            
            # ä¼˜åŒ–æ•°æ®ç±»å‹
            df = self.optimize_dtypes(df)
            
            # ä¿å­˜ä¸ºParquetæ ¼å¼
            df.to_parquet(parquet_file, compression='snappy', index=False)
            
            # è®¡ç®—å‹ç¼©æ¯”
            csv_size = csv_file.stat().st_size
            parquet_size = parquet_file.stat().st_size
            compression_ratio = (1 - parquet_size / csv_size) * 100 if csv_size > 0 else 0
            
            logging.info(f"âœ… {csv_file.name}: {len(df):,} è¡Œ, "
                        f"{csv_size/1024/1024:.2f}MB â†’ {parquet_size/1024/1024:.2f}MB "
                        f"(å‹ç¼© {compression_ratio:.1f}%)")
            
            return True, len(df), csv_size, parquet_size
            
        except Exception as e:
            logging.error(f"âŒ è½¬æ¢å¤±è´¥ {csv_file.name}: {e}")
            return False, 0, 0, 0
    
    def optimize_dtypes(self, df):
        """ä¼˜åŒ–æ•°æ®ç±»å‹ä»¥å‡å°‘å­˜å‚¨ç©ºé—´"""
        optimized_df = df.copy()
        
        for col in optimized_df.columns:
            col_data = optimized_df[col]
            
            # è·³è¿‡ç©ºåˆ—
            if col_data.isna().all():
                continue
            
            # å°è¯•è½¬æ¢æ—¥æœŸåˆ—
            if 'date' in col.lower() and col_data.dtype == 'object':
                try:
                    optimized_df[col] = pd.to_datetime(col_data, errors='ignore')
                    continue
                except:
                    pass
            
            # ä¼˜åŒ–æ•°å€¼åˆ—
            if pd.api.types.is_numeric_dtype(col_data):
                # æ•´æ•°ä¼˜åŒ–
                if pd.api.types.is_integer_dtype(col_data):
                    col_min = col_data.min()
                    col_max = col_data.max()
                    
                    if col_min >= 0:  # æ— ç¬¦å·æ•´æ•°
                        if col_max < 255:
                            optimized_df[col] = col_data.astype('uint8')
                        elif col_max < 65535:
                            optimized_df[col] = col_data.astype('uint16')
                        elif col_max < 4294967295:
                            optimized_df[col] = col_data.astype('uint32')
                    else:  # æœ‰ç¬¦å·æ•´æ•°
                        if col_min >= -128 and col_max < 127:
                            optimized_df[col] = col_data.astype('int8')
                        elif col_min >= -32768 and col_max < 32767:
                            optimized_df[col] = col_data.astype('int16')
                        elif col_min >= -2147483648 and col_max < 2147483647:
                            optimized_df[col] = col_data.astype('int32')
                
                # æµ®ç‚¹æ•°ä¼˜åŒ–
                elif pd.api.types.is_float_dtype(col_data):
                    optimized_df[col] = col_data.astype('float32')
            
            # å­—ç¬¦ä¸²åˆ†ç±»ä¼˜åŒ–
            elif col_data.dtype == 'object':
                unique_count = col_data.nunique()
                total_count = len(col_data)
                
                # å¦‚æœå”¯ä¸€å€¼å°‘äºæ€»æ•°çš„50%ï¼Œè½¬ä¸ºåˆ†ç±»ç±»å‹
                if unique_count / total_count < 0.5:
                    optimized_df[col] = col_data.astype('category')
        
        return optimized_df
    
    def convert_category(self, category):
        """è½¬æ¢å•ä¸ªç±»åˆ«çš„æ‰€æœ‰æ•°æ®"""
        logging.info(f"ğŸ”„ å¼€å§‹è½¬æ¢ {category} ç±»åˆ«...")
        
        csv_category_dir = self.optimized_dir / category
        parquet_category_dir = self.parquet_dir / category
        
        if not csv_category_dir.exists():
            logging.warning(f"âŒ CSVç±»åˆ«ç›®å½•ä¸å­˜åœ¨: {category}")
            return {}
        
        parquet_category_dir.mkdir(exist_ok=True)
        
        stats = {
            "apis_converted": 0,
            "files_converted": 0,
            "total_records": 0,
            "csv_size": 0,
            "parquet_size": 0,
            "failed_files": []
        }
        
        # å¤„ç†æ¯ä¸ªAPIç›®å½•
        for api_dir in csv_category_dir.iterdir():
            if not api_dir.is_dir():
                continue
                
            api_name = api_dir.name
            parquet_api_dir = parquet_category_dir / api_name
            parquet_api_dir.mkdir(exist_ok=True)
            
            logging.info(f"  ğŸ“ è½¬æ¢API: {api_name}")
            
            api_converted = False
            
            # è½¬æ¢æ‰€æœ‰CSVæ–‡ä»¶
            for csv_file in api_dir.glob("*.csv"):
                parquet_file = parquet_api_dir / f"{csv_file.stem}.parquet"
                
                result = self.convert_csv_to_parquet(csv_file, parquet_file)
                
                if isinstance(result, tuple) and result[0]:  # è½¬æ¢æˆåŠŸ
                    success, records, csv_size, parquet_size = result
                    stats["files_converted"] += 1
                    stats["total_records"] += records
                    stats["csv_size"] += csv_size
                    stats["parquet_size"] += parquet_size
                    api_converted = True
                    
                    # å¤åˆ¶å…ƒæ•°æ®æ–‡ä»¶
                    metadata_file = api_dir / f"{csv_file.stem}_metadata.json"
                    if metadata_file.exists():
                        parquet_metadata_file = parquet_api_dir / f"{csv_file.stem}_metadata.json"
                        # æ›´æ–°å…ƒæ•°æ®ä»¥åæ˜ æ ¼å¼å˜æ›´
                        with open(metadata_file, 'r', encoding='utf-8') as f:
                            metadata = json.load(f)
                        
                        metadata["parquet_conversion"] = {
                            "converted_time": datetime.now().isoformat(),
                            "csv_size": csv_size,
                            "parquet_size": parquet_size,
                            "compression_ratio": (1 - parquet_size / csv_size) * 100 if csv_size > 0 else 0
                        }
                        
                        with open(parquet_metadata_file, 'w', encoding='utf-8') as f:
                            json.dump(metadata, f, indent=2, ensure_ascii=False)
                else:
                    stats["failed_files"].append(str(csv_file.relative_to(self.optimized_dir)))
            
            if api_converted:
                stats["apis_converted"] += 1
        
        compression_ratio = (1 - stats["parquet_size"] / stats["csv_size"]) * 100 if stats["csv_size"] > 0 else 0
        
        logging.info(f"âœ… {category} è½¬æ¢å®Œæˆ: {stats['files_converted']} æ–‡ä»¶, "
                    f"{stats['csv_size']/1024/1024/1024:.2f}GB â†’ "
                    f"{stats['parquet_size']/1024/1024/1024:.2f}GB "
                    f"(å‹ç¼© {compression_ratio:.1f}%)")
        
        return stats
    
    def create_parquet_index(self):
        """åˆ›å»ºParquetæ ¼å¼çš„æ•°æ®ç´¢å¼•"""
        logging.info("ğŸ“‡ åˆ›å»ºParquetæ ¼å¼æ•°æ®ç´¢å¼•...")
        
        index_db_path = self.parquet_dir / "data_index.db"
        conn = sqlite3.connect(index_db_path)
        cursor = conn.cursor()
        
        # åˆ›å»ºç´¢å¼•è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS data_index (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                category TEXT,
                api_name TEXT,
                file_name TEXT,
                file_path TEXT,
                record_count INTEGER,
                file_size_mb REAL,
                compression_ratio REAL,
                last_updated TEXT,
                key_fields TEXT,
                data_hash TEXT
            )
        ''')
        
        # åˆ›å»ºç´¢å¼•
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_category_api ON data_index(category, api_name)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_last_updated ON data_index(last_updated)')
        
        categories = ["basic_info", "financial", "special_trading", "governance"]
        total_indexed = 0
        
        for category in categories:
            category_dir = self.parquet_dir / category
            if not category_dir.exists():
                continue
                
            for api_dir in category_dir.iterdir():
                if not api_dir.is_dir():
                    continue
                    
                api_name = api_dir.name
                
                for parquet_file in api_dir.glob("*.parquet"):
                    # è¯»å–è®°å½•æ•°
                    try:
                        pf = pq.ParquetFile(parquet_file)
                        record_count = pf.metadata.num_rows
                        file_size_mb = parquet_file.stat().st_size / 1024 / 1024
                    except:
                        record_count = 0
                        file_size_mb = 0
                    
                    # è¯»å–å…ƒæ•°æ®
                    metadata_file = api_dir / f"{parquet_file.stem}_metadata.json"
                    compression_ratio = 0
                    key_fields = []
                    data_hash = ""
                    last_updated = ""
                    
                    if metadata_file.exists():
                        with open(metadata_file, 'r', encoding='utf-8') as f:
                            metadata = json.load(f)
                        
                        compression_ratio = metadata.get('parquet_conversion', {}).get('compression_ratio', 0)
                        key_fields = metadata.get('key_columns', [])
                        data_hash = metadata.get('data_hash', '')
                        last_updated = metadata.get('parquet_conversion', {}).get('converted_time', '')
                    
                    # æ’å…¥ç´¢å¼•è®°å½•
                    cursor.execute('''
                        INSERT OR REPLACE INTO data_index 
                        (category, api_name, file_name, file_path, record_count, file_size_mb, 
                         compression_ratio, last_updated, key_fields, data_hash)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    ''', (
                        category,
                        api_name,
                        parquet_file.name,
                        str(parquet_file.relative_to(self.parquet_dir)),
                        record_count,
                        file_size_mb,
                        compression_ratio,
                        last_updated,
                        json.dumps(key_fields),
                        data_hash
                    ))
                    
                    total_indexed += 1
        
        conn.commit()
        conn.close()
        
        logging.info(f"ğŸ“‡ Parquetç´¢å¼•åˆ›å»ºå®Œæˆ: {total_indexed} ä¸ªæ–‡ä»¶")
        return total_indexed
    
    def run_conversion(self):
        """æ‰§è¡Œå®Œæ•´çš„æ ¼å¼è½¬æ¢"""
        logging.info("ğŸš€ å¼€å§‹CSVåˆ°Parquetæ ¼å¼è½¬æ¢...")
        
        start_time = datetime.now()
        categories = ["basic_info", "financial", "special_trading", "governance"]
        
        total_stats = {
            "categories_converted": 0,
            "apis_converted": 0,
            "files_converted": 0,
            "total_records": 0,
            "csv_size": 0,
            "parquet_size": 0,
            "failed_files": []
        }
        
        # è½¬æ¢æ¯ä¸ªç±»åˆ«
        for category in categories:
            try:
                category_stats = self.convert_category(category)
                
                # ç´¯è®¡ç»Ÿè®¡
                for key in ['apis_converted', 'files_converted', 'total_records', 'csv_size', 'parquet_size']:
                    if key in category_stats:
                        total_stats[key] += category_stats[key]
                
                total_stats['failed_files'].extend(category_stats.get('failed_files', []))
                total_stats["categories_converted"] += 1
                
            except Exception as e:
                logging.error(f"âŒ {category} è½¬æ¢å¤±è´¥: {e}")
        
        # åˆ›å»ºParquetç´¢å¼•
        try:
            indexed_files = self.create_parquet_index()
        except Exception as e:
            logging.error(f"âŒ ç´¢å¼•åˆ›å»ºå¤±è´¥: {e}")
            indexed_files = 0
        
        # ç”Ÿæˆè½¬æ¢æŠ¥å‘Š
        end_time = datetime.now()
        duration = end_time - start_time
        
        overall_compression = (1 - total_stats["parquet_size"] / total_stats["csv_size"]) * 100 if total_stats["csv_size"] > 0 else 0
        
        conversion_report = {
            "conversion_time": end_time.isoformat(),
            "duration_seconds": duration.total_seconds(),
            "statistics": total_stats,
            "indexed_files": indexed_files,
            "overall_compression_ratio": overall_compression,
            "storage_savings_gb": (total_stats["csv_size"] - total_stats["parquet_size"]) / 1024 / 1024 / 1024
        }
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = self.parquet_dir / "conversion_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(conversion_report, f, indent=2, ensure_ascii=False)
        
        # è¾“å‡ºæ‘˜è¦
        logging.info("ğŸŠ æ•°æ®æ ¼å¼è½¬æ¢å®Œæˆ!")
        logging.info("=" * 50)
        logging.info("ğŸ“Š **è½¬æ¢ç»“æœæ‘˜è¦**")
        logging.info(f"  ğŸ“ è½¬æ¢ç±»åˆ«: {total_stats['categories_converted']}")
        logging.info(f"  ğŸ”Œ è½¬æ¢API: {total_stats['apis_converted']}")
        logging.info(f"  ğŸ“„ è½¬æ¢æ–‡ä»¶: {total_stats['files_converted']}")
        logging.info(f"  ğŸ“ æ€»è®°å½•æ•°: {total_stats['total_records']:,}")
        logging.info(f"  ğŸ’¾ å­˜å‚¨å˜åŒ–: {total_stats['csv_size']/1024/1024/1024:.2f}GB â†’ {total_stats['parquet_size']/1024/1024/1024:.2f}GB")
        logging.info(f"  ğŸ“Š æ€»å‹ç¼©æ¯”: {overall_compression:.1f}%")
        logging.info(f"  ğŸ’° èŠ‚çœå­˜å‚¨: {conversion_report['storage_savings_gb']:.2f}GB")
        logging.info(f"  ğŸ“‡ å»ºç«‹ç´¢å¼•: {indexed_files} ä¸ªæ–‡ä»¶")
        logging.info(f"  â±ï¸ è½¬æ¢æ—¶é•¿: {duration}")
        
        if total_stats['failed_files']:
            logging.warning(f"âš ï¸ {len(total_stats['failed_files'])} ä¸ªæ–‡ä»¶è½¬æ¢å¤±è´¥")
        
        return conversion_report

if __name__ == "__main__":
    converter = DataFormatConverter()
    converter.run_conversion()