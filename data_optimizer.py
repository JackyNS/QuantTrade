#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®ä¼˜åŒ–æ•´åˆå·¥å…·
"""

import pandas as pd
import numpy as np
from pathlib import Path
import json
from datetime import datetime
import shutil
import os
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

class DataOptimizer:
    """æ•°æ®ä¼˜åŒ–å™¨"""
    
    def __init__(self):
        self.data_root = Path("data")
        self.optimized_root = self.data_root / "optimized"
        self.backup_root = self.data_root / "backup"
        
    def analyze_redundancy(self):
        """åˆ†ææ•°æ®å†—ä½™"""
        print("ğŸ” åˆ†ææ•°æ®å†—ä½™...")
        
        redundancy_report = {
            'overlapping_years': [],
            'duplicate_data': {},
            'size_comparison': {}
        }
        
        # 1. åˆ†æå¹´ä»½é‡å 
        overlap_analysis = self._analyze_year_overlap()
        redundancy_report['overlapping_years'] = overlap_analysis
        
        # 2. åˆ†æé‡å¤æ•°æ®
        duplicate_analysis = self._analyze_duplicate_content()
        redundancy_report['duplicate_data'] = duplicate_analysis
        
        # 3. åˆ†æå­˜å‚¨æ•ˆç‡
        size_analysis = self._analyze_storage_efficiency()
        redundancy_report['size_comparison'] = size_analysis
        
        return redundancy_report
    
    def _analyze_year_overlap(self):
        """åˆ†æå¹´ä»½é‡å """
        overlap_data = []
        
        # æ£€æŸ¥2003-2025å¹´çš„é‡å æƒ…å†µ
        for year in range(2003, 2026):
            year_data = {
                'year': year,
                'sources': [],
                'file_counts': {},
                'total_size_mb': 0
            }
            
            # å†å²ä¸‹è½½å™¨
            hist_path = self.data_root / f"historical_download/market_data/year_{year}"
            if hist_path.exists():
                files = list(hist_path.glob("*.csv"))
                if files:
                    size_mb = sum(f.stat().st_size for f in files) / (1024*1024)
                    year_data['sources'].append('historical')
                    year_data['file_counts']['historical'] = len(files)
                    year_data['total_size_mb'] += size_mb
            
            # æ™ºèƒ½ä¸‹è½½å™¨
            smart_path = self.data_root / f"smart_download/year_{year}"
            if smart_path.exists():
                files = list(smart_path.glob("*.csv"))
                if files:
                    size_mb = sum(f.stat().st_size for f in files) / (1024*1024)
                    year_data['sources'].append('smart')
                    year_data['file_counts']['smart'] = len(files)
                    year_data['total_size_mb'] += size_mb
            
            if len(year_data['sources']) > 1:
                overlap_data.append(year_data)
        
        return overlap_data
    
    def _analyze_duplicate_content(self):
        """åˆ†æé‡å¤å†…å®¹"""
        duplicate_info = {}
        
        # æŠ½æ ·æ£€æŸ¥2024å¹´æ•°æ®çš„é‡å¤æƒ…å†µ
        year = 2024
        
        sample_data = {}
        
        # è¯»å–å„ä¸‹è½½å™¨çš„æ ·æœ¬æ•°æ®
        sources = {
            'historical': self.data_root / f"historical_download/market_data/year_{year}/batch_001.csv",
            'smart': self.data_root / f"smart_download/year_{year}/batch_001.csv"
        }
        
        for source, file_path in sources.items():
            if file_path.exists():
                try:
                    df = pd.read_csv(file_path)
                    if 'ticker' in df.columns and 'tradeDate' in df.columns:
                        # åˆ›å»ºå”¯ä¸€é”®
                        df['key'] = df['ticker'].astype(str) + '_' + df['tradeDate'].astype(str)
                        sample_data[source] = set(df['key'].tolist())
                except:
                    continue
        
        # è®¡ç®—é‡å 
        if len(sample_data) >= 2:
            sources = list(sample_data.keys())
            for i, s1 in enumerate(sources[:-1]):
                for s2 in sources[i+1:]:
                    overlap = sample_data[s1] & sample_data[s2]
                    total = sample_data[s1] | sample_data[s2]
                    overlap_ratio = len(overlap) / len(total) if len(total) > 0 else 0
                    
                    duplicate_info[f"{s1}_vs_{s2}"] = {
                        'overlap_records': len(overlap),
                        'total_unique': len(total),
                        'overlap_ratio': overlap_ratio
                    }
        
        return duplicate_info
    
    def _analyze_storage_efficiency(self):
        """åˆ†æå­˜å‚¨æ•ˆç‡"""
        size_info = {}
        
        # åˆ†æå„ç›®å½•å¤§å°
        directories = [
            'historical_download',
            'smart_download',
            'priority_download'
        ]
        
        for dir_name in directories:
            dir_path = self.data_root / dir_name
            if dir_path.exists():
                total_size = 0
                file_count = 0
                
                for file_path in dir_path.rglob("*.csv"):
                    total_size += file_path.stat().st_size
                    file_count += 1
                
                size_info[dir_name] = {
                    'size_mb': total_size / (1024*1024),
                    'file_count': file_count,
                    'avg_file_size_mb': (total_size / file_count / (1024*1024)) if file_count > 0 else 0
                }
        
        return size_info
    
    def design_optimized_structure(self):
        """è®¾è®¡ä¼˜åŒ–åçš„å­˜å‚¨ç»“æ„"""
        print("ğŸ¨ è®¾è®¡ä¼˜åŒ–å­˜å‚¨ç»“æ„...")
        
        structure = {
            'unified': {
                'daily': {  # æ—¥è¡Œæƒ…æ•°æ®
                    'description': 'ç»Ÿä¸€çš„æ—¥è¡Œæƒ…æ•°æ®ï¼Œå»é‡åˆå¹¶',
                    'structure': 'year_YYYY/YYYY_MM.parquet',
                    'source': 'merge historical + smart + priority daily'
                },
                'weekly': {  # å‘¨è¡Œæƒ…æ•°æ®
                    'description': 'å‘¨è¡Œæƒ…æ•°æ®',
                    'structure': 'weekly/YYYY.parquet',
                    'source': 'priority_download weekly'
                },
                'monthly': {  # æœˆè¡Œæƒ…æ•°æ®
                    'description': 'æœˆè¡Œæƒ…æ•°æ®',
                    'structure': 'monthly/YYYY.parquet',
                    'source': 'priority_download monthly'
                },
                'adjusted': {  # å¤æƒæ•°æ®
                    'description': 'å‰å¤æƒè¡Œæƒ…æ•°æ®',
                    'structure': 'adjusted/year_YYYY/YYYY_MM.parquet',
                    'source': 'priority_download adj_daily'
                },
                'factors': {  # å¤æƒå› å­
                    'description': 'å¤æƒå› å­æ•°æ®',
                    'structure': 'factors/YYYY.parquet',
                    'source': 'priority_download adj_factor'
                },
                'flow': {  # èµ„é‡‘æµå‘
                    'description': 'ä¸ªè‚¡å’Œè¡Œä¸šèµ„é‡‘æµå‘',
                    'structure': 'flow/stock/YYYY.parquet, flow/industry/YYYY.parquet',
                    'source': 'priority_download flow_data'
                }
            },
            'metadata': {
                'stocks': 'stock_info.parquet',
                'calendars': 'trading_calendar.parquet',
                'data_quality': 'quality_metrics.json'
            }
        }
        
        return structure
    
    def create_unified_daily_data(self, year_start=2000, year_end=2025):
        """åˆ›å»ºç»Ÿä¸€çš„æ—¥è¡Œæƒ…æ•°æ®"""
        print(f"ğŸ”„ åˆ›å»ºç»Ÿä¸€æ—¥è¡Œæƒ…æ•°æ® ({year_start}-{year_end})...")
        
        # åˆ›å»ºç›®å½•
        unified_daily_dir = self.optimized_root / "daily"
        unified_daily_dir.mkdir(parents=True, exist_ok=True)
        
        success_count = 0
        
        for year in range(year_start, year_end + 1):
            print(f"\nğŸ“… å¤„ç† {year} å¹´æ•°æ®...")
            
            year_dir = unified_daily_dir / f"year_{year}"
            year_dir.mkdir(exist_ok=True)
            
            # æ”¶é›†è¯¥å¹´ä»½æ‰€æœ‰æ•°æ®æº
            all_data = []
            
            # 1. å†å²ä¸‹è½½å™¨æ•°æ®
            hist_dir = self.data_root / f"historical_download/market_data/year_{year}"
            if hist_dir.exists():
                hist_files = list(hist_dir.glob("*.csv"))
                print(f"   ğŸ“ å†å²æ•°æ®: {len(hist_files)} æ–‡ä»¶")
                for file_path in hist_files:
                    try:
                        df = pd.read_csv(file_path)
                        if not df.empty:
                            df['source'] = 'historical'
                            all_data.append(df)
                    except:
                        continue
            
            # 2. æ™ºèƒ½ä¸‹è½½å™¨æ•°æ®
            smart_dir = self.data_root / f"smart_download/year_{year}"
            if smart_dir.exists():
                smart_files = list(smart_dir.glob("*.csv"))
                print(f"   ğŸ“ æ™ºèƒ½æ•°æ®: {len(smart_files)} æ–‡ä»¶")
                for file_path in smart_files:
                    try:
                        df = pd.read_csv(file_path)
                        if not df.empty:
                            df['source'] = 'smart'
                            all_data.append(df)
                    except:
                        continue
            
            # 3. ä¼˜å…ˆçº§ä¸‹è½½å™¨æ•°æ®
            priority_files = list((self.data_root / "priority_download/market_data/daily").glob(f"{year}_batch_*.csv"))
            if priority_files:
                print(f"   ğŸ“ ä¼˜å…ˆæ•°æ®: {len(priority_files)} æ–‡ä»¶")
                for file_path in priority_files:
                    try:
                        df = pd.read_csv(file_path)
                        if not df.empty:
                            df['source'] = 'priority'
                            all_data.append(df)
                    except:
                        continue
            
            if not all_data:
                print(f"   âš ï¸ {year} å¹´æ— æ•°æ®")
                continue
            
            # åˆå¹¶æ•°æ®
            combined_df = pd.concat(all_data, ignore_index=True)
            print(f"   ğŸ“Š åˆå¹¶å‰: {len(combined_df):,} æ¡è®°å½•")
            
            # æ•°æ®æ¸…ç†å’Œå»é‡
            cleaned_df = self._clean_and_dedupe_data(combined_df)
            print(f"   âœ… æ¸…ç†å: {len(cleaned_df):,} æ¡è®°å½•")
            
            if len(cleaned_df) > 0:
                # æŒ‰æœˆåˆ†å‰²å­˜å‚¨
                self._save_monthly_data(cleaned_df, year_dir, year)
                success_count += 1
            
        print(f"\nğŸ‰ ç»Ÿä¸€æ—¥è¡Œæƒ…æ•°æ®å®Œæˆ: {success_count}/{year_end-year_start+1} å¹´")
        return success_count > 0
    
    def _clean_and_dedupe_data(self, df):
        """æ¸…ç†å’Œå»é‡æ•°æ®"""
        # 1. åŸºç¡€æ¸…ç†
        if 'ticker' not in df.columns or 'tradeDate' not in df.columns:
            return df
        
        # 2. åˆ é™¤å¼‚å¸¸ä»·æ ¼æ•°æ®
        price_cols = ['openPrice', 'highestPrice', 'lowestPrice', 'closePrice']
        for col in price_cols:
            if col in df.columns:
                df = df[df[col] > 0]  # åˆ é™¤è´Ÿä»·æ ¼å’Œé›¶ä»·æ ¼
        
        # 3. å»é‡ - ä¼˜å…ˆçº§: priority > smart > historical
        df['priority'] = df['source'].map({'priority': 3, 'smart': 2, 'historical': 1})
        df = df.sort_values(['ticker', 'tradeDate', 'priority'], ascending=[True, True, False])
        df = df.drop_duplicates(['ticker', 'tradeDate'], keep='first')
        df = df.drop(['source', 'priority'], axis=1)
        
        return df
    
    def _save_monthly_data(self, df, year_dir, year):
        """æŒ‰æœˆä¿å­˜æ•°æ®"""
        if 'tradeDate' not in df.columns:
            # å¦‚æœæ²¡æœ‰æ—¥æœŸåˆ—ï¼Œç›´æ¥ä¿å­˜æ•´å¹´æ•°æ®
            df.to_parquet(year_dir / f"{year}_all.parquet", compression='snappy')
            return
        
        try:
            df['tradeDate'] = pd.to_datetime(df['tradeDate'])
            df['month'] = df['tradeDate'].dt.month
            
            for month in range(1, 13):
                monthly_data = df[df['month'] == month].drop('month', axis=1)
                if len(monthly_data) > 0:
                    file_path = year_dir / f"{year}_{month:02d}.parquet"
                    monthly_data.to_parquet(file_path, compression='snappy')
                    
        except:
            # å¦‚æœæ—¥æœŸå¤„ç†å¤±è´¥ï¼Œä¿å­˜æ•´å¹´æ•°æ®
            df.to_parquet(year_dir / f"{year}_all.parquet", compression='snappy')
    
    def create_optimized_structure(self):
        """åˆ›å»ºä¼˜åŒ–åçš„ç›®å½•ç»“æ„"""
        print("ğŸ—ï¸ åˆ›å»ºä¼˜åŒ–å­˜å‚¨ç»“æ„...")
        
        # åˆ›å»ºä¸»è¦ç›®å½•
        directories = [
            'optimized/daily',
            'optimized/weekly', 
            'optimized/monthly',
            'optimized/adjusted',
            'optimized/factors',
            'optimized/flow/stock',
            'optimized/flow/industry',
            'optimized/metadata',
            'backup/original'
        ]
        
        for dir_path in directories:
            (self.data_root / dir_path).mkdir(parents=True, exist_ok=True)
        
        # å¤åˆ¶å…¶ä»–ç±»å‹æ•°æ®
        self._migrate_other_data()
        
        return True
    
    def _migrate_other_data(self):
        """è¿ç§»å…¶ä»–ç±»å‹æ•°æ®"""
        print("   ğŸ“¦ è¿ç§»å…¶ä»–æ•°æ®ç±»å‹...")
        
        # 1. è¿ç§»å‘¨è¡Œæƒ…æ•°æ®
        weekly_source = self.data_root / "priority_download/market_data/weekly"
        weekly_target = self.optimized_root / "weekly"
        if weekly_source.exists():
            self._migrate_and_compress(weekly_source, weekly_target, "weekly")
        
        # 2. è¿ç§»æœˆè¡Œæƒ…æ•°æ®
        monthly_source = self.data_root / "priority_download/market_data/monthly"
        monthly_target = self.optimized_root / "monthly"
        if monthly_source.exists():
            self._migrate_and_compress(monthly_source, monthly_target, "monthly")
        
        # 3. è¿ç§»å¤æƒæ•°æ®
        adj_source = self.data_root / "priority_download/market_data/adj_daily"
        adj_target = self.optimized_root / "adjusted"
        if adj_source.exists():
            self._migrate_and_compress(adj_source, adj_target, "adjusted")
        
        # 4. è¿ç§»å¤æƒå› å­
        factor_source = self.data_root / "priority_download/market_data/adj_factor"
        factor_target = self.optimized_root / "factors"
        if factor_source.exists():
            self._migrate_and_compress(factor_source, factor_target, "factors")
        
        # 5. è¿ç§»èµ„é‡‘æµå‘æ•°æ®
        flow_source = self.data_root / "priority_download/flow_data"
        flow_target = self.optimized_root / "flow"
        if flow_source.exists():
            self._migrate_flow_data(flow_source, flow_target)
    
    def _migrate_and_compress(self, source_dir, target_dir, data_type):
        """è¿ç§»å¹¶å‹ç¼©æ•°æ®"""
        csv_files = list(source_dir.rglob("*.csv"))
        if not csv_files:
            return
        
        print(f"      {data_type}: {len(csv_files)} æ–‡ä»¶")
        
        # æŒ‰å¹´ä»½å½’ç±»
        year_data = {}
        for file_path in csv_files:
            year = None
            for y in range(2000, 2026):
                if str(y) in file_path.name:
                    year = y
                    break
            
            if year:
                if year not in year_data:
                    year_data[year] = []
                year_data[year].append(file_path)
        
        # åˆå¹¶å¹¶å‹ç¼©
        for year, files in year_data.items():
            all_data = []
            for file_path in files:
                try:
                    df = pd.read_csv(file_path)
                    if not df.empty:
                        all_data.append(df)
                except:
                    continue
            
            if all_data:
                combined_df = pd.concat(all_data, ignore_index=True)
                combined_df = combined_df.drop_duplicates()
                
                output_file = target_dir / f"{year}.parquet"
                combined_df.to_parquet(output_file, compression='snappy')
    
    def _migrate_flow_data(self, source_dir, target_dir):
        """è¿ç§»èµ„é‡‘æµå‘æ•°æ®"""
        # ä¸ªè‚¡æµå‘
        stock_flow_dir = source_dir / "stock_flow"
        if stock_flow_dir.exists():
            target_stock_dir = target_dir / "stock"
            self._migrate_and_compress(stock_flow_dir, target_stock_dir, "stock_flow")
        
        # è¡Œä¸šæµå‘
        industry_flow_dir = source_dir / "industry_flow"
        if industry_flow_dir.exists():
            target_industry_dir = target_dir / "industry"
            self._migrate_and_compress(industry_flow_dir, target_industry_dir, "industry_flow")
    
    def generate_optimization_report(self):
        """ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š"""
        print("ğŸ“‹ ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š...")
        
        # åˆ†æå†—ä½™
        redundancy = self.analyze_redundancy()
        
        # ç»Ÿè®¡åŸå§‹å¤§å°
        original_size = 0
        original_files = 0
        for file_path in self.data_root.rglob("*.csv"):
            if "optimized" not in str(file_path) and "backup" not in str(file_path):
                original_size += file_path.stat().st_size
                original_files += 1
        
        # ç»Ÿè®¡ä¼˜åŒ–åå¤§å°
        optimized_size = 0
        optimized_files = 0
        if self.optimized_root.exists():
            for file_path in self.optimized_root.rglob("*"):
                if file_path.is_file():
                    optimized_size += file_path.stat().st_size
                    optimized_files += 1
        
        report = {
            'analysis_time': datetime.now().isoformat(),
            'redundancy_analysis': redundancy,
            'size_comparison': {
                'original': {
                    'size_mb': original_size / (1024*1024),
                    'files': original_files
                },
                'optimized': {
                    'size_mb': optimized_size / (1024*1024),
                    'files': optimized_files
                },
                'savings': {
                    'size_mb': (original_size - optimized_size) / (1024*1024),
                    'percentage': ((original_size - optimized_size) / original_size * 100) if original_size > 0 else 0
                }
            }
        }
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = self.data_root / 'optimization_report.json'
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        return report
    
    def print_optimization_summary(self, report):
        """æ‰“å°ä¼˜åŒ–æ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸ“Š æ•°æ®ä¼˜åŒ–æ‘˜è¦")
        print("="*60)
        
        # å†—ä½™åˆ†æ
        overlap_years = len(report['redundancy_analysis']['overlapping_years'])
        print(f"ğŸ” æ•°æ®é‡å : {overlap_years} ä¸ªå¹´ä»½å­˜åœ¨å†—ä½™")
        
        # å­˜å‚¨æ•ˆç‡
        size_comp = report['size_comparison']
        original_gb = size_comp['original']['size_mb'] / 1024
        optimized_gb = size_comp['optimized']['size_mb'] / 1024
        savings_gb = size_comp['savings']['size_mb'] / 1024
        savings_pct = size_comp['savings']['percentage']
        
        print(f"ğŸ’¾ å­˜å‚¨ä¼˜åŒ–:")
        print(f"   åŸå§‹å¤§å°: {original_gb:.2f} GB ({size_comp['original']['files']} æ–‡ä»¶)")
        print(f"   ä¼˜åŒ–å¤§å°: {optimized_gb:.2f} GB ({size_comp['optimized']['files']} æ–‡ä»¶)")
        print(f"   èŠ‚çœç©ºé—´: {savings_gb:.2f} GB ({savings_pct:.1f}%)")
        
        # é‡å¤æ•°æ®åˆ†æ
        duplicate_info = report['redundancy_analysis']['duplicate_data']
        if duplicate_info:
            print(f"\nğŸ”„ æ•°æ®é‡å¤åˆ†æ:")
            for comparison, info in duplicate_info.items():
                overlap_pct = info['overlap_ratio'] * 100
                print(f"   {comparison}: {overlap_pct:.1f}% é‡å¤")

def main():
    """ä¸»å‡½æ•°"""
    optimizer = DataOptimizer()
    
    # 1. åˆ†æç°çŠ¶
    print("ğŸš€ å¼€å§‹æ•°æ®ä¼˜åŒ–æ•´åˆ...\n")
    
    # 2. åˆ›å»ºä¼˜åŒ–ç»“æ„
    optimizer.create_optimized_structure()
    
    # 3. åˆ›å»ºç»Ÿä¸€æ—¥è¡Œæƒ…æ•°æ®
    optimizer.create_unified_daily_data()
    
    # 4. ç”ŸæˆæŠ¥å‘Š
    report = optimizer.generate_optimization_report()
    optimizer.print_optimization_summary(report)

if __name__ == "__main__":
    main()