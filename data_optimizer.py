#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®ä¼˜åŒ–æ•´ç†å·¥å…· - æ ‡å‡†åŒ–ã€å»é‡ã€å»ºç«‹ç´¢å¼•
"""

import pandas as pd
from pathlib import Path
import json
import logging
from datetime import datetime
import hashlib
from collections import defaultdict
import sqlite3

class DataOptimizer:
    """æ•°æ®ä¼˜åŒ–æ•´ç†å™¨"""
    
    def __init__(self):
        self.data_dir = Path("data/final_comprehensive_download")
        self.optimized_dir = Path("data/optimized_data")
        self.optimized_dir.mkdir(exist_ok=True)
        self.setup_logging()
        
        # å­—æ®µæ ‡å‡†åŒ–æ˜ å°„
        self.field_mappings = {
            # åŸºç¡€ä¿¡æ¯æ ‡å‡†åŒ–
            'secID': 'security_id',
            'ticker': 'stock_code', 
            'secShortName': 'stock_name',
            'exchangeCD': 'exchange',
            'tradeDate': 'trade_date',
            'publishDate': 'publish_date',
            'endDate': 'end_date',
            'beginDate': 'begin_date',
            
            # è´¢åŠ¡æ•°æ®æ ‡å‡†åŒ–
            'revenue': 'total_revenue',
            'NIncomeAttrP': 'net_income',
            'TAssets': 'total_assets',
            'TEquityAttrP': 'total_equity',
            'basicEPS': 'basic_eps',
            'ROE': 'roe',
            
            # å¸‚åœºæ•°æ®æ ‡å‡†åŒ–
            'openPrice': 'open_price',
            'highestPrice': 'high_price', 
            'lowestPrice': 'low_price',
            'closePrice': 'close_price',
            'turnoverVol': 'volume',
            'turnoverValue': 'turnover',
            'PE': 'pe_ratio',
            'PB': 'pb_ratio'
        }
        
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        log_file = self.optimized_dir / "data_optimization.log"
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )
    
    def standardize_field_names(self, df, source_info=""):
        """æ ‡å‡†åŒ–å­—æ®µå‘½å"""
        original_columns = df.columns.tolist()
        standardized_df = df.copy()
        
        # åº”ç”¨å­—æ®µæ˜ å°„
        rename_map = {}
        for old_name, new_name in self.field_mappings.items():
            if old_name in standardized_df.columns:
                rename_map[old_name] = new_name
        
        if rename_map:
            standardized_df.rename(columns=rename_map, inplace=True)
            logging.info(f"ğŸ“ {source_info}: æ ‡å‡†åŒ– {len(rename_map)} ä¸ªå­—æ®µå")
        
        # ç»Ÿä¸€æ—¥æœŸå­—æ®µæ ¼å¼
        date_fields = [col for col in standardized_df.columns if 'date' in col.lower()]
        for date_field in date_fields:
            try:
                standardized_df[date_field] = pd.to_datetime(standardized_df[date_field]).dt.date
            except:
                pass
        
        return standardized_df, len(rename_map)
    
    def detect_duplicates(self, df, key_columns=None):
        """æ£€æµ‹é‡å¤è®°å½•"""
        if df.empty:
            return df, 0, []
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šå…³é”®åˆ—ï¼Œå°è¯•è‡ªåŠ¨è¯†åˆ«
        if key_columns is None:
            key_columns = []
            potential_keys = ['security_id', 'stock_code', 'trade_date', 'publish_date', 'end_date']
            for key in potential_keys:
                if key in df.columns:
                    key_columns.append(key)
        
        if not key_columns:
            # ä½¿ç”¨æ‰€æœ‰åˆ—æ£€æµ‹å®Œå…¨é‡å¤
            duplicates = df.duplicated()
            duplicate_rows = df[duplicates]
            cleaned_df = df.drop_duplicates()
        else:
            # ä½¿ç”¨å…³é”®åˆ—æ£€æµ‹ä¸šåŠ¡é‡å¤
            duplicates = df.duplicated(subset=key_columns)
            duplicate_rows = df[duplicates]
            cleaned_df = df.drop_duplicates(subset=key_columns)
        
        duplicate_count = len(duplicate_rows)
        return cleaned_df, duplicate_count, key_columns
    
    def create_data_hash(self, df):
        """åˆ›å»ºæ•°æ®å“ˆå¸Œç”¨äºå®Œæ•´æ€§éªŒè¯"""
        if df.empty:
            return ""
        
        # åˆ›å»ºåŸºäºæ•°æ®å†…å®¹çš„å“ˆå¸Œ
        data_string = df.to_csv(index=False)
        return hashlib.md5(data_string.encode()).hexdigest()
    
    def optimize_single_category(self, category):
        """ä¼˜åŒ–å•ä¸ªæ•°æ®ç±»åˆ«"""
        logging.info(f"ğŸ”§ å¼€å§‹ä¼˜åŒ– {category} ç±»åˆ«...")
        
        category_dir = self.data_dir / category
        if not category_dir.exists():
            logging.warning(f"âŒ ç±»åˆ«ç›®å½•ä¸å­˜åœ¨: {category}")
            return {}
        
        optimized_category_dir = self.optimized_dir / category
        optimized_category_dir.mkdir(exist_ok=True)
        
        category_stats = {
            "apis_processed": 0,
            "files_processed": 0,
            "total_records_before": 0,
            "total_records_after": 0,
            "duplicates_removed": 0,
            "fields_standardized": 0
        }
        
        # å¤„ç†æ¯ä¸ªAPIç›®å½•
        for api_dir in category_dir.iterdir():
            if not api_dir.is_dir():
                continue
                
            api_name = api_dir.name
            optimized_api_dir = optimized_category_dir / api_name
            optimized_api_dir.mkdir(exist_ok=True)
            
            logging.info(f"  ğŸ“ å¤„ç†API: {api_name}")
            
            # æ”¶é›†æ‰€æœ‰CSVæ–‡ä»¶
            csv_files = list(api_dir.glob("*.csv"))
            if not csv_files:
                continue
            
            api_stats = {
                "files": len(csv_files),
                "records_before": 0,
                "records_after": 0,
                "duplicates": 0
            }
            
            # å¤„ç†æ¯ä¸ªCSVæ–‡ä»¶
            for csv_file in csv_files:
                try:
                    df = pd.read_csv(csv_file)
                    original_records = len(df)
                    api_stats["records_before"] += original_records
                    
                    if df.empty:
                        continue
                    
                    # 1. æ ‡å‡†åŒ–å­—æ®µå
                    df, fields_renamed = self.standardize_field_names(df, f"{category}/{api_name}/{csv_file.name}")
                    category_stats["fields_standardized"] += fields_renamed
                    
                    # 2. å»é‡å¤„ç†
                    df, duplicates_removed, key_columns = self.detect_duplicates(df)
                    api_stats["duplicates"] += duplicates_removed
                    api_stats["records_after"] += len(df)
                    
                    if duplicates_removed > 0:
                        logging.info(f"    ğŸ§¹ {csv_file.name}: ç§»é™¤ {duplicates_removed} æ¡é‡å¤è®°å½•")
                    
                    # 3. ä¿å­˜ä¼˜åŒ–åçš„æ•°æ®
                    if not df.empty:
                        optimized_file = optimized_api_dir / csv_file.name
                        df.to_csv(optimized_file, index=False, encoding='utf-8-sig')
                    
                    # 4. åˆ›å»ºå…ƒæ•°æ®
                    metadata = {
                        "original_file": str(csv_file.relative_to(self.data_dir)),
                        "records_before": original_records,
                        "records_after": len(df),
                        "duplicates_removed": duplicates_removed,
                        "key_columns": key_columns,
                        "fields_renamed": fields_renamed,
                        "data_hash": self.create_data_hash(df),
                        "optimization_time": datetime.now().isoformat()
                    }
                    
                    metadata_file = optimized_api_dir / f"{csv_file.stem}_metadata.json"
                    with open(metadata_file, 'w', encoding='utf-8') as f:
                        json.dump(metadata, f, indent=2, ensure_ascii=False)
                    
                except Exception as e:
                    logging.error(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥: {csv_file} - {e}")
            
            # æ›´æ–°ç»Ÿè®¡
            category_stats["apis_processed"] += 1
            category_stats["files_processed"] += api_stats["files"]
            category_stats["total_records_before"] += api_stats["records_before"]
            category_stats["total_records_after"] += api_stats["records_after"]
            category_stats["duplicates_removed"] += api_stats["duplicates"]
            
            logging.info(f"    âœ… {api_name}: {api_stats['files']} æ–‡ä»¶, "
                        f"{api_stats['records_before']:,} â†’ {api_stats['records_after']:,} è®°å½•")
        
        logging.info(f"âœ… {category} ä¼˜åŒ–å®Œæˆ: {category_stats['duplicates_removed']:,} æ¡é‡å¤è®°å½•ç§»é™¤")
        return category_stats
    
    def create_unified_index(self):
        """åˆ›å»ºç»Ÿä¸€æ•°æ®ç´¢å¼•"""
        logging.info("ğŸ“‡ åˆ›å»ºç»Ÿä¸€æ•°æ®ç´¢å¼•...")
        
        index_db_path = self.optimized_dir / "data_index.db"
        conn = sqlite3.connect(index_db_path)
        cursor = conn.cursor()
        
        # åˆ›å»ºä¸»ç´¢å¼•è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS data_index (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                category TEXT,
                api_name TEXT,
                file_name TEXT,
                file_path TEXT,
                record_count INTEGER,
                data_hash TEXT,
                last_updated TEXT,
                key_fields TEXT
            )
        ''')
        
        # åˆ›å»ºå¿«é€ŸæŸ¥è¯¢ç´¢å¼•
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_category_api ON data_index(category, api_name)
        ''')
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_last_updated ON data_index(last_updated)
        ''')
        
        # æ‰«æä¼˜åŒ–åçš„æ•°æ®å¹¶å»ºç«‹ç´¢å¼•
        categories = ["basic_info", "financial", "special_trading", "governance"]
        total_indexed = 0
        
        for category in categories:
            category_dir = self.optimized_dir / category
            if not category_dir.exists():
                continue
                
            for api_dir in category_dir.iterdir():
                if not api_dir.is_dir():
                    continue
                    
                api_name = api_dir.name
                
                for csv_file in api_dir.glob("*.csv"):
                    # è¯»å–å¯¹åº”çš„å…ƒæ•°æ®
                    metadata_file = api_dir / f"{csv_file.stem}_metadata.json"
                    metadata = {}
                    if metadata_file.exists():
                        with open(metadata_file, 'r', encoding='utf-8') as f:
                            metadata = json.load(f)
                    
                    # æ’å…¥ç´¢å¼•è®°å½•
                    cursor.execute('''
                        INSERT OR REPLACE INTO data_index 
                        (category, api_name, file_name, file_path, record_count, data_hash, last_updated, key_fields)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                    ''', (
                        category,
                        api_name,
                        csv_file.name,
                        str(csv_file.relative_to(self.optimized_dir)),
                        metadata.get('records_after', 0),
                        metadata.get('data_hash', ''),
                        metadata.get('optimization_time', ''),
                        json.dumps(metadata.get('key_columns', []))
                    ))
                    
                    total_indexed += 1
        
        conn.commit()
        conn.close()
        
        logging.info(f"ğŸ“‡ ç´¢å¼•åˆ›å»ºå®Œæˆ: {total_indexed} ä¸ªæ–‡ä»¶å·²å»ºç«‹ç´¢å¼•")
        return total_indexed
    
    def run_optimization(self):
        """æ‰§è¡Œå®Œæ•´çš„æ•°æ®ä¼˜åŒ–æµç¨‹"""
        logging.info("ğŸš€ å¼€å§‹æ•°æ®ä¼˜åŒ–æ•´ç†...")
        
        start_time = datetime.now()
        categories = ["basic_info", "financial", "special_trading", "governance"]
        
        total_stats = {
            "categories_processed": 0,
            "apis_processed": 0,
            "files_processed": 0,
            "total_records_before": 0,
            "total_records_after": 0,
            "duplicates_removed": 0,
            "fields_standardized": 0
        }
        
        # ä¼˜åŒ–æ¯ä¸ªç±»åˆ«
        for category in categories:
            try:
                category_stats = self.optimize_single_category(category)
                
                # ç´¯è®¡ç»Ÿè®¡
                for key in category_stats:
                    if key in total_stats:
                        total_stats[key] += category_stats[key]
                total_stats["categories_processed"] += 1
                
            except Exception as e:
                logging.error(f"âŒ {category} ä¼˜åŒ–å¤±è´¥: {e}")
        
        # åˆ›å»ºç»Ÿä¸€ç´¢å¼•
        try:
            indexed_files = self.create_unified_index()
        except Exception as e:
            logging.error(f"âŒ ç´¢å¼•åˆ›å»ºå¤±è´¥: {e}")
            indexed_files = 0
        
        # ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š
        end_time = datetime.now()
        duration = end_time - start_time
        
        optimization_report = {
            "optimization_time": end_time.isoformat(),
            "duration_seconds": duration.total_seconds(),
            "statistics": total_stats,
            "indexed_files": indexed_files,
            "compression_ratio": round((total_stats["total_records_before"] - total_stats["total_records_after"]) / total_stats["total_records_before"] * 100, 2) if total_stats["total_records_before"] > 0 else 0
        }
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = self.optimized_dir / "optimization_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(optimization_report, f, indent=2, ensure_ascii=False)
        
        # è¾“å‡ºæ‘˜è¦
        logging.info("ğŸŠ æ•°æ®ä¼˜åŒ–æ•´ç†å®Œæˆ!")
        logging.info("=" * 50)
        logging.info("ğŸ“Š **ä¼˜åŒ–ç»“æœæ‘˜è¦**")
        logging.info(f"  ğŸ“ å¤„ç†ç±»åˆ«: {total_stats['categories_processed']}")
        logging.info(f"  ğŸ”Œ å¤„ç†API: {total_stats['apis_processed']}")
        logging.info(f"  ğŸ“„ å¤„ç†æ–‡ä»¶: {total_stats['files_processed']}")
        logging.info(f"  ğŸ“ è®°å½•æ•°å˜åŒ–: {total_stats['total_records_before']:,} â†’ {total_stats['total_records_after']:,}")
        logging.info(f"  ğŸ§¹ ç§»é™¤é‡å¤: {total_stats['duplicates_removed']:,} æ¡")
        logging.info(f"  ğŸ“ æ ‡å‡†åŒ–å­—æ®µ: {total_stats['fields_standardized']} ä¸ª")
        logging.info(f"  ğŸ“‡ å»ºç«‹ç´¢å¼•: {indexed_files} ä¸ªæ–‡ä»¶")
        logging.info(f"  ğŸ“Š å‹ç¼©æ¯”: {optimization_report['compression_ratio']}%")
        logging.info(f"  â±ï¸ å¤„ç†æ—¶é•¿: {duration}")
        
        return optimization_report

if __name__ == "__main__":
    optimizer = DataOptimizer()
    optimizer.run_optimization()