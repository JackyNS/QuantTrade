#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®è´¨é‡éªŒè¯å·¥å…· - éªŒè¯å·²ä¸‹è½½æ•°æ®çš„å®Œæ•´æ€§å’Œè´¨é‡
"""

import pandas as pd
from pathlib import Path
import json
import logging
from datetime import datetime
import os

class DataQualityValidator:
    """æ•°æ®è´¨é‡éªŒè¯å™¨"""
    
    def __init__(self):
        self.data_dir = Path("data/final_comprehensive_download")
        self.setup_logging()
        
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        log_file = self.data_dir / "data_quality_validation.log"
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )
    
    def scan_data_structure(self):
        """æ‰«ææ•°æ®ç»“æ„"""
        logging.info("ğŸ” å¼€å§‹æ•°æ®ç»“æ„æ‰«æ...")
        
        categories = ["basic_info", "financial", "special_trading", "governance"]
        structure = {}
        total_files = 0
        total_size = 0
        
        for category in categories:
            category_dir = self.data_dir / category
            if not category_dir.exists():
                logging.warning(f"âŒ ç±»åˆ«ç›®å½•ä¸å­˜åœ¨: {category}")
                continue
                
            category_info = {
                "apis": {},
                "file_count": 0,
                "total_size": 0
            }
            
            # æ‰«ææ¯ä¸ªAPIç›®å½•
            for api_dir in category_dir.iterdir():
                if not api_dir.is_dir():
                    continue
                    
                api_info = {
                    "files": [],
                    "file_count": 0,
                    "total_size": 0,
                    "record_count": 0
                }
                
                # æ‰«æAPIç›®å½•ä¸­çš„CSVæ–‡ä»¶
                for csv_file in api_dir.glob("*.csv"):
                    file_size = csv_file.stat().st_size
                    api_info["files"].append({
                        "name": csv_file.name,
                        "size": file_size,
                        "size_mb": round(file_size / 1024 / 1024, 2)
                    })
                    api_info["file_count"] += 1
                    api_info["total_size"] += file_size
                    total_files += 1
                    total_size += file_size
                
                category_info["apis"][api_dir.name] = api_info
                category_info["file_count"] += api_info["file_count"]
                category_info["total_size"] += api_info["total_size"]
            
            structure[category] = category_info
            logging.info(f"âœ… {category}: {category_info['file_count']} æ–‡ä»¶, "
                        f"{round(category_info['total_size']/1024/1024/1024, 2)} GB")
        
        logging.info(f"ğŸ“Š æ€»è®¡: {total_files} æ–‡ä»¶, {round(total_size/1024/1024/1024, 2)} GB")
        return structure, total_files, total_size
    
    def validate_sample_data(self):
        """éªŒè¯æ ·æœ¬æ•°æ®è´¨é‡"""
        logging.info("ğŸ§ª å¼€å§‹æ ·æœ¬æ•°æ®éªŒè¯...")
        
        validation_results = {
            "empty_files": [],
            "corrupted_files": [],
            "sample_validations": [],
            "total_records_estimated": 0
        }
        
        categories = ["basic_info", "financial", "special_trading", "governance"]
        sample_count = 0
        
        for category in categories:
            category_dir = self.data_dir / category
            if not category_dir.exists():
                continue
                
            # ä»æ¯ä¸ªç±»åˆ«éšæœºé€‰æ‹©å‡ ä¸ªæ–‡ä»¶è¿›è¡ŒéªŒè¯
            csv_files = list(category_dir.rglob("*.csv"))
            if not csv_files:
                continue
                
            # é€‰æ‹©å‰å‡ ä¸ªæ–‡ä»¶è¿›è¡Œè¯¦ç»†éªŒè¯
            sample_files = csv_files[:min(3, len(csv_files))]
            
            for csv_file in sample_files:
                try:
                    df = pd.read_csv(csv_file)
                    sample_count += 1
                    
                    validation = {
                        "file": str(csv_file.relative_to(self.data_dir)),
                        "rows": len(df),
                        "columns": len(df.columns),
                        "file_size_mb": round(csv_file.stat().st_size / 1024 / 1024, 2),
                        "empty_ratio": df.isnull().sum().sum() / (len(df) * len(df.columns)) if len(df) > 0 else 1,
                        "column_names": list(df.columns)[:10],  # å‰10ä¸ªåˆ—å
                        "status": "valid"
                    }
                    
                    if len(df) == 0:
                        validation_results["empty_files"].append(str(csv_file.relative_to(self.data_dir)))
                        validation["status"] = "empty"
                    
                    validation_results["sample_validations"].append(validation)
                    validation_results["total_records_estimated"] += len(df) * (len(csv_files) / len(sample_files))
                    
                    if sample_count <= 5:  # åªæ˜¾ç¤ºå‰5ä¸ªè¯¦ç»†ä¿¡æ¯
                        logging.info(f"âœ… {validation['file']}: {validation['rows']:,} è¡Œ, "
                                   f"{validation['columns']} åˆ—, {validation['file_size_mb']} MB")
                    
                except Exception as e:
                    validation_results["corrupted_files"].append({
                        "file": str(csv_file.relative_to(self.data_dir)),
                        "error": str(e)
                    })
                    logging.error(f"âŒ æ–‡ä»¶éªŒè¯å¤±è´¥: {csv_file.relative_to(self.data_dir)} - {e}")
        
        logging.info(f"ğŸ“Š æ ·æœ¬éªŒè¯å®Œæˆ: {len(validation_results['sample_validations'])} ä¸ªæœ‰æ•ˆæ–‡ä»¶")
        logging.info(f"ğŸ“Š ä¼°è®¡æ€»è®°å½•æ•°: {validation_results['total_records_estimated']:,.0f}")
        
        return validation_results
    
    def check_api_coverage(self):
        """æ£€æŸ¥APIè¦†ç›–ç‡"""
        logging.info("ğŸ“‹ æ£€æŸ¥APIè¦†ç›–ç‡...")
        
        # è¯»å–è¿›åº¦æ–‡ä»¶
        progress_file = self.data_dir / "download_progress.json"
        if progress_file.exists():
            with open(progress_file, 'r', encoding='utf-8') as f:
                progress_data = json.load(f)
            
            completed_apis = progress_data.get("completed_apis", [])
            total_records = progress_data.get("statistics", {}).get("total_records", 0)
            
            logging.info(f"âœ… APIè¦†ç›–ç‡: {len(completed_apis)} ä¸ªAPIå®Œæˆ")
            logging.info(f"ğŸ“Š æ€»è®°å½•æ•°: {total_records:,}")
            
            # æŒ‰ç±»åˆ«ç»Ÿè®¡
            category_stats = {}
            for api in completed_apis:
                category = api.split('_')[0] if '_' in api else 'unknown'
                category_stats[category] = category_stats.get(category, 0) + 1
            
            for category, count in category_stats.items():
                logging.info(f"  - {category}: {count} ä¸ªAPI")
            
            return {
                "total_apis": len(completed_apis),
                "total_records": total_records,
                "category_stats": category_stats
            }
        else:
            logging.warning("âŒ æœªæ‰¾åˆ°è¿›åº¦æ–‡ä»¶")
            return None
    
    def generate_quality_report(self):
        """ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Š"""
        logging.info("ğŸ“ ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Š...")
        
        # æ”¶é›†æ‰€æœ‰éªŒè¯ç»“æœ
        structure, total_files, total_size = self.scan_data_structure()
        validation_results = self.validate_sample_data()
        api_coverage = self.check_api_coverage()
        
        # ç”ŸæˆæŠ¥å‘Š
        report = {
            "generation_time": datetime.now().isoformat(),
            "summary": {
                "total_files": total_files,
                "total_size_gb": round(total_size / 1024 / 1024 / 1024, 2),
                "total_apis": api_coverage["total_apis"] if api_coverage else 0,
                "total_records": api_coverage["total_records"] if api_coverage else 0,
                "estimated_records_from_sample": int(validation_results["total_records_estimated"])
            },
            "data_structure": structure,
            "quality_validation": validation_results,
            "api_coverage": api_coverage,
            "issues": {
                "empty_files": len(validation_results["empty_files"]),
                "corrupted_files": len(validation_results["corrupted_files"])
            }
        }
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = self.data_dir / "data_quality_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        logging.info(f"ğŸ“Š è´¨é‡æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        
        # æ‰“å°æ‘˜è¦
        logging.info("ğŸŠ æ•°æ®è´¨é‡éªŒè¯å®Œæˆ!")
        logging.info("=" * 50)
        logging.info("ğŸ“Š **æœ€ç»ˆæ•°æ®æ‘˜è¦**")
        logging.info(f"  ğŸ“ æ€»æ–‡ä»¶æ•°: {report['summary']['total_files']:,}")
        logging.info(f"  ğŸ’¾ æ€»æ•°æ®é‡: {report['summary']['total_size_gb']} GB")
        logging.info(f"  ğŸ”Œ APIæ•°é‡: {report['summary']['total_apis']}")
        logging.info(f"  ğŸ“ æ€»è®°å½•æ•°: {report['summary']['total_records']:,}")
        
        if api_coverage:
            logging.info("\nğŸ“‹ **APIåˆ†ç±»ç»Ÿè®¡**:")
            for category, count in api_coverage["category_stats"].items():
                logging.info(f"  - {category}: {count} ä¸ªAPI")
        
        if validation_results["empty_files"]:
            logging.info(f"\nâš ï¸  **å‘ç°é—®é¢˜**: {len(validation_results['empty_files'])} ä¸ªç©ºæ–‡ä»¶")
        if validation_results["corrupted_files"]:
            logging.info(f"âš ï¸  **å‘ç°é—®é¢˜**: {len(validation_results['corrupted_files'])} ä¸ªæŸåæ–‡ä»¶")
        
        if not validation_results["empty_files"] and not validation_results["corrupted_files"]:
            logging.info("\nâœ… **æ•°æ®è´¨é‡**: ä¼˜ç§€ï¼Œæ— å‘ç°é‡å¤§é—®é¢˜")
        
        return report

if __name__ == "__main__":
    validator = DataQualityValidator()
    validator.generate_quality_report()