#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®æŸ¥è¯¢å·¥å…· - åŸºäºä¼˜åŒ–åçš„æ•°æ®ç´¢å¼•è¿›è¡Œå¿«é€ŸæŸ¥è¯¢
"""

import sqlite3
import pandas as pd
from pathlib import Path
import json

class DataQueryTool:
    """æ•°æ®æŸ¥è¯¢å·¥å…·"""
    
    def __init__(self):
        self.optimized_dir = Path("data/optimized_data")
        self.index_db_path = self.optimized_dir / "data_index.db"
        
    def query_data_catalog(self):
        """æŸ¥è¯¢æ•°æ®ç›®å½•"""
        if not self.index_db_path.exists():
            print("âŒ æ•°æ®ç´¢å¼•ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œæ•°æ®ä¼˜åŒ–")
            return
        
        conn = sqlite3.connect(self.index_db_path)
        cursor = conn.cursor()
        
        # æ€»ä½“ç»Ÿè®¡
        cursor.execute("SELECT COUNT(*), SUM(record_count) FROM data_index")
        total_files, total_records = cursor.fetchone()
        
        print("ğŸ“Š **ä¼˜åŒ–åæ•°æ®ç›®å½•**")
        print("=" * 50)
        print(f"ğŸ“ æ€»æ–‡ä»¶æ•°: {total_files:,}")
        print(f"ğŸ“ æ€»è®°å½•æ•°: {total_records:,}")
        
        # æŒ‰ç±»åˆ«ç»Ÿè®¡
        cursor.execute("""
            SELECT category, COUNT(*) as file_count, SUM(record_count) as record_count
            FROM data_index 
            GROUP BY category 
            ORDER BY record_count DESC
        """)
        
        print("\nğŸ“‹ **æŒ‰ç±»åˆ«ç»Ÿè®¡**:")
        for category, file_count, record_count in cursor.fetchall():
            print(f"  {category}: {file_count} æ–‡ä»¶, {record_count:,} è®°å½•")
        
        # æŒ‰APIç»Ÿè®¡å‰10
        cursor.execute("""
            SELECT category, api_name, COUNT(*) as file_count, SUM(record_count) as record_count
            FROM data_index 
            GROUP BY category, api_name 
            ORDER BY record_count DESC
            LIMIT 10
        """)
        
        print("\nğŸ”¥ **è®°å½•æ•°æœ€å¤šçš„å‰10ä¸ªAPI**:")
        for category, api_name, file_count, record_count in cursor.fetchall():
            print(f"  {category}/{api_name}: {file_count} æ–‡ä»¶, {record_count:,} è®°å½•")
        
        conn.close()
    
    def search_apis(self, keyword=""):
        """æœç´¢API"""
        if not self.index_db_path.exists():
            print("âŒ æ•°æ®ç´¢å¼•ä¸å­˜åœ¨")
            return []
        
        conn = sqlite3.connect(self.index_db_path)
        cursor = conn.cursor()
        
        if keyword:
            cursor.execute("""
                SELECT DISTINCT category, api_name, COUNT(*) as file_count, SUM(record_count) as record_count
                FROM data_index 
                WHERE api_name LIKE ? OR category LIKE ?
                GROUP BY category, api_name
                ORDER BY record_count DESC
            """, (f"%{keyword}%", f"%{keyword}%"))
            print(f"ğŸ” æœç´¢ç»“æœ (å…³é”®è¯: '{keyword}'):")
        else:
            cursor.execute("""
                SELECT DISTINCT category, api_name, COUNT(*) as file_count, SUM(record_count) as record_count
                FROM data_index 
                GROUP BY category, api_name
                ORDER BY category, api_name
            """)
            print("ğŸ“‹ æ‰€æœ‰å¯ç”¨API:")
        
        results = cursor.fetchall()
        for category, api_name, file_count, record_count in results:
            print(f"  {category}/{api_name}: {file_count} æ–‡ä»¶, {record_count:,} è®°å½•")
        
        conn.close()
        return results
    
    def load_sample_data(self, category, api_name, max_rows=100):
        """åŠ è½½æ ·æœ¬æ•°æ®"""
        if not self.index_db_path.exists():
            print("âŒ æ•°æ®ç´¢å¼•ä¸å­˜åœ¨")
            return None
        
        conn = sqlite3.connect(self.index_db_path)
        cursor = conn.cursor()
        
        # æŸ¥æ‰¾æ–‡ä»¶
        cursor.execute("""
            SELECT file_path, record_count 
            FROM data_index 
            WHERE category = ? AND api_name = ?
            ORDER BY record_count DESC
            LIMIT 1
        """, (category, api_name))
        
        result = cursor.fetchone()
        if not result:
            print(f"âŒ æœªæ‰¾åˆ° {category}/{api_name}")
            conn.close()
            return None
        
        file_path, record_count = result
        full_path = self.optimized_dir / file_path
        
        if not full_path.exists():
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {full_path}")
            conn.close()
            return None
        
        try:
            # åŠ è½½æ ·æœ¬æ•°æ®
            df = pd.read_csv(full_path, nrows=max_rows)
            print(f"âœ… æˆåŠŸåŠ è½½ {category}/{api_name}")
            print(f"ğŸ“Š æ ·æœ¬: {len(df)} è¡Œ (æ€»è®¡ {record_count:,} è¡Œ)")
            print(f"ğŸ“‹ å­—æ®µ: {list(df.columns)}")
            
            if not df.empty:
                print("\nğŸ“„ **å‰5è¡Œæ•°æ®é¢„è§ˆ**:")
                print(df.head().to_string())
            
            conn.close()
            return df
            
        except Exception as e:
            print(f"âŒ åŠ è½½æ•°æ®å¤±è´¥: {e}")
            conn.close()
            return None
    
    def get_data_quality_summary(self):
        """è·å–æ•°æ®è´¨é‡æ‘˜è¦"""
        quality_report_path = Path("data/final_comprehensive_download/data_quality_report.json")
        optimization_report_path = self.optimized_dir / "optimization_report.json"
        
        summary = {}
        
        # è¯»å–è´¨é‡æŠ¥å‘Š
        if quality_report_path.exists():
            with open(quality_report_path, 'r', encoding='utf-8') as f:
                quality_data = json.load(f)
            summary["original_data"] = quality_data["summary"]
        
        # è¯»å–ä¼˜åŒ–æŠ¥å‘Š
        if optimization_report_path.exists():
            with open(optimization_report_path, 'r', encoding='utf-8') as f:
                optimization_data = json.load(f)
            summary["optimization"] = optimization_data["statistics"]
            summary["compression_ratio"] = optimization_data["compression_ratio"]
        
        return summary
    
    def show_performance_comparison(self):
        """æ˜¾ç¤ºæ€§èƒ½ä¼˜åŒ–å¯¹æ¯”"""
        summary = self.get_data_quality_summary()
        
        if not summary:
            print("âŒ æœªæ‰¾åˆ°æ€§èƒ½å¯¹æ¯”æ•°æ®")
            return
        
        print("ğŸš€ **æ•°æ®ä¼˜åŒ–æ€§èƒ½å¯¹æ¯”**")
        print("=" * 50)
        
        if "original_data" in summary:
            orig = summary["original_data"]
            print("ğŸ“Š **åŸå§‹æ•°æ®**:")
            print(f"  ğŸ“ æ–‡ä»¶æ•°: {orig.get('total_files', 0):,}")
            print(f"  ğŸ’¾ æ•°æ®é‡: {orig.get('total_size_gb', 0)} GB")
            print(f"  ğŸ“ è®°å½•æ•°: {orig.get('total_records', 0):,}")
        
        if "optimization" in summary:
            opt = summary["optimization"]
            print("\nâœ¨ **ä¼˜åŒ–åæ•°æ®**:")
            print(f"  ğŸ“ å¤„ç†æ–‡ä»¶: {opt.get('files_processed', 0):,}")
            print(f"  ğŸ“ è®°å½•æ•°å˜åŒ–: {opt.get('total_records_before', 0):,} â†’ {opt.get('total_records_after', 0):,}")
            print(f"  ğŸ§¹ ç§»é™¤é‡å¤: {opt.get('duplicates_removed', 0):,} æ¡")
            print(f"  ğŸ“ æ ‡å‡†åŒ–å­—æ®µ: {opt.get('fields_standardized', 0):,} ä¸ª")
            print(f"  ğŸ“Š å‹ç¼©æ¯”: {summary.get('compression_ratio', 0)}%")
        
        print("\nğŸ¯ **ä¼˜åŒ–æ•ˆæœ**:")
        print("  âœ… æ•°æ®æ ‡å‡†åŒ–å®Œæˆ")
        print("  âœ… é‡å¤æ•°æ®æ¸…ç†")
        print("  âœ… å¿«é€Ÿç´¢å¼•å»ºç«‹") 
        print("  âœ… æŸ¥è¯¢æ€§èƒ½ä¼˜åŒ–")

def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºæ•°æ®æŸ¥è¯¢åŠŸèƒ½"""
    tool = DataQueryTool()
    
    print("ğŸ” **æ•°æ®æŸ¥è¯¢å·¥å…·æ¼”ç¤º**\n")
    
    # 1. æ˜¾ç¤ºæ•°æ®ç›®å½•
    tool.query_data_catalog()
    
    # 2. æ˜¾ç¤ºæ€§èƒ½å¯¹æ¯”
    print("\n")
    tool.show_performance_comparison()
    
    # 3. æœç´¢ç¤ºä¾‹
    print("\n")
    tool.search_apis("fdmt")
    
    # 4. åŠ è½½æ ·æœ¬æ•°æ®ç¤ºä¾‹
    print("\nğŸ§ª **æ ·æœ¬æ•°æ®åŠ è½½æ¼”ç¤º**:")
    sample_df = tool.load_sample_data("financial", "fdmtindipsget")
    
    print("\nğŸŠ æ•°æ®æŸ¥è¯¢ç³»ç»Ÿæ¼”ç¤ºå®Œæˆï¼")
    print("ğŸ’¡ æç¤º: ç°åœ¨å¯ä»¥é€šè¿‡ç´¢å¼•å¿«é€ŸæŸ¥è¯¢å’Œè®¿é—®ä»»ä½•APIæ•°æ®")

if __name__ == "__main__":
    main()