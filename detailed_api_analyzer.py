#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¯¦ç»†APIåˆ†æå™¨ - æä¾›æ‰€æœ‰APIçš„å…·ä½“æè¿°ã€æ•°æ®æ—¶é—´èŒƒå›´å’Œè´¨é‡åˆ†æ
"""

import pandas as pd
import numpy as np
from pathlib import Path
import logging
from datetime import datetime
import re
import warnings
warnings.filterwarnings('ignore')

class DetailedAPIAnalyzer:
    """è¯¦ç»†APIåˆ†æå™¨"""
    
    def __init__(self):
        self.base_dir = Path("data/final_comprehensive_download")
        self.api_descriptions = self.load_api_descriptions()
        self.setup_logging()
        
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        log_file = Path("detailed_api_analysis.log")
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
    
    def load_api_descriptions(self):
        """åŠ è½½APIä¸­æ–‡æè¿°æ˜ å°„"""
        return {
            # Basic Info APIs
            'secidget': 'è¯åˆ¸åŸºç¡€ä¿¡æ¯è¡¨ - åŒ…å«è‚¡ç¥¨ä»£ç ã€åç§°ã€ä¸Šå¸‚æ—¥æœŸç­‰åŸºç¡€ä¿¡æ¯',
            'equget': 'è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯ - Aè‚¡ã€Bè‚¡ã€Hè‚¡ç­‰è‚¡ç¥¨åŸºæœ¬å±æ€§ä¿¡æ¯', 
            'equipoget': 'IPOåŸºæœ¬ä¿¡æ¯ - é¦–æ¬¡å…¬å¼€å‘è¡Œè‚¡ç¥¨çš„åŸºæœ¬ä¿¡æ¯',
            'equindustryget': 'è‚¡ç¥¨è¡Œä¸šåˆ†ç±» - å„ç§è¡Œä¸šåˆ†ç±»æ ‡å‡†ä¸‹çš„è‚¡ç¥¨å½’å±',
            'equdivget': 'åˆ†çº¢é€è‚¡ä¿¡æ¯ - å†å²åˆ†çº¢ã€é€è‚¡ã€è½¬è‚¡ç­‰æƒç›Šåˆ†æ´¾æ•°æ®',
            'equsplitsget': 'è‚¡ç¥¨æ‹†ç»†ä¿¡æ¯ - è‚¡ç¥¨æ‹†ç»†ã€åˆå¹¶ç­‰èµ„æœ¬ç»“æ„è°ƒæ•´',
            'mktidxdget': 'æŒ‡æ•°æ—¥è¡Œæƒ…æ•°æ® - å„ç±»æŒ‡æ•°çš„å†å²ä»·æ ¼å’Œæˆäº¤æ•°æ®',
            
            # Financial APIs  
            'fdmtisalllatestget': 'åˆ©æ¶¦è¡¨æœ€æ–°æ•°æ® - ä¸Šå¸‚å…¬å¸åˆ©æ¶¦è¡¨ç§‘ç›®æœ€æ–°è´¢æŠ¥æ•°æ®',
            'fdmtbsalllatestget': 'èµ„äº§è´Ÿå€ºè¡¨æœ€æ–°æ•°æ® - ä¸Šå¸‚å…¬å¸èµ„äº§è´Ÿå€ºè¡¨æœ€æ–°è´¢æŠ¥',
            'fdmtcfalllatestget': 'ç°é‡‘æµé‡è¡¨æœ€æ–°æ•°æ® - ä¸Šå¸‚å…¬å¸ç°é‡‘æµé‡è¡¨æœ€æ–°è´¢æŠ¥',
            'fdmtisindualllatestget': 'è¡Œä¸šåˆ©æ¶¦è¡¨æ•°æ® - æŒ‰è¡Œä¸šæ±‡æ€»çš„åˆ©æ¶¦è¡¨æ•°æ®',
            'fdmtbsindualllatestget': 'è¡Œä¸šèµ„äº§è´Ÿå€ºè¡¨æ•°æ® - æŒ‰è¡Œä¸šæ±‡æ€»çš„èµ„äº§è´Ÿå€ºè¡¨',
            'fdmtcfindualllatestget': 'è¡Œä¸šç°é‡‘æµé‡è¡¨æ•°æ® - æŒ‰è¡Œä¸šæ±‡æ€»çš„ç°é‡‘æµé‡è¡¨',
            'fdmtisbankalllatestget': 'é“¶è¡Œåˆ©æ¶¦è¡¨æ•°æ® - é“¶è¡Œä¸šä¸“ç”¨åˆ©æ¶¦è¡¨æ ¼å¼æ•°æ®',
            'fdmtbsbankalllatestget': 'é“¶è¡Œèµ„äº§è´Ÿå€ºè¡¨æ•°æ® - é“¶è¡Œä¸šä¸“ç”¨èµ„äº§è´Ÿå€ºè¡¨',
            'fdmtcfbankalllatestget': 'é“¶è¡Œç°é‡‘æµé‡è¡¨æ•°æ® - é“¶è¡Œä¸šä¸“ç”¨ç°é‡‘æµé‡è¡¨',
            'fdmtderget': 'è¡ç”Ÿå·¥å…·å…¬å…ä»·å€¼ - é‡‘èè¡ç”Ÿå·¥å…·çš„å…¬å…ä»·å€¼æ•°æ®',
            'fdmtindipsget': 'è¡Œä¸šæŒ‡æ ‡æ•°æ® - å„è¡Œä¸šçš„å…³é”®è´¢åŠ¡æŒ‡æ ‡ç»Ÿè®¡',
            'fdmtindigrowthget': 'è¡Œä¸šæˆé•¿æ€§æŒ‡æ ‡ - è¡Œä¸šå¢é•¿ç‡ç­‰æˆé•¿æ€§æŒ‡æ ‡',
            
            # Special Trading APIs
            'mktlimitget': 'æ¶¨è·Œåœæ•°æ® - æ¯æ—¥æ¶¨åœã€è·Œåœè‚¡ç¥¨ä¿¡æ¯åŠè§¦æ¿æ—¶é—´',
            'mktblockdget': 'å¤§å®—äº¤æ˜“æ•°æ® - å¤§å®—äº¤æ˜“æˆäº¤æ˜ç»†å’Œç»Ÿè®¡ä¿¡æ¯',
            'fstdetailget': 'èèµ„èåˆ¸æ˜ç»† - ä¸ªè‚¡èèµ„èåˆ¸æ¯æ—¥äº¤æ˜“æ˜ç»†æ•°æ®',
            'fsttotalget': 'èèµ„èåˆ¸æ±‡æ€» - å¸‚åœºèèµ„èåˆ¸æ•´ä½“ç»Ÿè®¡æ•°æ®',
            'fstdetailget': 'èèµ„èåˆ¸æ¯æ—¥æ˜ç»† - ä¸ªè‚¡èèµ„èåˆ¸è¯¦ç»†äº¤æ˜“è®°å½•',
            'sechaltget': 'åœç‰Œå¤ç‰Œä¿¡æ¯ - è‚¡ç¥¨åœç‰Œã€å¤ç‰Œå…¬å‘ŠåŠåŸå› ',
            'vfsttargetget': 'èèµ„èåˆ¸æ ‡çš„ - å¯è¿›è¡Œèèµ„èåˆ¸äº¤æ˜“çš„è¯åˆ¸åå•',
            'equisactivityget': 'å¢å‘é…è‚¡æ´»åŠ¨ - è‚¡ç¥¨å¢å‘ã€é…è‚¡ç­‰å†èèµ„æ´»åŠ¨',
            'equisparticipantqaget': 'ç½‘ä¸Šå‘è¡Œä¸­ç­¾ä¿¡æ¯ - IPOç½‘ä¸Šç”³è´­ä¸­ç­¾ç»“æœ',
            'mktconsbondpremiumget': 'å¯è½¬å€ºè½¬è‚¡ä»·å€¼ - å¯è½¬æ¢å€ºåˆ¸è½¬è‚¡ä»·å€¼åŠæº¢ä»·ç‡',
            'mktrankdivyieldget': 'è‚¡æ¯ç‡æ’è¡Œ - æŒ‰è‚¡æ¯ç‡æ’åçš„è‚¡ç¥¨åˆ—è¡¨',
            'mktranklistsalesget': 'è¥ä¸šæ”¶å…¥æ’è¡Œ - æŒ‰è¥ä¸šæ”¶å…¥æ’åçš„ä¸Šå¸‚å…¬å¸',
            'mktrankliststocksget': 'è‚¡ç¥¨æ’è¡Œæ¦œ - å„ç±»è‚¡ç¥¨æ’è¡Œæ¦œæ•°æ®',
            'mktrankinsttrget': 'æœºæ„å¸­ä½äº¤æ˜“æ’å - æœºæ„ä¸“ç”¨å¸­ä½äº¤æ˜“ç»Ÿè®¡',
            'mktequdstatsget': 'è‚¡ç¥¨äº¤æ˜“ç»Ÿè®¡ - è‚¡ç¥¨å¸‚åœºäº¤æ˜“é‡ä»·ç»Ÿè®¡æ•°æ®',
            'mktipocontraddaysget': 'IPOè¿ç»­æ¶¨åœå¤©æ•° - æ–°è‚¡ä¸Šå¸‚åè¿ç»­æ¶¨åœç»Ÿè®¡',
            'fdmtee': 'ç¯å¢ƒä¿æŠ¤æŠ•èµ„ - ä¸Šå¸‚å…¬å¸ç¯ä¿æŠ•èµ„æ”¯å‡ºæ•°æ®',
            'mktrankinstr': 'æœºæ„æŠ•èµ„è€…æ’è¡Œ - æœºæ„æŠ•èµ„è€…æŒä»“æ’è¡Œæ•°æ®',
            'equmarginsecget': 'èèµ„èåˆ¸æ ‡çš„è¯åˆ¸ - èèµ„èåˆ¸ä¸šåŠ¡æ ‡çš„è¯åˆ¸æ¸…å•',
            'mktequperfget': 'è‚¡ç¥¨ä¸šç»©è¡¨ç° - è‚¡ç¥¨å„ç±»ä¸šç»©è¡¨ç°æŒ‡æ ‡',
            
            # Governance APIs
            'equshtenget': 'åå¤§è‚¡ä¸œä¿¡æ¯ - ä¸Šå¸‚å…¬å¸å‰åå¤§è‚¡ä¸œæŒè‚¡æ˜ç»†',
            'equfloatshtenget': 'åå¤§æµé€šè‚¡ä¸œ - å‰åå¤§æµé€šè‚¡ä¸œæŒè‚¡ä¿¡æ¯',
            'equshareholdernumget': 'è‚¡ä¸œæˆ·æ•°å˜åŒ– - å†å²è‚¡ä¸œæˆ·æ•°å˜åŒ–è¶‹åŠ¿',
            'equactualcontrollerget': 'å®é™…æ§åˆ¶äººä¿¡æ¯ - ä¸Šå¸‚å…¬å¸å®é™…æ§åˆ¶äººå˜æ›´',
            'equexecsholdingsget': 'é«˜ç®¡æŒè‚¡å˜åŠ¨ - è‘£ç›‘é«˜äººå‘˜æŒè‚¡å˜åŠ¨æ˜ç»†',
            'equpledgeget': 'è‚¡æƒè´¨æŠ¼ä¿¡æ¯ - æ§è‚¡è‚¡ä¸œåŠé«˜ç®¡è‚¡æƒè´¨æŠ¼æƒ…å†µ',
            'equstockpledgeget': 'è‚¡ç¥¨è´¨æŠ¼å›è´­ - è‚¡ç¥¨è´¨æŠ¼å¼å›è´­äº¤æ˜“ä¿¡æ¯',
            'equshareholdersmeetingget': 'è‚¡ä¸œå¤§ä¼šä¿¡æ¯ - è‚¡ä¸œå¤§ä¼šå¬å¼€åŠå†³è®®æƒ…å†µ',
            'equrelatedtransactionget': 'å…³è”äº¤æ˜“ä¿¡æ¯ - ä¸Šå¸‚å…¬å¸å…³è”äº¤æ˜“æ˜ç»†',
            'equsharesexcitget': 'é™å”®è‚¡è§£ç¦ - é™å”®è‚¡ä»½è§£ç¦ä¸Šå¸‚è®¡åˆ’å’Œå®æ–½',
            'equchangeplanget': 'è‚¡æœ¬å˜åŠ¨è®¡åˆ’ - é€è‚¡ã€è½¬è‚¡ã€å¢å‘ç­‰è‚¡æœ¬å˜åŠ¨è®¡åˆ’',
            'equiposharefloatget': 'IPOé™å”®è‚¡ä¸Šå¸‚ - IPOé™å”®è‚¡ä»½ä¸Šå¸‚æµé€šä¿¡æ¯',
            'equreformsharefloatget': 'è‚¡æ”¹é™å”®è‚¡ä¸Šå¸‚ - è‚¡æƒåˆ†ç½®æ”¹é©é™å”®è‚¡ä¸Šå¸‚',
            'equpartynatureget': 'æ§è‚¡è‚¡ä¸œæ€§è´¨ - æ§è‚¡è‚¡ä¸œçš„æ€§è´¨åˆ†ç±»ä¿¡æ¯',
            'equallotget': 'é…è‚¡å®æ–½ - é…è‚¡æ–¹æ¡ˆå®æ–½æƒ…å†µåŠç»“æœ',
            'equspopubresultget': 'å¢å‘ç»“æœå…¬å‘Š - å¢å‘è‚¡ç¥¨å‘è¡Œç»“æœå…¬å‘Š',
            'equallotmentsubscriptionresultsget': 'é…è‚¡ç¼´æ¬¾ç»“æœ - é…è‚¡ç¼´æ¬¾ç»“æœç»Ÿè®¡',
            'equspoget': 'å¢å‘è‚¡ç¥¨ä¿¡æ¯ - å¢å‘è‚¡ç¥¨æ–¹æ¡ˆåŠå®æ–½æƒ…å†µ',
            'equoldshofferget': 'è€è‚¡è½¬è®© - IPOè€è‚¡ä¸œå”®è‚¡è½¬è®©ä¿¡æ¯',
            'equmschangesget': 'è‚¡æœ¬ç»“æ„å˜åŠ¨ - è‚¡æœ¬ç»“æ„å†å²å˜åŠ¨è®°å½•',
            'equmanagersget': 'ç®¡ç†å±‚ä¿¡æ¯ - è‘£äº‹ã€ç›‘äº‹ã€é«˜ç®¡äººå‘˜åŸºæœ¬ä¿¡æ¯',
            'equsharesfloatget': 'è§£ç¦è‚¡ä»½æ˜ç»† - å„ç±»é™å”®è‚¡ä»½è§£ç¦æ˜ç»†',
            
            # Additional APIs
            'equ_fancy_factors_lite': 'ç²¾é€‰é‡åŒ–å› å­ - ä¼˜çŸ¿ç²¾é€‰çš„é‡åŒ–æŠ•èµ„å› å­æ•°æ®',
            'sec_type_region': 'åœ°åŸŸåˆ†ç±»ä¿¡æ¯ - ä¸Šå¸‚å…¬å¸æŒ‰åœ°åŸŸçš„åˆ†ç±»æ ‡å‡†',
            'sec_type_rel': 'æ¿å—æˆåˆ†å…³ç³» - è¯åˆ¸ä¸å„ç±»æ¿å—çš„å½’å±å…³ç³»',
            'sec_type': 'æ¿å—åˆ†ç±»ä¿¡æ¯ - æ¦‚å¿µæ¿å—ã€è¡Œä¸šæ¿å—ç­‰åˆ†ç±»æ ‡å‡†',
            'industry': 'è¡Œä¸šåˆ†ç±»æ ‡å‡† - å„ç±»è¡Œä¸šåˆ†ç±»ä½“ç³»åŠæ ‡å‡†å®šä¹‰',
            'fst_total': 'èèµ„èåˆ¸æ±‡æ€» - å…¨å¸‚åœºèèµ„èåˆ¸ä¸šåŠ¡æ±‡æ€»æ•°æ®',
            'fst_detail': 'èèµ„èåˆ¸æ˜ç»† - ä¸ªè‚¡èèµ„èåˆ¸äº¤æ˜“æ˜ç»†è®°å½•',
            'eco_data_china_lite': 'ä¸­å›½å®è§‚ç»æµæ•°æ® - é‡è¦å®è§‚ç»æµæŒ‡æ ‡æ•°æ®'
        }
    
    def analyze_file_time_range(self, file_path):
        """åˆ†ææ–‡ä»¶çš„æ—¶é—´èŒƒå›´"""
        time_info = {
            'start_date': None,
            'end_date': None,
            'date_pattern': 'unknown',
            'sample_dates': []
        }
        
        try:
            # ä»æ–‡ä»¶åæ¨æ–­æ—¶é—´ä¿¡æ¯
            filename = file_path.stem
            
            # æ£€æŸ¥å­£åº¦æ¨¡å¼ï¼š2023_Q1, 2024_Q4ç­‰
            quarter_match = re.search(r'(\d{4})_Q(\d)', filename)
            if quarter_match:
                year, quarter = quarter_match.groups()
                time_info['date_pattern'] = 'quarterly'
                time_info['sample_dates'].append(f"{year}Q{quarter}")
                return time_info
            
            # æ£€æŸ¥å¹´åº¦æ¨¡å¼ï¼šyear_2023, 2024ç­‰
            year_match = re.search(r'(year_)?(\d{4})', filename)
            if year_match:
                year = year_match.group(2)
                time_info['date_pattern'] = 'yearly'
                time_info['sample_dates'].append(year)
                return time_info
            
            # æ£€æŸ¥å¿«ç…§æ¨¡å¼
            if 'snapshot' in filename.lower():
                time_info['date_pattern'] = 'snapshot'
                time_info['sample_dates'].append('å½“å‰å¿«ç…§')
                return time_info
            
            # å°è¯•ä»æ•°æ®å†…å®¹åˆ†æ
            df_sample = pd.read_csv(file_path, nrows=100)
            date_columns = []
            
            for col in df_sample.columns:
                col_lower = col.lower()
                if any(date_word in col_lower for date_word in 
                      ['date', 'time', 'year', 'month', 'day', 'enddate', 'tradedate']):
                    date_columns.append(col)
            
            if date_columns and len(df_sample) > 0:
                # åˆ†æç¬¬ä¸€ä¸ªæ—¥æœŸåˆ—
                date_col = date_columns[0]
                sample_values = df_sample[date_col].dropna().head(5).tolist()
                
                if sample_values:
                    time_info['sample_dates'] = [str(val)[:10] for val in sample_values]
                    time_info['date_pattern'] = 'data_driven'
                    
                    # å°è¯•è§£ææœ€æ—©å’Œæœ€æ™šæ—¥æœŸ
                    try:
                        all_dates = pd.to_datetime(df_sample[date_col].dropna())
                        time_info['start_date'] = all_dates.min().strftime('%Y-%m-%d')
                        time_info['end_date'] = all_dates.max().strftime('%Y-%m-%d')
                    except:
                        pass
        
        except Exception as e:
            time_info['date_pattern'] = 'error'
            time_info['sample_dates'] = [f"è§£æé”™è¯¯: {str(e)[:30]}"]
        
        return time_info
    
    def analyze_data_quality_details(self, file_path):
        """åˆ†ææ•°æ®è´¨é‡è¯¦æƒ…"""
        quality_info = {
            'row_count': 0,
            'column_count': 0,
            'missing_rate': 0.0,
            'duplicate_rate': 0.0,
            'data_types': {},
            'key_columns': [],
            'data_sample': {}
        }
        
        try:
            # è¯»å–æ ·æœ¬æ•°æ®
            df_sample = pd.read_csv(file_path, nrows=1000)
            
            quality_info.update({
                'row_count': len(df_sample),
                'column_count': len(df_sample.columns),
                'missing_rate': (df_sample.isnull().sum().sum() / (len(df_sample) * len(df_sample.columns)) * 100),
                'duplicate_rate': (df_sample.duplicated().sum() / len(df_sample) * 100) if len(df_sample) > 0 else 0
            })
            
            # æ•°æ®ç±»å‹åˆ†æ
            type_counts = df_sample.dtypes.value_counts()
            quality_info['data_types'] = {str(dtype): int(count) for dtype, count in type_counts.items()}
            
            # å…³é”®åˆ—è¯†åˆ«
            key_columns = []
            for col in df_sample.columns[:10]:  # åªåˆ†æå‰10åˆ—
                col_lower = col.lower()
                if any(key_word in col_lower for key_word in 
                      ['code', 'symbol', 'id', 'name', 'date', 'price', 'volume']):
                    key_columns.append(col)
            quality_info['key_columns'] = key_columns
            
            # æ•°æ®æ ·æœ¬
            if len(df_sample) > 0:
                sample_dict = {}
                for col in df_sample.columns[:5]:  # å‰5åˆ—çš„æ ·æœ¬
                    sample_values = df_sample[col].dropna().head(3).tolist()
                    sample_dict[col] = [str(val)[:20] for val in sample_values]
                quality_info['data_sample'] = sample_dict
        
        except Exception as e:
            quality_info['error'] = str(e)
        
        return quality_info
    
    def analyze_single_api(self, api_dir):
        """åˆ†æå•ä¸ªAPIçš„è¯¦ç»†ä¿¡æ¯"""
        api_name = api_dir.name
        csv_files = list(api_dir.glob("*.csv"))
        
        api_info = {
            'api_name': api_name,
            'chinese_description': self.api_descriptions.get(api_name, 'æš‚æ— æè¿°'),
            'file_count': len(csv_files),
            'total_size_mb': 0,
            'time_coverage': {},
            'quality_metrics': {},
            'sample_files': []
        }
        
        if not csv_files:
            api_info['status'] = 'no_data'
            return api_info
        
        # è®¡ç®—æ€»å¤§å°
        total_size = sum(f.stat().st_size for f in csv_files)
        api_info['total_size_mb'] = total_size / (1024 * 1024)
        
        # åˆ†ææ—¶é—´è¦†ç›–ï¼ˆé‡‡æ ·åˆ†æï¼‰
        time_patterns = {}
        sample_files_info = []
        
        for i, csv_file in enumerate(csv_files[:5]):  # åˆ†æå‰5ä¸ªæ–‡ä»¶
            time_info = self.analyze_file_time_range(csv_file)
            quality_info = self.analyze_data_quality_details(csv_file)
            
            sample_files_info.append({
                'filename': csv_file.name,
                'size_mb': csv_file.stat().st_size / (1024 * 1024),
                'time_info': time_info,
                'quality_info': quality_info
            })
            
            pattern = time_info['date_pattern']
            time_patterns[pattern] = time_patterns.get(pattern, 0) + 1
        
        # æ¨æ–­æ€»ä½“æ—¶é—´è¦†ç›–æ¨¡å¼
        most_common_pattern = max(time_patterns, key=time_patterns.get) if time_patterns else 'unknown'
        api_info['time_coverage'] = {
            'pattern': most_common_pattern,
            'pattern_distribution': time_patterns,
            'estimated_date_range': self.estimate_date_range(csv_files, most_common_pattern)
        }
        
        # è´¨é‡æŒ‡æ ‡æ±‡æ€»
        if sample_files_info:
            avg_rows = np.mean([f['quality_info'].get('row_count', 0) for f in sample_files_info])
            avg_cols = np.mean([f['quality_info'].get('column_count', 0) for f in sample_files_info])
            avg_missing = np.mean([f['quality_info'].get('missing_rate', 0) for f in sample_files_info])
            
            api_info['quality_metrics'] = {
                'avg_rows_per_file': int(avg_rows),
                'avg_columns_per_file': int(avg_cols),
                'avg_missing_rate': round(avg_missing, 2),
                'estimated_total_records': int(avg_rows * len(csv_files))
            }
        
        api_info['sample_files'] = sample_files_info
        api_info['status'] = 'analyzed'
        
        return api_info
    
    def estimate_date_range(self, csv_files, pattern):
        """æ ¹æ®æ–‡ä»¶åä¼°ç®—æ—¥æœŸèŒƒå›´"""
        if pattern == 'snapshot':
            return {'type': 'snapshot', 'description': 'å½“å‰æ—¶ç‚¹å¿«ç…§æ•°æ®'}
        
        years = []
        for file in csv_files:
            filename = file.stem
            year_match = re.search(r'(\d{4})', filename)
            if year_match:
                years.append(int(year_match.group(1)))
        
        if years:
            min_year, max_year = min(years), max(years)
            if pattern == 'quarterly':
                return {
                    'type': 'quarterly',
                    'start_year': min_year,
                    'end_year': max_year,
                    'description': f'{min_year}å¹´-{max_year}å¹´æŒ‰å­£åº¦æ•°æ®'
                }
            elif pattern == 'yearly':
                return {
                    'type': 'yearly', 
                    'start_year': min_year,
                    'end_year': max_year,
                    'description': f'{min_year}å¹´-{max_year}å¹´æŒ‰å¹´åº¦æ•°æ®'
                }
        
        return {'type': 'unknown', 'description': 'æ—¥æœŸèŒƒå›´æœªçŸ¥'}
    
    def generate_detailed_report(self):
        """ç”Ÿæˆè¯¦ç»†APIåˆ†ææŠ¥å‘Š"""
        logging.info("ğŸ“Š å¼€å§‹è¯¦ç»†APIåˆ†æ...")
        
        detailed_report = {
            'report_metadata': {
                'generation_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'total_apis': 0,
                'total_categories': 0
            },
            'categories': {}
        }
        
        all_api_details = []
        
        for category_dir in self.base_dir.iterdir():
            if category_dir.is_dir():
                category_name = category_dir.name
                category_apis = []
                
                logging.info(f"ğŸ“‚ åˆ†æåˆ†ç±»: {category_name}")
                
                for api_dir in category_dir.iterdir():
                    if api_dir.is_dir():
                        api_info = self.analyze_single_api(api_dir)
                        category_apis.append(api_info)
                        all_api_details.append({
                            'category': category_name,
                            **api_info
                        })
                        logging.info(f"  âœ… {api_info['api_name']}: {api_info['file_count']} æ–‡ä»¶")
                
                detailed_report['categories'][category_name] = {
                    'api_count': len(category_apis),
                    'apis': category_apis
                }
        
        detailed_report['report_metadata']['total_apis'] = len(all_api_details)
        detailed_report['report_metadata']['total_categories'] = len(detailed_report['categories'])
        
        # ç”Ÿæˆæ§åˆ¶å°æŠ¥å‘Š
        self.print_detailed_report(detailed_report)
        
        # ç”ŸæˆCSVè¯¦ç»†æŠ¥å‘Š
        self.create_detailed_csv_reports(all_api_details)
        
        return detailed_report
    
    def print_detailed_report(self, report):
        """æ‰“å°è¯¦ç»†æŠ¥å‘Š"""
        print("\n" + "="*120)
        print("ğŸ” **QuantTrade APIè¯¦ç»†åˆ†ææŠ¥å‘Š** ğŸ”")
        print("="*120)
        print(f"ğŸ“… ç”Ÿæˆæ—¶é—´: {report['report_metadata']['generation_time']}")
        print(f"ğŸ“Š åˆ†æèŒƒå›´: {report['report_metadata']['total_categories']} ä¸ªåˆ†ç±», {report['report_metadata']['total_apis']} ä¸ªAPI")
        
        for category_name, category_data in report['categories'].items():
            print(f"\n" + "â”€"*100)
            print(f"ğŸ“ **{category_name.upper()}** ({category_data['api_count']} APIs)")
            print("â”€"*100)
            
            for api in category_data['apis']:
                if api['status'] == 'no_data':
                    print(f"\nğŸ”´ {api['api_name']}")
                    print(f"   ğŸ“ æè¿°: {api['chinese_description']}")
                    print(f"   âš ï¸ çŠ¶æ€: æ— æ•°æ®æ–‡ä»¶")
                    continue
                
                print(f"\nğŸŸ¢ **{api['api_name']}**")
                print(f"   ğŸ“ æè¿°: {api['chinese_description']}")
                print(f"   ğŸ“Š æ•°æ®è§„æ¨¡: {api['file_count']} æ–‡ä»¶, {api['total_size_mb']:.1f}MB")
                
                # æ—¶é—´è¦†ç›–ä¿¡æ¯
                time_info = api['time_coverage']
                print(f"   ğŸ“… æ—¶é—´è¦†ç›–: {time_info['estimated_date_range'].get('description', 'æœªçŸ¥')}")
                
                # è´¨é‡æŒ‡æ ‡
                if api['quality_metrics']:
                    metrics = api['quality_metrics']
                    print(f"   ğŸ¯ æ•°æ®è´¨é‡: å¹³å‡{metrics['avg_rows_per_file']:,}è¡Œ/æ–‡ä»¶, "
                          f"{metrics['avg_columns_per_file']}åˆ—, "
                          f"ç¼ºå¤±ç‡{metrics['avg_missing_rate']}%")
                    print(f"   ğŸ“ˆ æ€»è®°å½•æ•°: çº¦{metrics['estimated_total_records']:,}æ¡")
                
                # å…³é”®åˆ—ä¿¡æ¯
                if api['sample_files']:
                    sample_file = api['sample_files'][0]
                    key_columns = sample_file['quality_info'].get('key_columns', [])
                    if key_columns:
                        print(f"   ğŸ”‘ å…³é”®å­—æ®µ: {', '.join(key_columns[:5])}")
                    
                    # æ•°æ®æ ·æœ¬
                    data_sample = sample_file['quality_info'].get('data_sample', {})
                    if data_sample:
                        print(f"   ğŸ“‹ æ•°æ®æ ·æœ¬:")
                        for col, values in list(data_sample.items())[:3]:
                            print(f"      â€¢ {col}: {', '.join(values)}")
    
    def create_detailed_csv_reports(self, all_api_details):
        """åˆ›å»ºè¯¦ç»†çš„CSVæŠ¥å‘Š"""
        
        # 1. APIæ¦‚è§ˆæŠ¥å‘Š
        api_overview = []
        for api in all_api_details:
            time_info = api.get('time_coverage', {})
            quality_info = api.get('quality_metrics', {})
            
            api_overview.append({
                'category': api['category'],
                'api_name': api['api_name'],
                'chinese_description': api['chinese_description'],
                'file_count': api['file_count'],
                'total_size_mb': api['total_size_mb'],
                'time_pattern': time_info.get('pattern', 'unknown'),
                'date_range_description': time_info.get('estimated_date_range', {}).get('description', ''),
                'avg_rows_per_file': quality_info.get('avg_rows_per_file', 0),
                'avg_columns_per_file': quality_info.get('avg_columns_per_file', 0),
                'avg_missing_rate': quality_info.get('avg_missing_rate', 0),
                'estimated_total_records': quality_info.get('estimated_total_records', 0),
                'status': api.get('status', 'unknown')
            })
        
        df_overview = pd.DataFrame(api_overview)
        df_overview.to_csv('APIè¯¦ç»†åˆ†ææŠ¥å‘Š_æ¦‚è§ˆ.csv', index=False, encoding='utf-8-sig')
        logging.info("âœ… ç”ŸæˆAPIæ¦‚è§ˆæŠ¥å‘Š: APIè¯¦ç»†åˆ†ææŠ¥å‘Š_æ¦‚è§ˆ.csv")
        
        # 2. æ•°æ®è´¨é‡è¯¦ç»†æŠ¥å‘Š
        quality_details = []
        for api in all_api_details:
            if api.get('sample_files'):
                for sample_file in api['sample_files']:
                    quality_info = sample_file['quality_info']
                    quality_details.append({
                        'category': api['category'],
                        'api_name': api['api_name'],
                        'filename': sample_file['filename'],
                        'file_size_mb': sample_file['size_mb'],
                        'row_count': quality_info.get('row_count', 0),
                        'column_count': quality_info.get('column_count', 0),
                        'missing_rate': quality_info.get('missing_rate', 0),
                        'duplicate_rate': quality_info.get('duplicate_rate', 0),
                        'key_columns': ', '.join(quality_info.get('key_columns', [])),
                        'data_types': str(quality_info.get('data_types', {}))
                    })
        
        if quality_details:
            df_quality = pd.DataFrame(quality_details)
            df_quality.to_csv('APIè¯¦ç»†åˆ†ææŠ¥å‘Š_è´¨é‡.csv', index=False, encoding='utf-8-sig')
            logging.info("âœ… ç”Ÿæˆè´¨é‡è¯¦ç»†æŠ¥å‘Š: APIè¯¦ç»†åˆ†ææŠ¥å‘Š_è´¨é‡.csv")
        
        # 3. æŒ‰åˆ†ç±»æ±‡æ€»æŠ¥å‘Š
        category_summary = []
        categories = {}
        
        for api in all_api_details:
            cat = api['category']
            if cat not in categories:
                categories[cat] = {
                    'api_count': 0,
                    'total_files': 0,
                    'total_size_mb': 0,
                    'total_records': 0,
                    'apis_with_data': 0
                }
            
            categories[cat]['api_count'] += 1
            categories[cat]['total_files'] += api['file_count']
            categories[cat]['total_size_mb'] += api['total_size_mb']
            
            if api.get('quality_metrics'):
                categories[cat]['total_records'] += api['quality_metrics'].get('estimated_total_records', 0)
            
            if api['file_count'] > 0:
                categories[cat]['apis_with_data'] += 1
        
        for cat_name, cat_data in categories.items():
            category_summary.append({
                'category': cat_name,
                'api_count': cat_data['api_count'],
                'apis_with_data': cat_data['apis_with_data'],
                'data_coverage_rate': (cat_data['apis_with_data'] / cat_data['api_count'] * 100) if cat_data['api_count'] > 0 else 0,
                'total_files': cat_data['total_files'],
                'total_size_mb': cat_data['total_size_mb'],
                'estimated_total_records': cat_data['total_records']
            })
        
        df_category = pd.DataFrame(category_summary)
        df_category.to_csv('APIè¯¦ç»†åˆ†ææŠ¥å‘Š_åˆ†ç±»æ±‡æ€».csv', index=False, encoding='utf-8-sig')
        logging.info("âœ… ç”Ÿæˆåˆ†ç±»æ±‡æ€»æŠ¥å‘Š: APIè¯¦ç»†åˆ†ææŠ¥å‘Š_åˆ†ç±»æ±‡æ€».csv")

if __name__ == "__main__":
    analyzer = DetailedAPIAnalyzer()
    detailed_report = analyzer.generate_detailed_report()