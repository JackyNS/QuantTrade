#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¯¦ç»†æ•°æ®åˆ†æå·¥å…·
==============

æ·±å…¥åˆ†ææœ¬åœ°æ•°æ®çš„å®é™…è¦†ç›–æƒ…å†µå’Œè´¨é‡
"""

import os
import sys
from pathlib import Path
from datetime import datetime
import pandas as pd
import numpy as np
from collections import defaultdict, Counter
import json

def analyze_all_data_files():
    """åˆ†ææ‰€æœ‰æ•°æ®æ–‡ä»¶"""
    print("ğŸ” è¯¦ç»†åˆ†ææœ¬åœ°æ•°æ®è¦†ç›–æƒ…å†µ...")
    print("=" * 60)
    
    data_dir = Path('./data')
    if not data_dir.exists():
        print("âŒ dataç›®å½•ä¸å­˜åœ¨")
        return {}
    
    analysis_results = {
        'directories': {},
        'total_symbols': set(),
        'file_count_by_type': defaultdict(int),
        'size_statistics': {},
        'date_coverage': {},
        'data_quality': {}
    }
    
    # éå†æ‰€æœ‰å­ç›®å½•
    for sub_dir in data_dir.iterdir():
        if sub_dir.is_dir():
            print(f"\nğŸ“‚ åˆ†æç›®å½•: {sub_dir.name}")
            dir_analysis = analyze_directory_detailed(sub_dir)
            analysis_results['directories'][sub_dir.name] = dir_analysis
            analysis_results['total_symbols'].update(dir_analysis['unique_symbols'])
    
    # è½¬æ¢setä¸ºlistä»¥ä¾¿åºåˆ—åŒ–
    analysis_results['total_symbols'] = sorted(list(analysis_results['total_symbols']))
    
    return analysis_results

def analyze_directory_detailed(directory):
    """è¯¦ç»†åˆ†æå•ä¸ªç›®å½•"""
    dir_path = Path(directory)
    
    # é€’å½’æŸ¥æ‰¾æ‰€æœ‰æ•°æ®æ–‡ä»¶
    csv_files = list(dir_path.rglob('*.csv'))
    parquet_files = list(dir_path.rglob('*.parquet'))
    all_files = csv_files + parquet_files
    
    print(f"   ğŸ“Š æ‰¾åˆ° {len(all_files)} ä¸ªæ•°æ®æ–‡ä»¶")
    
    if not all_files:
        return {
            'file_count': 0,
            'unique_symbols': set(),
            'file_sizes': [],
            'subdirectories': {},
            'date_ranges': {},
            'data_samples': []
        }
    
    # æŒ‰å­ç›®å½•åˆ†ç»„
    files_by_subdir = defaultdict(list)
    for file_path in all_files:
        relative_path = file_path.relative_to(dir_path)
        if len(relative_path.parts) > 1:
            subdir = relative_path.parts[0]
        else:
            subdir = '.'
        files_by_subdir[subdir].append(file_path)
    
    dir_analysis = {
        'file_count': len(all_files),
        'unique_symbols': set(),
        'file_sizes': [],
        'subdirectories': {},
        'date_ranges': {},
        'data_samples': []
    }
    
    # åˆ†ææ¯ä¸ªå­ç›®å½•
    for subdir, files in files_by_subdir.items():
        print(f"      ğŸ“ {subdir}: {len(files)} ä¸ªæ–‡ä»¶")
        subdir_analysis = analyze_files_in_subdir(files, subdir)
        dir_analysis['subdirectories'][subdir] = subdir_analysis
        dir_analysis['unique_symbols'].update(subdir_analysis['symbols'])
        dir_analysis['file_sizes'].extend(subdir_analysis['file_sizes'])
    
    return dir_analysis

def analyze_files_in_subdir(files, subdir_name):
    """åˆ†æå­ç›®å½•ä¸­çš„æ–‡ä»¶"""
    symbols = set()
    file_sizes = []
    date_ranges = {}
    sample_data = []
    
    # é™åˆ¶åˆ†æçš„æ–‡ä»¶æ•°é‡ï¼Œé¿å…å¤ªæ…¢
    files_to_analyze = files[:100] if len(files) > 100 else files
    
    for i, file_path in enumerate(files_to_analyze):
        try:
            # åŸºç¡€æ–‡ä»¶ä¿¡æ¯
            file_size = file_path.stat().st_size / (1024 * 1024)  # MB
            file_sizes.append(file_size)
            
            # ä»æ–‡ä»¶åæå–è‚¡ç¥¨ä»£ç 
            symbol = extract_symbol_from_filename(file_path.name)
            if symbol:
                symbols.add(symbol)
            
            # æ¯10ä¸ªæ–‡ä»¶è¯»å–ä¸€ä¸ªæ ·æœ¬åˆ†ææ•°æ®å†…å®¹
            if i % 10 == 0:
                try:
                    file_info = analyze_single_file(file_path)
                    if file_info:
                        sample_data.append(file_info)
                        if file_info.get('date_range'):
                            date_ranges[symbol] = file_info['date_range']
                except Exception as e:
                    print(f"         âš ï¸ è¯»å–æ–‡ä»¶å¤±è´¥ {file_path.name}: {e}")
        
        except Exception as e:
            print(f"         âš ï¸ åˆ†ææ–‡ä»¶å¤±è´¥ {file_path.name}: {e}")
    
    if len(files) > len(files_to_analyze):
        print(f"         ğŸ“‹ åˆ†æäº†å‰{len(files_to_analyze)}ä¸ªæ–‡ä»¶ï¼Œå‰©ä½™{len(files) - len(files_to_analyze)}ä¸ªæ–‡ä»¶")
    
    return {
        'file_count': len(files),
        'symbols': symbols,
        'unique_symbol_count': len(symbols),
        'file_sizes': file_sizes,
        'avg_file_size_mb': np.mean(file_sizes) if file_sizes else 0,
        'total_size_mb': sum(file_sizes),
        'date_ranges': date_ranges,
        'sample_data': sample_data[:5]  # åªä¿ç•™å‰5ä¸ªæ ·æœ¬
    }

def extract_symbol_from_filename(filename):
    """ä»æ–‡ä»¶åæå–è‚¡ç¥¨ä»£ç """
    # ç§»é™¤æ‰©å±•å
    name = filename.replace('.csv', '').replace('.parquet', '')
    
    # å¸¸è§çš„è‚¡ç¥¨ä»£ç æ¨¡å¼
    import re
    
    # 6ä½æ•°å­—ä»£ç  (å¦‚ 000001, 600000)
    match = re.search(r'\b(\d{6})\b', name)
    if match:
        code = match.group(1)
        # æ ¹æ®ä»£ç åˆ¤æ–­äº¤æ˜“æ‰€
        if code.startswith('0') or code.startswith('3'):
            return f"{code}.SZ"
        elif code.startswith('6') or code.startswith('9'):
            return f"{code}.SH"
        else:
            return code
    
    # å·²ç»å¸¦æœ‰åç¼€çš„ä»£ç  (å¦‚ 000001.SZ)
    match = re.search(r'\b(\d{6}\.(SZ|SH|XSHE|XSHG))\b', name)
    if match:
        return match.group(1)
    
    # å…¶ä»–æ¨¡å¼
    if name.isdigit() and len(name) == 6:
        return name
    
    return name

def analyze_single_file(file_path):
    """åˆ†æå•ä¸ªæ–‡ä»¶çš„è¯¦ç»†å†…å®¹"""
    try:
        if file_path.suffix == '.csv':
            # å…ˆè¯»å–å°‘é‡è¡Œæ•°ç¡®å®šç»“æ„
            df_sample = pd.read_csv(file_path, nrows=100)
        elif file_path.suffix == '.parquet':
            df_sample = pd.read_parquet(file_path)
            if len(df_sample) > 100:
                df_sample = df_sample.head(100)
        else:
            return None
        
        if df_sample.empty:
            return None
        
        file_info = {
            'filename': file_path.name,
            'rows_sampled': len(df_sample),
            'columns': list(df_sample.columns),
            'column_count': len(df_sample.columns),
            'data_types': df_sample.dtypes.astype(str).to_dict(),
            'missing_data': df_sample.isnull().sum().to_dict(),
            'date_range': None
        }
        
        # æŸ¥æ‰¾æ—¥æœŸåˆ—å¹¶åˆ†ææ—¥æœŸèŒƒå›´
        date_columns = [col for col in df_sample.columns 
                       if 'date' in col.lower() or 'time' in col.lower()]
        
        if date_columns:
            date_col = date_columns[0]
            try:
                dates = pd.to_datetime(df_sample[date_col], errors='coerce')
                valid_dates = dates.dropna()
                if not valid_dates.empty:
                    file_info['date_range'] = {
                        'start': valid_dates.min().strftime('%Y-%m-%d'),
                        'end': valid_dates.max().strftime('%Y-%m-%d'),
                        'total_days': (valid_dates.max() - valid_dates.min()).days + 1
                    }
            except:
                pass
        
        return file_info
    
    except Exception as e:
        return {'filename': file_path.name, 'error': str(e)}

def get_comprehensive_statistics(analysis_results):
    """è·å–ç»¼åˆç»Ÿè®¡ä¿¡æ¯"""
    print("\n" + "=" * 60)
    print("ğŸ“Š ç»¼åˆæ•°æ®ç»Ÿè®¡")
    print("=" * 60)
    
    total_symbols = len(analysis_results['total_symbols'])
    total_files = sum(dir_info['file_count'] for dir_info in analysis_results['directories'].values())
    
    print(f"ğŸ“ æ€»æ•°æ®æ–‡ä»¶: {total_files:,} ä¸ª")
    print(f"ğŸ“ˆ æ€»è‚¡ç¥¨æ•°é‡: {total_symbols:,} åª")
    
    # æŒ‰ç›®å½•æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
    for dir_name, dir_info in analysis_results['directories'].items():
        print(f"\nğŸ“‚ {dir_name}:")
        print(f"   æ–‡ä»¶æ•°: {dir_info['file_count']:,}")
        print(f"   è‚¡ç¥¨æ•°: {len(dir_info['unique_symbols'])}")
        
        if dir_info['subdirectories']:
            for subdir, subdir_info in dir_info['subdirectories'].items():
                if subdir != '.':
                    print(f"   ğŸ“ {subdir}:")
                    print(f"      æ–‡ä»¶: {subdir_info['file_count']:,}")
                    print(f"      è‚¡ç¥¨: {subdir_info['unique_symbol_count']}")
                    print(f"      å¤§å°: {subdir_info['total_size_mb']:.1f} MB")
    
    # æ˜¾ç¤ºè‚¡ç¥¨ä»£ç æ ·æœ¬
    if analysis_results['total_symbols']:
        print(f"\nğŸ“‹ è‚¡ç¥¨ä»£ç æ ·æœ¬ (å‰20ä¸ª):")
        for i, symbol in enumerate(analysis_results['total_symbols'][:20]):
            print(f"   {symbol}")
        
        if len(analysis_results['total_symbols']) > 20:
            print(f"   ... è¿˜æœ‰ {len(analysis_results['total_symbols']) - 20} ä¸ªè‚¡ç¥¨ä»£ç ")
    
    return {
        'total_files': total_files,
        'total_symbols': total_symbols,
        'directories_analyzed': len(analysis_results['directories'])
    }

def save_detailed_analysis(analysis_results, statistics):
    """ä¿å­˜è¯¦ç»†åˆ†æç»“æœ"""
    print("\nğŸ“ ä¿å­˜è¯¦ç»†åˆ†æç»“æœ...")
    
    # åˆ›å»ºæŠ¥å‘Šç›®å½•
    reports_dir = Path('reports')
    reports_dir.mkdir(exist_ok=True)
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # ä¿å­˜å®Œæ•´åˆ†æç»“æœ (JSON)
    json_file = reports_dir / f'detailed_data_analysis_{timestamp}.json'
    
    # è½¬æ¢setä¸ºlistä»¥ä¾¿JSONåºåˆ—åŒ–
    serializable_results = convert_sets_to_lists(analysis_results)
    
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(serializable_results, f, ensure_ascii=False, indent=2, default=str)
    
    # ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š
    text_report = generate_text_report(analysis_results, statistics)
    text_file = reports_dir / f'detailed_data_analysis_{timestamp}.txt'
    
    with open(text_file, 'w', encoding='utf-8') as f:
        f.write(text_report)
    
    print(f"âœ… JSONæŠ¥å‘Š: {json_file}")
    print(f"âœ… æ–‡æœ¬æŠ¥å‘Š: {text_file}")
    
    return json_file, text_file

def convert_sets_to_lists(obj):
    """é€’å½’è½¬æ¢setä¸ºlistä»¥ä¾¿JSONåºåˆ—åŒ–"""
    if isinstance(obj, set):
        return sorted(list(obj))
    elif isinstance(obj, dict):
        return {key: convert_sets_to_lists(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_sets_to_lists(item) for item in obj]
    else:
        return obj

def generate_text_report(analysis_results, statistics):
    """ç”Ÿæˆæ–‡æœ¬æ ¼å¼çš„è¯¦ç»†æŠ¥å‘Š"""
    lines = []
    lines.append("=" * 80)
    lines.append("ğŸ“Š æœ¬åœ°æ•°æ®è¯¦ç»†åˆ†ææŠ¥å‘Š")
    lines.append("=" * 80)
    lines.append(f"ğŸ“… åˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    lines.append("")
    
    # æ¦‚è§ˆç»Ÿè®¡
    lines.append("ğŸ¯ æ•°æ®æ¦‚è§ˆ:")
    lines.append(f"   ğŸ“ æ€»æ–‡ä»¶æ•°: {statistics['total_files']:,}")
    lines.append(f"   ğŸ“ˆ æ€»è‚¡ç¥¨æ•°: {statistics['total_symbols']:,}")
    lines.append(f"   ğŸ“‚ ç›®å½•æ•°: {statistics['directories_analyzed']}")
    lines.append("")
    
    # ç›®å½•è¯¦æƒ…
    lines.append("ğŸ“‚ ç›®å½•è¯¦æƒ…:")
    for dir_name, dir_info in analysis_results['directories'].items():
        lines.append(f"   {dir_name}:")
        lines.append(f"      ğŸ“Š æ–‡ä»¶æ•°: {dir_info['file_count']:,}")
        lines.append(f"      ğŸ“ˆ è‚¡ç¥¨æ•°: {len(dir_info['unique_symbols'])}")
        
        if dir_info['subdirectories']:
            for subdir, subdir_info in dir_info['subdirectories'].items():
                if subdir != '.':
                    lines.append(f"      ğŸ“ {subdir}:")
                    lines.append(f"         æ–‡ä»¶: {subdir_info['file_count']:,}")
                    lines.append(f"         è‚¡ç¥¨: {subdir_info['unique_symbol_count']}")
                    lines.append(f"         å¤§å°: {subdir_info['total_size_mb']:.1f} MB")
        lines.append("")
    
    # è‚¡ç¥¨ä»£ç åˆ—è¡¨
    lines.append("ğŸ“‹ å®Œæ•´è‚¡ç¥¨åˆ—è¡¨:")
    symbols = analysis_results['total_symbols']
    if symbols:
        # æŒ‰äº¤æ˜“æ‰€åˆ†ç»„
        sz_symbols = [s for s in symbols if '.SZ' in s or (s.isdigit() and len(s) == 6 and (s.startswith('0') or s.startswith('3')))]
        sh_symbols = [s for s in symbols if '.SH' in s or (s.isdigit() and len(s) == 6 and s.startswith('6'))]
        other_symbols = [s for s in symbols if s not in sz_symbols and s not in sh_symbols]
        
        if sz_symbols:
            lines.append(f"   æ·±å¸‚ ({len(sz_symbols)}åª):")
            for i in range(0, len(sz_symbols), 10):
                batch = sz_symbols[i:i+10]
                lines.append(f"      {' '.join(batch)}")
        
        if sh_symbols:
            lines.append(f"   æ²ªå¸‚ ({len(sh_symbols)}åª):")
            for i in range(0, len(sh_symbols), 10):
                batch = sh_symbols[i:i+10]
                lines.append(f"      {' '.join(batch)}")
        
        if other_symbols:
            lines.append(f"   å…¶ä»– ({len(other_symbols)}åª):")
            for i in range(0, len(other_symbols), 10):
                batch = other_symbols[i:i+10]
                lines.append(f"      {' '.join(batch)}")
    
    lines.append("")
    lines.append("=" * 80)
    
    return "\n".join(lines)

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” æœ¬åœ°æ•°æ®è¯¦ç»†åˆ†æå·¥å…·")
    print("=" * 60)
    
    # æ‰§è¡Œè¯¦ç»†åˆ†æ
    analysis_results = analyze_all_data_files()
    
    # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
    statistics = get_comprehensive_statistics(analysis_results)
    
    # ä¿å­˜åˆ†æç»“æœ
    json_file, text_file = save_detailed_analysis(analysis_results, statistics)
    
    print("\n" + "=" * 60)
    print("ğŸ‰ è¯¦ç»†æ•°æ®åˆ†æå®Œæˆï¼")
    print("=" * 60)
    print(f"ğŸ“Š å‘ç° {statistics['total_symbols']} åªè‚¡ç¥¨çš„æ•°æ®")
    print(f"ğŸ“ æ€»å…± {statistics['total_files']} ä¸ªæ•°æ®æ–‡ä»¶")
    print(f"ğŸ“ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ° reports/ ç›®å½•")
    
    return analysis_results, statistics

if __name__ == "__main__":
    main()