#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é«˜æ•ˆçš„è‚¡ç¥¨æ•°æ®å®Œæ•´æ€§æ£€æŸ¥å™¨
å¿«é€Ÿç¡®å®šæ¯åªAè‚¡åœ¨2000-2025å¹´8æœˆçš„æ•°æ®å®Œæ•´æ€§
"""

import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
import warnings
from collections import defaultdict
warnings.filterwarnings('ignore')

class EfficientCompletenessChecker:
    """é«˜æ•ˆçš„å®Œæ•´æ€§æ£€æŸ¥å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ£€æŸ¥å™¨"""
        self.base_path = Path("/Users/jackstudio/QuantTrade/data")
        self.target_start = pd.Timestamp('2000-01-01')
        self.target_end = pd.Timestamp('2025-08-31')
    
    def quick_check_using_csv_complete(self):
        """ä½¿ç”¨csv_completeç›®å½•å¿«é€Ÿæ£€æŸ¥"""
        print("âš¡ ä½¿ç”¨ä¸ªè‚¡æ–‡ä»¶å¿«é€Ÿæ£€æŸ¥æ•°æ®å®Œæ•´æ€§...")
        
        csv_daily_path = self.base_path / "csv_complete/daily"
        
        if not csv_daily_path.exists():
            print("âŒ csv_complete/dailyç›®å½•ä¸å­˜åœ¨")
            return None
        
        # è·å–æ‰€æœ‰ä¸ªè‚¡æ–‡ä»¶
        stock_files = list(csv_daily_path.rglob("*.csv"))
        print(f"   ğŸ“Š æ‰¾åˆ°ä¸ªè‚¡æ–‡ä»¶: {len(stock_files)} ä¸ª")
        
        # éšæœºæŠ½æ ·æ£€æŸ¥
        sample_size = min(100, len(stock_files))
        sample_files = stock_files[:sample_size]
        
        print(f"   ğŸ” æŠ½æ ·æ£€æŸ¥: {sample_size} ä¸ªæ–‡ä»¶")
        print("=" * 60)
        
        results = {
            'complete_stocks': [],
            'incomplete_stocks': [],
            'error_stocks': []
        }
        
        for i, file_path in enumerate(sample_files, 1):
            try:
                # ä»æ–‡ä»¶åæå–è‚¡ç¥¨ä»£ç 
                stock_id = file_path.stem.replace('_', '.')
                
                # è¯»å–æ–‡ä»¶å¹¶æ£€æŸ¥æ—¶é—´èŒƒå›´
                df = pd.read_csv(file_path)
                
                if len(df) == 0:
                    results['error_stocks'].append(f"{stock_id}: ç©ºæ–‡ä»¶")
                    continue
                
                if 'tradeDate' not in df.columns:
                    results['error_stocks'].append(f"{stock_id}: æ— tradeDateåˆ—")
                    continue
                
                df['tradeDate'] = pd.to_datetime(df['tradeDate'])
                min_date = df['tradeDate'].min()
                max_date = df['tradeDate'].max()
                
                # æ£€æŸ¥æ—¶é—´è¦†ç›–
                start_ok = min_date <= self.target_start + pd.Timedelta(days=365)
                end_ok = max_date >= self.target_end - pd.Timedelta(days=30)
                
                if start_ok and end_ok:
                    results['complete_stocks'].append({
                        'stock_id': stock_id,
                        'start': min_date.strftime('%Y-%m-%d'),
                        'end': max_date.strftime('%Y-%m-%d'),
                        'records': len(df)
                    })
                    status = "âœ… å®Œæ•´"
                else:
                    results['incomplete_stocks'].append({
                        'stock_id': stock_id,
                        'start': min_date.strftime('%Y-%m-%d'),
                        'end': max_date.strftime('%Y-%m-%d'),
                        'records': len(df),
                        'issue': f"å¼€å§‹{'âœ“' if start_ok else 'âœ—'} ç»“æŸ{'âœ“' if end_ok else 'âœ—'}"
                    })
                    status = "âš ï¸ ä¸å®Œæ•´"
                
                if i % 20 == 0 or i <= 10:
                    print(f"[{i:2}/{sample_size}] {stock_id}: {status} ({min_date.date()} - {max_date.date()})")
                
            except Exception as e:
                results['error_stocks'].append(f"{file_path.stem}: è¯»å–é”™è¯¯")
                
        return results
    
    def check_batch_files_coverage(self):
        """æ£€æŸ¥æ‰¹æ¬¡æ–‡ä»¶çš„æ•´ä½“è¦†ç›–æƒ…å†µ"""
        print("\\nğŸ“„ æ£€æŸ¥æ‰¹æ¬¡æ–‡ä»¶æ•´ä½“è¦†ç›–...")
        
        daily_path = self.base_path / "priority_download/market_data/daily"
        batch_files = list(daily_path.glob("*.csv"))
        
        print(f"   ğŸ“Š æ‰¹æ¬¡æ–‡ä»¶æ•°: {len(batch_files)}")
        
        # æ£€æŸ¥æ—¶é—´è·¨åº¦
        year_files = defaultdict(list)
        for file_path in batch_files:
            try:
                year = file_path.stem.split('_')[0]
                year_files[year].append(file_path)
            except:
                continue
        
        print(f"   ğŸ“… å¹´ä»½è¦†ç›–: {min(year_files.keys())} - {max(year_files.keys())}")
        
        # æ£€æŸ¥è‚¡ç¥¨æ•°é‡åˆ†å¸ƒ
        sample_years = ['2000', '2010', '2020', '2025']
        
        for year in sample_years:
            if year in year_files:
                sample_file = year_files[year][0]
                try:
                    df = pd.read_csv(sample_file)
                    unique_stocks = len(df['secID'].unique()) if 'secID' in df.columns else 0
                    print(f"   ğŸ“ˆ {year}å¹´æ ·æœ¬: {unique_stocks} åªè‚¡ç¥¨/æ–‡ä»¶")
                except:
                    print(f"   âŒ {year}å¹´æ ·æœ¬: è¯»å–å¤±è´¥")
        
        return {
            'total_batch_files': len(batch_files),
            'year_range': f"{min(year_files.keys())} - {max(year_files.keys())}",
            'years_covered': len(year_files)
        }
    
    def generate_summary_report(self):
        """ç”Ÿæˆæ€»ç»“æŠ¥å‘Š"""
        print("ğŸ” Aè‚¡æ•°æ®å®Œæ•´æ€§å¿«é€Ÿæ£€æŸ¥")
        print("ğŸ¯ ç›®æ ‡: 2000å¹´1æœˆ1æ—¥ - 2025å¹´8æœˆ31æ—¥")
        print("=" * 80)
        
        # æ£€æŸ¥ä¸ªè‚¡æ–‡ä»¶
        csv_results = self.quick_check_using_csv_complete()
        
        if csv_results:
            total_sample = len(csv_results['complete_stocks']) + len(csv_results['incomplete_stocks'])
            complete_count = len(csv_results['complete_stocks'])
            incomplete_count = len(csv_results['incomplete_stocks'])
            error_count = len(csv_results['error_stocks'])
            
            coverage_rate = (complete_count / total_sample * 100) if total_sample > 0 else 0
            
            print("\\nğŸ“Š ä¸ªè‚¡æ–‡ä»¶æ£€æŸ¥ç»“æœ:")
            print(f"   ğŸ“ˆ æ€»è‚¡ç¥¨æ–‡ä»¶: 50,658 ä¸ª")
            print(f"   ğŸ” æŠ½æ ·æ£€æŸ¥: {total_sample} ä¸ª")
            print(f"   âœ… å®Œæ•´è¦†ç›–: {complete_count} ({coverage_rate:.1f}%)")
            print(f"   âš ï¸ ä¸å®Œæ•´: {incomplete_count}")
            print(f"   âŒ é”™è¯¯æ–‡ä»¶: {error_count}")
            
            # æ˜¾ç¤ºå®Œæ•´æ•°æ®æ ·æœ¬
            if csv_results['complete_stocks']:
                print("\\nâœ… å®Œæ•´æ•°æ®æ ·æœ¬:")
                for stock in csv_results['complete_stocks'][:5]:
                    print(f"      {stock['stock_id']}: {stock['start']} - {stock['end']} ({stock['records']} æ¡)")
            
            # æ˜¾ç¤ºä¸å®Œæ•´æ•°æ®æ ·æœ¬
            if csv_results['incomplete_stocks']:
                print("\\nâš ï¸ ä¸å®Œæ•´æ•°æ®æ ·æœ¬:")
                for stock in csv_results['incomplete_stocks'][:5]:
                    print(f"      {stock['stock_id']}: {stock['start']} - {stock['end']} ({stock['issue']})")
        
        # æ£€æŸ¥æ‰¹æ¬¡æ–‡ä»¶
        batch_info = self.check_batch_files_coverage()
        
        print("\\nğŸ“„ æ‰¹æ¬¡æ–‡ä»¶æ£€æŸ¥ç»“æœ:")
        print(f"   ğŸ“Š æ‰¹æ¬¡æ–‡ä»¶: {batch_info['total_batch_files']} ä¸ª")
        print(f"   ğŸ“… å¹´ä»½è·¨åº¦: {batch_info['year_range']}")
        print(f"   ğŸ—“ï¸ è¦†ç›–å¹´æ•°: {batch_info['years_covered']} å¹´")
        
        # ç”Ÿæˆæœ€ç»ˆç»“è®º
        print("\\nğŸ’¡ å…³é”®ç»“è®º:")
        if csv_results:
            if coverage_rate >= 90:
                print("   ğŸ¯ æ•°æ®è´¨é‡ä¼˜ç§€: 90%+ çš„è‚¡ç¥¨æœ‰å®Œæ•´çš„2000-2025æ•°æ®")
                print("   âœ… å¯ä»¥æ”¾å¿ƒåŸºäºç°æœ‰æ•°æ®è¿›è¡Œåˆ†æ")
                recommendation = "æ•°æ®å®Œæ•´æ€§ä¼˜ç§€ï¼Œå¯ç›´æ¥ä½¿ç”¨"
            elif coverage_rate >= 70:
                print("   ğŸŸ¡ æ•°æ®è´¨é‡è‰¯å¥½: 70%+ çš„è‚¡ç¥¨æœ‰å®Œæ•´æ•°æ®")
                print("   ğŸ’¡ å»ºè®®ä¼˜å…ˆä½¿ç”¨å®Œæ•´æ•°æ®è‚¡ç¥¨")
                recommendation = "æ•°æ®è´¨é‡è‰¯å¥½ï¼Œå»ºè®®ç­›é€‰ä½¿ç”¨"
            else:
                print("   ğŸ”´ æ•°æ®å®Œæ•´æ€§æœ‰é™: å°‘äº70% çš„è‚¡ç¥¨æœ‰å®Œæ•´æ•°æ®")
                print("   âš ï¸ éœ€è¦è°¨æ…ä½¿ç”¨æˆ–è¡¥å……æ•°æ®")
                recommendation = "æ•°æ®éœ€è¦æ”¹è¿›"
        else:
            print("   âŒ æ— æ³•æ£€æŸ¥ä¸ªè‚¡æ–‡ä»¶")
            recommendation = "éœ€è¦è¿›ä¸€æ­¥è°ƒæŸ¥"
        
        print(f"   ğŸ“‹ æ¨èè¡ŒåŠ¨: {recommendation}")
        
        # åŸºäº50,658ä¸ªè‚¡ç¥¨æ–‡ä»¶ä¼°ç®—
        if csv_results and coverage_rate > 0:
            estimated_complete_stocks = int(50658 * coverage_rate / 100)
            print(f"\\nğŸ“ˆ åŸºäºæŠ½æ ·ä¼°ç®—:")
            print(f"   ğŸ¯ é¢„è®¡æœ‰å®Œæ•´æ•°æ®çš„è‚¡ç¥¨: ~{estimated_complete_stocks:,} åª")
            print(f"   ğŸ“Š è¿™å·²ç»æ˜¯ä¸€ä¸ªéå¸¸åºå¤§çš„æ•°æ®é›†")

def main():
    """ä¸»å‡½æ•°"""
    checker = EfficientCompletenessChecker()
    checker.generate_summary_report()

if __name__ == "__main__":
    main()