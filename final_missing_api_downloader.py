#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æœ€ç»ˆç‰ˆç¼ºå¤±APIè¡¥å……ä¸‹è½½å™¨
åŸºäºæœ€æ–°ä¼˜çŸ¿æ¥å£æ¸…å•å’Œä½¿ç”¨è¯´æ˜ï¼Œä½¿ç”¨æ­£ç¡®çš„APIè°ƒç”¨æ–¹å¼
å®Œæˆspecial_tradingåˆ†ç±»ä¸­ç¼ºå¤±çš„3ä¸ªAPI:
1. getEquMarginSec - å¯å……æŠµä¿è¯é‡‘è¯åˆ¸
2. getMktRANKInstTr - è¡Œä¸šæˆåˆ†æ¢æ‰‹ç‡æ’å  
3. getFdmtEe - ä¸šç»©å¿«æŠ¥
"""

import uqer
import pandas as pd
from datetime import datetime, timedelta
from pathlib import Path
import time
import logging
import json

UQER_TOKEN = "68b9922817ae6273137bda7acba81e293582ba347281dfcc056dcb245b23faf3"

class FinalMissingAPIDownloader:
    """æœ€ç»ˆç‰ˆç¼ºå¤±APIè¡¥å……ä¸‹è½½å™¨"""
    
    def __init__(self):
        # æ ¹æ®PDFè¯´æ˜ï¼Œæ­£ç¡®çš„åˆå§‹åŒ–æ–¹å¼
        self.client = uqer.Client(token=UQER_TOKEN)
        self.data_dir = Path("data/final_comprehensive_download")
        
        # é…ç½®æ—¥å¿—
        log_file = self.data_dir / "final_missing_apis_download.log"
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )
        
        # åŸºäºuqer.DataAPIæ¢æµ‹çš„æ­£ç¡®APIé…ç½®
        self.missing_apis = {
            "EquMarginSecGet": {
                "desc": "å¯å……æŠµä¿è¯é‡‘è¯åˆ¸",
                "time_range": True,
                "category": "special_trading",
                "dir_name": "equmarginsec"
            },
            "MktRANKInstTrGet": {
                "desc": "è¡Œä¸šæˆåˆ†æ¢æ‰‹ç‡æ’å",
                "time_range": True,
                "category": "special_trading", 
                "dir_name": "mktrankinstr"
            },
            "FdmtEeGet": {
                "desc": "ä¸šç»©å¿«æŠ¥",
                "time_range": True,
                "category": "special_trading",
                "dir_name": "fdmtee"
            }
        }
        
    def download_with_time_range(self, api_name, desc, dir_name, year_start=2000, year_end=2025):
        """æŒ‰å¹´ä»½èŒƒå›´ä¸‹è½½æ•°æ®"""
        category_dir = self.data_dir / "special_trading"
        api_dir = category_dir / dir_name
        api_dir.mkdir(parents=True, exist_ok=True)
        
        total_records = 0
        logging.info(f"ğŸ“¥ å¼€å§‹ä¸‹è½½ {desc}...")
        
        for year in range(year_start, year_end + 1):
            try:
                begin_date = f"{year}-01-01"
                end_date = f"{year}-12-31" if year < 2025 else "2025-09-01"
                
                # æ ¹æ®PDFè¯´æ˜ï¼Œæ­£ç¡®çš„APIè°ƒç”¨æ–¹å¼æ˜¯ uqer.DataAPI.XXX
                if hasattr(uqer.DataAPI, api_name):
                    api_func = getattr(uqer.DataAPI, api_name)
                    
                    # è°ƒç”¨API
                    df = api_func(beginDate=begin_date, endDate=end_date)
                    
                    if not df.empty:
                        # ä¿å­˜æ•°æ®
                        file_path = api_dir / f"year_{year}.csv"
                        df.to_csv(file_path, index=False, encoding='utf-8-sig')
                        logging.info(f"âœ… {desc} {year}å¹´: {len(df)} æ¡è®°å½•")
                        total_records += len(df)
                    else:
                        logging.info(f"âšª {desc} {year}å¹´: æ— æ•°æ®")
                else:
                    logging.error(f"âŒ APIå‡½æ•° {api_name} ä¸å­˜åœ¨")
                    break
                
                # é¿å…è¯·æ±‚è¿‡é¢‘
                time.sleep(1)
                
            except Exception as e:
                if "æ— æ•ˆçš„è¯·æ±‚å‚æ•°" in str(e):
                    logging.warning(f"âš ï¸ {desc} {year}å¹´: APIå‚æ•°æ— æ•ˆï¼Œè·³è¿‡")
                else:
                    logging.error(f"âŒ {desc} {year}å¹´ä¸‹è½½å¤±è´¥: {str(e)}")
                time.sleep(2)
        
        logging.info(f"ğŸ“Š {desc} å®Œæˆ: æ€»è®¡ {total_records} æ¡è®°å½•")
        return total_records
    
    def download_static_data(self, api_name, desc, dir_name):
        """ä¸‹è½½é™æ€æ•°æ®"""
        category_dir = self.data_dir / "special_trading"
        api_dir = category_dir / dir_name
        api_dir.mkdir(parents=True, exist_ok=True)
        
        try:
            logging.info(f"ğŸ“¥ å¼€å§‹ä¸‹è½½ {desc}...")
            
            # æ­£ç¡®çš„APIè°ƒç”¨æ–¹å¼
            if hasattr(uqer.DataAPI, api_name):
                api_func = getattr(uqer.DataAPI, api_name)
                
                # å…ˆå°è¯•æ— å‚æ•°è°ƒç”¨
                try:
                    df = api_func()
                except Exception as e1:
                    # å¦‚æœå¤±è´¥ï¼Œå°è¯•æ·»åŠ æ—¶é—´å‚æ•°
                    try:
                        df = api_func(beginDate="2020-01-01", endDate="2025-09-01")
                    except Exception as e2:
                        logging.error(f"âŒ {desc} ä¸‹è½½å¤±è´¥: {str(e1)} / {str(e2)}")
                        return 0
                
                if not df.empty:
                    file_path = api_dir / "static_data.csv"
                    df.to_csv(file_path, index=False, encoding='utf-8-sig')
                    logging.info(f"âœ… {desc}: {len(df)} æ¡è®°å½•")
                    return len(df)
                else:
                    logging.info(f"âšª {desc}: æ— æ•°æ®")
                    return 0
            else:
                logging.error(f"âŒ APIå‡½æ•° {api_name} ä¸å­˜åœ¨")
                return 0
                
        except Exception as e:
            logging.error(f"âŒ {desc} ä¸‹è½½å¤±è´¥: {str(e)}")
            return 0
    
    def run(self):
        """æ‰§è¡Œè¡¥å……ä¸‹è½½"""
        logging.info("ğŸš€ å¼€å§‹è¡¥å……ä¸‹è½½ç¼ºå¤±çš„3ä¸ªAPIï¼ˆä½¿ç”¨æ­£ç¡®çš„è°ƒç”¨æ–¹å¼ï¼‰...")
        logging.info("ğŸ“‹ åŸºäºæœ€æ–°ä¼˜çŸ¿æ¥å£æ¸…å•å’Œä½¿ç”¨è¯´æ˜")
        
        success_count = 0
        total_records = 0
        
        for api_name, config in self.missing_apis.items():
            desc = config["desc"]
            dir_name = config.get("dir_name", api_name.lower())
            
            try:
                if config.get("time_range", False):
                    records = self.download_with_time_range(api_name, desc, dir_name)
                else:
                    records = self.download_static_data(api_name, desc, dir_name)
                
                if records > 0:
                    success_count += 1
                    total_records += records
                    
            except Exception as e:
                logging.error(f"âŒ {api_name} å¤„ç†å¤±è´¥: {str(e)}")
        
        # æ›´æ–°è¿›åº¦æ–‡ä»¶
        self.update_progress(success_count, total_records)
        
        logging.info(f"ğŸ‰ è¡¥å……ä¸‹è½½å®Œæˆ! æˆåŠŸ: {success_count}/3, æ€»è®°å½•: {total_records}")
        
        if success_count == 3:
            logging.info("ğŸŒŸ æ‰€æœ‰ç¼ºå¤±APIä¸‹è½½æˆåŠŸï¼ç°åœ¨æ‹¥æœ‰å®Œæ•´çš„58ä¸ªAPIæ•°æ®ï¼")
        elif success_count > 0:
            logging.info(f"âœ¨ éƒ¨åˆ†APIä¸‹è½½æˆåŠŸï¼Œè¿˜æœ‰ {3 - success_count} ä¸ªAPIéœ€è¦å¤„ç†")
        else:
            logging.warning("âš ï¸ æ‰€æœ‰APIä¸‹è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥APIè°ƒç”¨æ–¹å¼")
        
    def update_progress(self, success_count, total_records):
        """æ›´æ–°ä¸»è¿›åº¦æ–‡ä»¶"""
        progress_file = self.data_dir / "download_progress.json"
        
        try:
            with open(progress_file, 'r', encoding='utf-8') as f:
                progress_data = json.load(f)
            
            # æ·»åŠ æ–°å®Œæˆçš„API
            completed_apis = progress_data.get("completed_apis", [])
            for api_name in self.missing_apis.keys():
                api_key = f"special_trading_{api_name}"
                if api_key not in completed_apis:
                    completed_apis.append(api_key)
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            progress_data["completed_apis"] = completed_apis
            progress_data["statistics"]["success_count"] = len(completed_apis)
            progress_data["statistics"]["total_records"] += total_records
            progress_data["last_update"] = datetime.now().isoformat()
            
            # ä¿å­˜æ›´æ–°åçš„è¿›åº¦
            with open(progress_file, 'w', encoding='utf-8') as f:
                json.dump(progress_data, f, indent=2, ensure_ascii=False)
                
            logging.info(f"ğŸ“Š è¿›åº¦æ–‡ä»¶å·²æ›´æ–°: æ€»APIæ•° {len(completed_apis)}")
            
            if len(completed_apis) >= 58:
                logging.info("ğŸŠ æ­å–œï¼å·²å®Œæˆæ‰€æœ‰58ä¸ªAPIæ¥å£ä¸‹è½½ï¼")
            
        except Exception as e:
            logging.error(f"âŒ æ›´æ–°è¿›åº¦æ–‡ä»¶å¤±è´¥: {str(e)}")

if __name__ == "__main__":
    downloader = FinalMissingAPIDownloader()
    downloader.run()