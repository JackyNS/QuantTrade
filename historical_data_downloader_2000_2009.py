#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
2000-2009å¹´å†å²æ•°æ®ä¸‹è½½å™¨
=====================

è¡¥é½æ ¸å¿ƒ10ä¸ªæ¥å£åœ¨2000å¹´1æœˆ1æ—¥è‡³2009å¹´12æœˆ31æ—¥çš„å†å²æ•°æ®
ç¡®ä¿QuantTradeæ¡†æ¶æ‹¥æœ‰å®Œæ•´çš„25å¹´æ•°æ®è¦†ç›–
"""

import uqer
import pandas as pd
from pathlib import Path
from datetime import datetime
import time
import logging
from typing import Dict, List, Optional

# ä¼˜çŸ¿Token
UQER_TOKEN = "68b9922817ae6273137bda7acba81e293582ba347281dfcc056dcb245b23faf3"

class HistoricalDataDownloader2000_2009:
    """2000-2009å¹´å†å²æ•°æ®ä¸‹è½½å™¨"""
    
    def __init__(self):
        # åˆå§‹åŒ–uqerå®¢æˆ·ç«¯
        uqer.Client(token=UQER_TOKEN)
        self.client = uqer
        self.base_path = Path("/Users/jackstudio/QuantTrade/data")
        self.base_path.mkdir(exist_ok=True)
        
        # è®¾ç½®æ—¥å¿—
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {'success': 0, 'failed': 0, 'records': 0}
        
        print("ğŸ“œ ä¼˜çŸ¿2000-2009å¹´å†å²æ•°æ®ä¸‹è½½å™¨")
        print("ğŸ¯ ç›®æ ‡: è¡¥é½25å¹´æ•°æ®è¦†ç›–çš„å†å²éƒ¨åˆ†")
        print("ğŸ“Š èŒƒå›´: å…¨å¸‚åœºè‚¡ç¥¨ï¼Œ2000-2009å¹´")
        print("â° æ—¶é—´çª—å£: 2000å¹´1æœˆ1æ—¥ - 2009å¹´12æœˆ31æ—¥")
        print("=" * 70)
        
        # æ ¸å¿ƒ10ä¸ªæ¥å£é…ç½®
        self.apis = {
            "EquGet": {
                "name": "è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯", 
                "dir": "basic_info",
                "fields": "secID,ticker,secShortName,listDate,delistDate",
                "date_field": None,  # ä¸éœ€è¦æ—¥æœŸèŒƒå›´
                "batch_size": 300
            },
            "TradeCalGet": {
                "name": "äº¤æ˜“æ—¥å†",
                "dir": "calendar", 
                "fields": "calendarDate,isOpen",
                "date_field": ("beginDate", "endDate"),
                "batch_size": None  # ä¸åˆ†æ‰¹
            },
            "MktEqudGet": {
                "name": "è‚¡ç¥¨æ—¥è¡Œæƒ…",
                "dir": "daily",
                "fields": "secID,ticker,tradeDate,preClosePrice,openPrice,highestPrice,lowestPrice,closePrice,turnoverVol,marketValue",
                "date_field": ("beginDate", "endDate"),
                "batch_size": 50
            },
            "MktAdjfGet": {
                "name": "å¤æƒå› å­",
                "dir": "adjustment",
                "fields": "secID,ticker,exDivDate,adjfactor",
                "date_field": ("beginDate", "endDate"),
                "batch_size": 200
            },
            "EquDivGet": {
                "name": "è‚¡ç¥¨åˆ†çº¢",
                "dir": "dividend",
                "fields": "secID,ticker,exDate,dividend,splitRatio",
                "date_field": ("beginDate", "endDate"), 
                "batch_size": 200
            },
            "FdmtBs2018Get": {
                "name": "èµ„äº§è´Ÿå€ºè¡¨",
                "dir": "financial",
                "fields": "secID,ticker,endDate,totalAssets,totalLiab,totalShrhldrEqty",
                "date_field": ("beginDate", "endDate"),
                "batch_size": 100
            },
            "FdmtDerGet": {
                "name": "è´¢åŠ¡è¡ç”Ÿæ•°æ®",
                "dir": "financial",
                "fields": "secID,ticker,endDate,revenue,netProfit,roe",
                "date_field": ("beginDate", "endDate"),
                "batch_size": 100
            },
            "MktEquFlowGet": {
                "name": "èµ„é‡‘æµå‘",
                "dir": "capital_flow",
                "fields": "secID,ticker,tradeDate,mainNetFlow,superNetFlow,largeNetFlow,mediumNetFlow,smallNetFlow",
                "date_field": ("beginDate", "endDate"),
                "batch_size": 30
            },
            "MktLimitGet": {
                "name": "æ¶¨è·Œåœæ•°æ®",
                "dir": "limit_info",
                "fields": "secID,ticker,tradeDate,upLimit,downLimit",
                "date_field": ("beginDate", "endDate"),
                "batch_size": 100
            },
            "MktRankListStocksGet": {
                "name": "é¾™è™æ¦œæ•°æ®", 
                "dir": "rank_list",
                "fields": "secID,ticker,tradeDate,rankReason,buyAmt,sellAmt",
                "date_field": ("beginDate", "endDate"),
                "batch_size": None  # ä¸åˆ†æ‰¹
            }
        }
    
    def get_all_stocks(self) -> List[str]:
        """è·å–å…¨éƒ¨è‚¡ç¥¨åˆ—è¡¨"""
        try:
            print("ğŸ“‹ è·å–å…¨å¸‚åœºè‚¡ç¥¨åˆ—è¡¨...")
            result = self.client.DataAPI.EquGet(
                listStatusCD='',  # åŒ…å«é€€å¸‚è‚¡ç¥¨
                field='secID,ticker,listDate,delistDate',
                pandas='1'
            )
            
            if isinstance(result, pd.DataFrame) and not result.empty:
                stocks = result['secID'].unique().tolist()
                print(f"âœ… è·å–åˆ° {len(stocks)} åªè‚¡ç¥¨ (åŒ…å«é€€å¸‚è‚¡ç¥¨)")
                return stocks
            return []
        except Exception as e:
            print(f"âŒ è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")
            return []
    
    def download_single_api_historical(self, api_name: str, api_config: Dict, stocks: List[str]) -> bool:
        """ä¸‹è½½å•ä¸ªAPIçš„2000-2009å†å²æ•°æ®"""
        print(f"\nğŸ“Š ä¸‹è½½ {api_config['name']} (2000-2009)...")
        
        output_dir = self.base_path / api_config['dir']
        output_dir.mkdir(exist_ok=True)
        
        try:
            api_func = getattr(self.client.DataAPI, api_name)
            all_data = []
            
            # ç‰¹æ®Šå¤„ç†äº¤æ˜“æ—¥å†å’Œé¾™è™æ¦œ (ä¸éœ€è¦åˆ†è‚¡ç¥¨)
            if api_name in ["TradeCalGet", "MktRankListStocksGet"]:
                print(f"  ğŸ“… ä¸‹è½½æ•´ä½“æ•°æ®...")
                
                try:
                    if api_name == "TradeCalGet":
                        result = api_func(
                            beginDate='20000101',
                            endDate='20091231',
                            field=api_config['fields'],
                            pandas='1'
                        )
                    else:  # MktRankListStocksGet
                        # åˆ†å¹´ä¸‹è½½é¾™è™æ¦œ
                        for year in range(2000, 2010):
                            year_result = api_func(
                                beginDate=f'{year}0101',
                                endDate=f'{year}1231', 
                                field=api_config['fields'],
                                pandas='1'
                            )
                            if isinstance(year_result, pd.DataFrame) and not year_result.empty:
                                all_data.append(year_result)
                                print(f"    âœ… {year}å¹´: {len(year_result)} æ¡è®°å½•")
                            time.sleep(1)
                        
                        if all_data:
                            result = pd.concat(all_data, ignore_index=True)
                        else:
                            result = pd.DataFrame()
                    
                    if isinstance(result, pd.DataFrame) and not result.empty:
                        output_file = output_dir / f"{api_config['name'].replace('æ•°æ®', '').replace('ä¿¡æ¯', '')}_2000_2009.csv"
                        result.to_csv(output_file, index=False)
                        
                        self.stats['success'] += 1
                        self.stats['records'] += len(result)
                        print(f"âœ… {api_config['name']} å®Œæˆ: {len(result)} æ¡è®°å½•")
                        return True
                    
                except Exception as e:
                    print(f"    âŒ ä¸‹è½½å¤±è´¥: {e}")
                    return False
            
            # å¤„ç†éœ€è¦åˆ†è‚¡ç¥¨çš„API
            elif api_config['batch_size'] and stocks:
                batch_size = api_config['batch_size']
                
                # åˆ†å¹´åˆ†æ‰¹ä¸‹è½½ (2000-2009å¹´æ•°æ®é‡å¤§)
                for year in range(2000, 2010):
                    print(f"  ğŸ“… ä¸‹è½½ {year} å¹´æ•°æ®...")
                    year_data = []
                    
                    for i in range(0, len(stocks), batch_size):
                        batch_stocks = stocks[i:i+batch_size]
                        batch_tickers = ','.join([s.split('.')[0] for s in batch_stocks])
                        
                        print(f"    ğŸ”„ {year}å¹´ æ‰¹æ¬¡ {i//batch_size + 1}/{(len(stocks)-1)//batch_size + 1}: {len(batch_stocks)} åªè‚¡ç¥¨")
                        
                        try:
                            if api_name == "EquGet":
                                # åŸºæœ¬ä¿¡æ¯ä¸éœ€è¦æ—¥æœŸèŒƒå›´
                                result = api_func(
                                    secID='',
                                    ticker=batch_tickers,
                                    listStatusCD='',
                                    field=api_config['fields'],
                                    pandas='1'
                                )
                            else:
                                # å…¶ä»–APIéœ€è¦æ—¥æœŸèŒƒå›´
                                result = api_func(
                                    secID='',
                                    ticker=batch_tickers,
                                    beginDate=f'{year}0101',
                                    endDate=f'{year}1231',
                                    field=api_config['fields'],
                                    pandas='1'
                                )
                            
                            if isinstance(result, pd.DataFrame) and not result.empty:
                                year_data.append(result)
                                print(f"      âœ… å®Œæˆ: {len(result)} æ¡è®°å½•")
                            
                            time.sleep(0.5)  # é˜²æ­¢é¢‘ç‡é™åˆ¶
                            
                        except Exception as e:
                            print(f"      âŒ æ‰¹æ¬¡å¤±è´¥: {e}")
                            continue
                    
                    # ä¿å­˜å¹´åº¦æ•°æ®
                    if year_data:
                        year_combined = pd.concat(year_data, ignore_index=True)
                        all_data.append(year_combined)
                        print(f"    âœ… {year}å¹´å®Œæˆ: {len(year_combined)} æ¡è®°å½•")
                    
                    # åŸºæœ¬ä¿¡æ¯åªéœ€è¦ä¸‹è½½ä¸€æ¬¡
                    if api_name == "EquGet":
                        break
            
            # ä¿å­˜æœ€ç»ˆæ•°æ®
            if all_data:
                combined = pd.concat(all_data, ignore_index=True)
                
                # å»é‡å¤„ç†
                if 'secID' in combined.columns:
                    before_count = len(combined)
                    if api_name == "EquGet":
                        combined = combined.drop_duplicates(subset=['secID'])
                    else:
                        # å…¶ä»–APIæŒ‰è‚¡ç¥¨å’Œæ—¥æœŸå»é‡
                        date_cols = ['tradeDate', 'exDivDate', 'exDate', 'endDate', 'calendarDate']
                        date_col = next((col for col in date_cols if col in combined.columns), None)
                        if date_col:
                            combined = combined.drop_duplicates(subset=['secID', date_col])
                        else:
                            combined = combined.drop_duplicates()
                    
                    after_count = len(combined)
                    if before_count != after_count:
                        print(f"    ğŸ”„ å»é‡: {before_count} â†’ {after_count} æ¡è®°å½•")
                
                output_file = output_dir / f"{api_config['name'].replace('æ•°æ®', '').replace('ä¿¡æ¯', '')}_2000_2009.csv"
                combined.to_csv(output_file, index=False)
                
                self.stats['success'] += 1
                self.stats['records'] += len(combined)
                print(f"âœ… {api_config['name']} å®Œæˆ: {len(combined)} æ¡è®°å½•")
                return True
            
            return False
            
        except Exception as e:
            print(f"âŒ {api_config['name']} ä¸‹è½½å¤±è´¥: {e}")
            self.stats['failed'] += 1
            return False
    
    def run_historical_download(self):
        """è¿è¡Œ2000-2009å¹´å†å²æ•°æ®ä¸‹è½½"""
        start_time = datetime.now()
        
        try:
            # 1. è·å–å…¨éƒ¨è‚¡ç¥¨åˆ—è¡¨ (åŒ…å«é€€å¸‚è‚¡ç¥¨)
            stocks = self.get_all_stocks()
            if not stocks:
                print("âŒ æ— æ³•è·å–è‚¡ç¥¨åˆ—è¡¨")
                return
            
            print(f"\nğŸ¯ å¼€å§‹ä¸‹è½½2000-2009å¹´å†å²æ•°æ®")
            print(f"ğŸ“Š èŒƒå›´: {len(stocks)} åªè‚¡ç¥¨ (å«é€€å¸‚)")
            print(f"ğŸ”§ æ¥å£: {len(self.apis)} ä¸ªæ ¸å¿ƒAPI")
            print(f"â±ï¸ é¢„è®¡æ—¶é—´: 4-6å°æ—¶")
            print(f"ğŸ’¾ é¢„è®¡æ•°æ®é‡: 2-3GB")
            print()
            
            # 2. æŒ‰ä¼˜å…ˆçº§ä¸‹è½½å„APIæ•°æ®
            priority_order = [
                "EquGet",           # åŸºæœ¬ä¿¡æ¯ (å¿…é¡»æœ€å…ˆ)
                "TradeCalGet",      # äº¤æ˜“æ—¥å†
                "MktEqudGet",       # æ—¥è¡Œæƒ… (æ ¸å¿ƒ)
                "MktAdjfGet",       # å¤æƒå› å­
                "EquDivGet",        # åˆ†çº¢æ•°æ®
                "FdmtBs2018Get",    # è´¢åŠ¡æ•°æ®1
                "FdmtDerGet",       # è´¢åŠ¡æ•°æ®2
                "MktEquFlowGet",    # èµ„é‡‘æµå‘
                "MktLimitGet",      # æ¶¨è·Œåœ
                "MktRankListStocksGet"  # é¾™è™æ¦œ
            ]
            
            print("ğŸ”„ å¼€å§‹æŒ‰ä¼˜å…ˆçº§ä¸‹è½½...")
            
            for i, api_name in enumerate(priority_order, 1):
                if api_name in self.apis:
                    print(f"\n{'='*60}")
                    print(f"ğŸ“‹ é˜¶æ®µ {i}/{len(priority_order)}: {self.apis[api_name]['name']}")
                    print(f"{'='*60}")
                    
                    success = self.download_single_api_historical(
                        api_name, 
                        self.apis[api_name], 
                        stocks
                    )
                    
                    if success:
                        print(f"âœ… é˜¶æ®µ{i}å®Œæˆ: {self.apis[api_name]['name']}")
                    else:
                        print(f"âš ï¸ é˜¶æ®µ{i}éƒ¨åˆ†å¤±è´¥: {self.apis[api_name]['name']}")
                    
                    # é˜¶æ®µé—´ä¼‘æ¯
                    print("â³ é˜¶æ®µé—´ä¼‘æ¯ 5 ç§’...")
                    time.sleep(5)
            
            # 3. ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
            end_time = datetime.now()
            duration = end_time - start_time
            
            print(f"\nğŸŠ 2000-2009å¹´å†å²æ•°æ®ä¸‹è½½å®Œæˆ!")
            print(f"â±ï¸ æ€»è€—æ—¶: {duration}")
            print(f"âœ… æˆåŠŸ: {self.stats['success']} ä¸ªæ¥å£")
            print(f"âŒ å¤±è´¥: {self.stats['failed']} ä¸ªæ¥å£") 
            print(f"ğŸ“‹ æ€»è®°å½•: {self.stats['records']:,} æ¡")
            
            # è®¡ç®—æ–‡ä»¶å¤§å°
            total_size = 0
            for api_config in self.apis.values():
                dir_path = self.base_path / api_config['dir']
                if dir_path.exists():
                    for file in dir_path.glob("*_2000_2009.csv"):
                        total_size += file.stat().st_size
            
            print(f"ğŸ’¾ æ•°æ®å¤§å°: {total_size / (1024*1024*1024):.2f} GB")
            print(f"ğŸ¯ å®Œæˆåº¦: {self.stats['success']}/{len(self.apis)} ({self.stats['success']/len(self.apis)*100:.1f}%)")
            
            if self.stats['success'] >= 8:  # 80%ä»¥ä¸ŠæˆåŠŸç‡
                print(f"\nğŸš€ å†å²æ•°æ®è¡¥é½æˆåŠŸï¼")
                print(f"ğŸ“Š QuantTradeæ¡†æ¶ç°å·²æ‹¥æœ‰å®Œæ•´25å¹´æ•°æ®è¦†ç›– (2000-2025)")
            else:
                print(f"\nâš ï¸ éƒ¨åˆ†æ•°æ®ä¸‹è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–APIæƒé™")
                
        except Exception as e:
            print(f"âŒ ä¸‹è½½è¿‡ç¨‹å¼‚å¸¸: {e}")

def main():
    downloader = HistoricalDataDownloader2000_2009()
    downloader.run_historical_download()

if __name__ == "__main__":
    main()