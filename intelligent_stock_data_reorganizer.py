#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ™ºèƒ½ä¸ªè‚¡æ•°æ®é‡ç»„å™¨
=================

ç›®æ ‡ï¼š
1. ä»æ‰¹æ¬¡æ–‡ä»¶ä¸­æå–æ‰€æœ‰è‚¡ç¥¨æ•°æ®
2. æŒ‰ä¸ªè‚¡é‡ç»„ï¼Œè€ƒè™‘ä¸Šå¸‚æ—¶é—´å’Œé€€å¸‚æ—¶é—´
3. ç”Ÿæˆå®Œæ•´çš„ä¸ªè‚¡æ–‡ä»¶ï¼Œæ›¿æ¢ä¸å®Œæ•´çš„ç°æœ‰æ–‡ä»¶
4. å»ºç«‹åˆç†çš„æ•°æ®å­˜å‚¨ç»“æ„

ç­–ç•¥ï¼š
- æ‰«ææ‰€æœ‰æ‰¹æ¬¡æ–‡ä»¶ï¼Œæ”¶é›†æ¯åªè‚¡ç¥¨çš„å®Œæ•´æ•°æ®
- è·å–è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯ï¼ˆä¸Šå¸‚æ—¶é—´ã€é€€å¸‚æ—¶é—´ï¼‰
- æŒ‰è‚¡ç¥¨åˆå¹¶æ‰€æœ‰æ—¶é—´æ®µçš„æ•°æ®
- ç”Ÿæˆç»Ÿä¸€æ ¼å¼çš„ä¸ªè‚¡æ–‡ä»¶

"""

import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
import logging
import json
import time
import warnings
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

warnings.filterwarnings('ignore')

class IntelligentStockDataReorganizer:
    """æ™ºèƒ½ä¸ªè‚¡æ•°æ®é‡ç»„å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–"""
        self.base_path = Path("/Users/jackstudio/QuantTrade/data")
        self.batch_path = self.base_path / "priority_download/market_data/daily"
        self.output_path = self.base_path / "reorganized_stocks"
        self.output_path.mkdir(exist_ok=True)
        
        # è®¾ç½®æ—¥å¿—
        self.setup_logging()
        
        # è‚¡ç¥¨ä¿¡æ¯ç¼“å­˜
        self.stock_info_cache = {}
        self.stock_data_cache = defaultdict(list)
        
        # è¿›åº¦è¿½è¸ª
        self.progress_lock = threading.Lock()
        self.processed_files = 0
        self.total_files = 0
        
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        log_file = self.output_path / "reorganization.log"
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def scan_all_batch_files(self):
        """æ‰«ææ‰€æœ‰æ‰¹æ¬¡æ–‡ä»¶ï¼Œæ”¶é›†è‚¡ç¥¨åˆ—è¡¨"""
        print("ğŸ” æ‰«ææ‰€æœ‰æ‰¹æ¬¡æ–‡ä»¶ï¼Œæ”¶é›†è‚¡ç¥¨åˆ—è¡¨...")
        print("=" * 80)
        
        batch_files = list(self.batch_path.glob("*.csv"))
        self.total_files = len(batch_files)
        
        print(f"ğŸ“Š å‘ç°æ‰¹æ¬¡æ–‡ä»¶: {len(batch_files)} ä¸ª")
        
        all_stocks = set()
        year_stats = defaultdict(set)
        
        for i, batch_file in enumerate(batch_files, 1):
            try:
                df = pd.read_csv(batch_file, nrows=0)  # åªè¯»å–åˆ—å
                if 'secID' not in df.columns:
                    continue
                
                # è¯»å–å®Œæ•´æ•°æ®
                df = pd.read_csv(batch_file)
                if 'secID' in df.columns:
                    batch_stocks = df['secID'].unique()
                    all_stocks.update(batch_stocks)
                    
                    # æŒ‰å¹´ä»½ç»Ÿè®¡
                    year = batch_file.stem.split('_')[0]
                    year_stats[year].update(batch_stocks)
                
                if i % 50 == 0 or i <= 10:
                    print(f"   å¤„ç†è¿›åº¦: {i}/{len(batch_files)} | ç´¯è®¡è‚¡ç¥¨: {len(all_stocks)}")
                    
            except Exception as e:
                self.logger.warning(f"è·³è¿‡æ–‡ä»¶ {batch_file.name}: {str(e)}")
                continue
        
        print(f"âœ… æ‰«æå®Œæˆ")
        print(f"ğŸ“ˆ å‘ç°è‚¡ç¥¨æ€»æ•°: {len(all_stocks)}")
        print(f"ğŸ“… å¹´ä»½åˆ†å¸ƒ: {len(year_stats)} å¹´")
        
        # æ˜¾ç¤ºå¹´ä»½ç»Ÿè®¡
        print("\nğŸ“Š å„å¹´ä»½è‚¡ç¥¨æ•°é‡:")
        for year in sorted(year_stats.keys()):
            print(f"   {year}: {len(year_stats[year])} åª")
        
        return sorted(list(all_stocks)), year_stats
    
    def get_stock_basic_info(self, stock_list):
        """è·å–è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯ï¼ˆä¸Šå¸‚æ—¶é—´ã€é€€å¸‚æ—¶é—´ç­‰ï¼‰"""
        print(f"\nğŸ“‹ è·å– {len(stock_list)} åªè‚¡ç¥¨çš„åŸºæœ¬ä¿¡æ¯...")
        print("-" * 60)
        
        # å°è¯•ä»ç°æœ‰çš„åŸºæœ¬ä¿¡æ¯æ–‡ä»¶ä¸­è¯»å–
        basic_info_paths = [
            "final_comprehensive_download/basic_info",
            "optimized_data/basic_info",
            "raw/basic_info",
            "csv_complete/static"
        ]
        
        stock_info = {}
        
        for info_path in basic_info_paths:
            full_path = self.base_path / info_path
            if full_path.exists():
                print(f"ğŸ” æ£€æŸ¥è·¯å¾„: {info_path}")
                info_files = list(full_path.rglob("*.csv"))
                
                for info_file in info_files[:5]:  # æ£€æŸ¥å‰5ä¸ªæ–‡ä»¶
                    try:
                        df = pd.read_csv(info_file)
                        
                        # æŸ¥æ‰¾ç›¸å…³åˆ—
                        secID_col = None
                        listDate_col = None
                        delistDate_col = None
                        
                        for col in df.columns:
                            col_lower = col.lower()
                            if 'secid' in col_lower:
                                secID_col = col
                            elif any(keyword in col_lower for keyword in ['listdate', 'list_date', 'ä¸Šå¸‚']):
                                listDate_col = col
                            elif any(keyword in col_lower for keyword in ['delistdate', 'delist_date', 'é€€å¸‚']):
                                delistDate_col = col
                        
                        if secID_col and len(stock_info) < 1000:  # é™åˆ¶è·å–æ•°é‡
                            for _, row in df.iterrows():
                                stock_id = row[secID_col]
                                if stock_id in stock_list and stock_id not in stock_info:
                                    info = {'secID': stock_id}
                                    
                                    # è·å–ä¸Šå¸‚æ—¥æœŸ
                                    if listDate_col and pd.notna(row[listDate_col]):
                                        try:
                                            list_date = pd.to_datetime(row[listDate_col])
                                            if list_date.year >= 1990:
                                                info['listDate'] = list_date.strftime('%Y-%m-%d')
                                        except:
                                            pass
                                    
                                    # è·å–é€€å¸‚æ—¥æœŸ
                                    if delistDate_col and pd.notna(row[delistDate_col]):
                                        try:
                                            delist_date = pd.to_datetime(row[delistDate_col])
                                            if delist_date.year >= 1990:
                                                info['delistDate'] = delist_date.strftime('%Y-%m-%d')
                                        except:
                                            pass
                                    
                                    stock_info[stock_id] = info
                        
                        if len(stock_info) >= 1000:
                            break
                            
                    except Exception as e:
                        continue
                
                if stock_info:
                    print(f"   âœ… ä» {info_file.name} è·å–äº† {len(stock_info)} åªè‚¡ç¥¨ä¿¡æ¯")
                    break
        
        print(f"ğŸ“Š è·å–è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯: {len(stock_info)} åª")
        
        # å¯¹äºæ²¡æœ‰åŸºæœ¬ä¿¡æ¯çš„è‚¡ç¥¨ï¼Œä½¿ç”¨æ•°æ®æ¨æ–­
        missing_stocks = set(stock_list) - set(stock_info.keys())
        print(f"âš ï¸ ç¼ºå¤±åŸºæœ¬ä¿¡æ¯çš„è‚¡ç¥¨: {len(missing_stocks)} åªï¼ˆå°†ä»æ•°æ®æ¨æ–­ï¼‰")
        
        self.stock_info_cache = stock_info
        return stock_info
    
    def collect_stock_data_from_batches(self, stock_id, max_workers=4):
        """ä»æ‰€æœ‰æ‰¹æ¬¡æ–‡ä»¶ä¸­æ”¶é›†å•åªè‚¡ç¥¨çš„æ•°æ®"""
        batch_files = list(self.batch_path.glob("*.csv"))
        stock_data_pieces = []
        
        def process_batch_file(batch_file):
            """å¤„ç†å•ä¸ªæ‰¹æ¬¡æ–‡ä»¶"""
            try:
                df = pd.read_csv(batch_file)
                if 'secID' in df.columns:
                    stock_data = df[df['secID'] == stock_id]
                    if len(stock_data) > 0:
                        return stock_data
            except Exception as e:
                pass
            return None
        
        # å¤šçº¿ç¨‹å¤„ç†æ‰¹æ¬¡æ–‡ä»¶
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_file = {executor.submit(process_batch_file, batch_file): batch_file 
                             for batch_file in batch_files}
            
            for future in as_completed(future_to_file):
                result = future.result()
                if result is not None:
                    stock_data_pieces.append(result)
        
        if not stock_data_pieces:
            return None
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®ç‰‡æ®µ
        combined_data = pd.concat(stock_data_pieces, ignore_index=True)
        
        # å»é‡å¹¶æ’åº
        if 'tradeDate' in combined_data.columns:
            combined_data['tradeDate'] = pd.to_datetime(combined_data['tradeDate'])
            combined_data = combined_data.drop_duplicates(subset=['tradeDate']).sort_values('tradeDate')
            combined_data['tradeDate'] = combined_data['tradeDate'].dt.strftime('%Y-%m-%d')
        
        return combined_data
    
    def determine_stock_time_range(self, stock_id, stock_data):
        """ç¡®å®šè‚¡ç¥¨çš„åˆç†æ—¶é—´èŒƒå›´"""
        if stock_data is None or len(stock_data) == 0:
            return None, None
        
        # ä»æ•°æ®ä¸­è·å–å®é™…çš„æ—¶é—´èŒƒå›´
        dates = pd.to_datetime(stock_data['tradeDate'])
        actual_start = dates.min().strftime('%Y-%m-%d')
        actual_end = dates.max().strftime('%Y-%m-%d')
        
        # å¦‚æœæœ‰åŸºæœ¬ä¿¡æ¯ï¼Œä½¿ç”¨åŸºæœ¬ä¿¡æ¯æ ¡å‡†
        expected_start = actual_start
        expected_end = actual_end
        
        if stock_id in self.stock_info_cache:
            info = self.stock_info_cache[stock_id]
            if 'listDate' in info:
                list_date = info['listDate']
                # é¢„æœŸå¼€å§‹æ—¶é—´ä¸åº”è¯¥æ—©äºä¸Šå¸‚æ—¶é—´
                if list_date > actual_start:
                    expected_start = list_date
            
            if 'delistDate' in info:
                delist_date = info['delistDate']
                # é¢„æœŸç»“æŸæ—¶é—´ä¸åº”è¯¥æ™šäºé€€å¸‚æ—¶é—´
                if delist_date < actual_end:
                    expected_end = delist_date
        
        return expected_start, expected_end
    
    def reorganize_single_stock(self, stock_id):
        """é‡ç»„å•åªè‚¡ç¥¨çš„æ•°æ®"""
        try:
            # æ”¶é›†è‚¡ç¥¨æ•°æ®
            stock_data = self.collect_stock_data_from_batches(stock_id)
            
            if stock_data is None or len(stock_data) == 0:
                return {'status': 'no_data', 'stock_id': stock_id}
            
            # ç¡®å®šæ—¶é—´èŒƒå›´
            expected_start, expected_end = self.determine_stock_time_range(stock_id, stock_data)
            
            # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
            safe_stock_id = stock_id.replace('.', '_')
            output_file = self.output_path / f"{safe_stock_id}.csv"
            
            # ä¿å­˜æ•°æ®
            stock_data.to_csv(output_file, index=False)
            
            result = {
                'status': 'success',
                'stock_id': stock_id,
                'output_file': str(output_file),
                'records': len(stock_data),
                'date_range': f"{expected_start} - {expected_end}",
                'years': round((pd.to_datetime(expected_end) - pd.to_datetime(expected_start)).days / 365.25, 1)
            }
            
            # æ›´æ–°è¿›åº¦
            with self.progress_lock:
                self.processed_files += 1
                if self.processed_files % 50 == 0 or self.processed_files <= 10:
                    print(f"âœ… è¿›åº¦ [{self.processed_files}/{len(self.target_stocks)}] {stock_id}: {result['records']} æ¡è®°å½•")
            
            return result
            
        except Exception as e:
            self.logger.error(f"å¤„ç†è‚¡ç¥¨ {stock_id} å¤±è´¥: {str(e)}")
            return {'status': 'error', 'stock_id': stock_id, 'error': str(e)}
    
    def reorganize_all_stocks(self, stock_list, max_workers=8, batch_size=100):
        """é‡ç»„æ‰€æœ‰è‚¡ç¥¨æ•°æ®"""
        print(f"\nğŸ”„ å¼€å§‹é‡ç»„ {len(stock_list)} åªè‚¡ç¥¨çš„æ•°æ®...")
        print("=" * 80)
        
        self.target_stocks = stock_list
        results = []
        
        # åˆ†æ‰¹å¤„ç†
        for i in range(0, len(stock_list), batch_size):
            batch_stocks = stock_list[i:i+batch_size]
            batch_num = i // batch_size + 1
            total_batches = (len(stock_list) + batch_size - 1) // batch_size
            
            print(f"\nğŸ“¦ å¤„ç†æ‰¹æ¬¡ {batch_num}/{total_batches}: {len(batch_stocks)} åªè‚¡ç¥¨")
            
            # å¤šçº¿ç¨‹å¤„ç†å½“å‰æ‰¹æ¬¡
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                future_to_stock = {executor.submit(self.reorganize_single_stock, stock): stock 
                                 for stock in batch_stocks}
                
                batch_results = []
                for future in as_completed(future_to_stock):
                    result = future.result()
                    batch_results.append(result)
                    results.append(result)
            
            # æ‰¹æ¬¡ç»Ÿè®¡
            success_count = sum(1 for r in batch_results if r['status'] == 'success')
            print(f"   âœ… æ‰¹æ¬¡å®Œæˆ: {success_count}/{len(batch_stocks)} æˆåŠŸ")
        
        return results
    
    def generate_reorganization_report(self, results):
        """ç”Ÿæˆé‡ç»„æŠ¥å‘Š"""
        print(f"\nğŸ“Š ç”Ÿæˆé‡ç»„æŠ¥å‘Š...")
        print("=" * 80)
        
        # ç»Ÿè®¡ç»“æœ
        success_results = [r for r in results if r['status'] == 'success']
        error_results = [r for r in results if r['status'] == 'error']
        no_data_results = [r for r in results if r['status'] == 'no_data']
        
        total_records = sum(r.get('records', 0) for r in success_results)
        
        print(f"âœ… é‡ç»„æˆåŠŸ: {len(success_results)} åªè‚¡ç¥¨")
        print(f"âŒ é‡ç»„å¤±è´¥: {len(error_results)} åªè‚¡ç¥¨")
        print(f"âš ï¸ æ— æ•°æ®: {len(no_data_results)} åªè‚¡ç¥¨")
        print(f"ğŸ“‹ æ€»è®°å½•æ•°: {total_records:,} æ¡")
        
        # æ˜¾ç¤ºæˆåŠŸæ¡ˆä¾‹æ ·æœ¬
        if success_results:
            print(f"\nâœ… æˆåŠŸæ¡ˆä¾‹æ ·æœ¬ï¼ˆå‰10åªï¼‰:")
            for result in success_results[:10]:
                print(f"   {result['stock_id']}: {result['records']} æ¡è®°å½•, {result['date_range']}")
        
        # æ˜¾ç¤ºå¤±è´¥æ¡ˆä¾‹
        if error_results:
            print(f"\nâŒ å¤±è´¥æ¡ˆä¾‹ï¼ˆå‰5åªï¼‰:")
            for result in error_results[:5]:
                print(f"   {result['stock_id']}: {result.get('error', 'Unknown error')}")
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report = {
            'reorganization_time': datetime.now().isoformat(),
            'summary': {
                'total_stocks': len(results),
                'success_count': len(success_results),
                'error_count': len(error_results),
                'no_data_count': len(no_data_results),
                'total_records': total_records
            },
            'results': results
        }
        
        report_file = self.output_path / f"reorganization_report_{timestamp}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        
        return report
    
    def run_complete_reorganization(self):
        """è¿è¡Œå®Œæ•´çš„æ•°æ®é‡ç»„æµç¨‹"""
        start_time = datetime.now()
        
        print("ğŸš€ æ™ºèƒ½ä¸ªè‚¡æ•°æ®é‡ç»„å™¨")
        print("ğŸ¯ ç›®æ ‡ï¼šä»æ‰¹æ¬¡æ–‡ä»¶é‡ç»„å®Œæ•´çš„ä¸ªè‚¡æ•°æ®")
        print("=" * 80)
        
        try:
            # 1. æ‰«ææ‰¹æ¬¡æ–‡ä»¶ï¼Œæ”¶é›†è‚¡ç¥¨åˆ—è¡¨
            all_stocks, year_stats = self.scan_all_batch_files()
            
            if not all_stocks:
                print("âŒ æœªå‘ç°ä»»ä½•è‚¡ç¥¨æ•°æ®")
                return
            
            # 2. è·å–è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯
            stock_info = self.get_stock_basic_info(all_stocks)
            
            # 3. é‡ç»„æ‰€æœ‰è‚¡ç¥¨æ•°æ®
            results = self.reorganize_all_stocks(all_stocks)
            
            # 4. ç”ŸæˆæŠ¥å‘Š
            report = self.generate_reorganization_report(results)
            
            # 5. æ€»ç»“
            end_time = datetime.now()
            duration = end_time - start_time
            
            print(f"\nğŸŠ é‡ç»„å®Œæˆï¼")
            print(f"â±ï¸ æ€»è€—æ—¶: {duration}")
            print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_path}")
            print(f"âœ… æˆåŠŸé‡ç»„: {report['summary']['success_count']} åªè‚¡ç¥¨")
            
        except Exception as e:
            self.logger.error(f"é‡ç»„è¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {str(e)}")
            raise

def main():
    """ä¸»å‡½æ•°"""
    reorganizer = IntelligentStockDataReorganizer()
    reorganizer.run_complete_reorganization()

if __name__ == "__main__":
    main()