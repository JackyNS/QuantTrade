#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¼˜å…ˆçº§ä¸‹è½½å™¨ - è¡Œæƒ…æ•°æ®(5ä¸ª) + èµ„é‡‘æµå‘(2ä¸ª)
é«˜ä¼˜å…ˆçº§æ ¸å¿ƒæ¥å£ï¼Œæ”¯æŒé‡åŒ–ç­–ç•¥å¼€å‘
"""

import uqer
import pandas as pd
from datetime import datetime
from pathlib import Path
import time
import logging

UQER_TOKEN = "68b9922817ae6273137bda7acba81e293582ba347281dfcc056dcb245b23faf3"

class PriorityMarketFlowDownloader:
    """ä¼˜å…ˆçº§å¸‚åœºæ•°æ®å’Œèµ„é‡‘æµå‘ä¸‹è½½å™¨"""
    
    def __init__(self):
        self.client = uqer.Client(token=UQER_TOKEN)
        self.data_dir = Path("data/priority_download")
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        # 7ä¸ªé«˜ä¼˜å…ˆçº§æ¥å£é…ç½®
        self.priority_apis = {
            # è¡Œæƒ…æ•°æ® (5ä¸ª) - æ”¯æŒæŠ€æœ¯åˆ†æå’Œè¶‹åŠ¿ç­–ç•¥
            "market_data": {
                "MktEqudGet": {
                    "desc": "è‚¡ç¥¨æ—¥è¡Œæƒ…", 
                    "dir": "daily",
                    "params": {"field": "secID,ticker,tradeDate,preClosePrice,openPrice,highestPrice,lowestPrice,closePrice,turnoverVol,marketValue,dealAmount,turnoverRate"},
                    "time_range": True
                },
                "MktEquwGet": {
                    "desc": "è‚¡ç¥¨å‘¨è¡Œæƒ…",
                    "dir": "weekly", 
                    "params": {"field": "secID,ticker,endDate,preClosePrice,openPrice,highestPrice,lowestPrice,closePrice,turnoverVol"},
                    "time_range": True
                },
                "MktEqumGet": {
                    "desc": "è‚¡ç¥¨æœˆè¡Œæƒ…",
                    "dir": "monthly",
                    "params": {"field": "secID,ticker,endDate,preClosePrice,openPrice,highestPrice,lowestPrice,closePrice,turnoverVol"},
                    "time_range": True
                },
                "MktEqudAdjGet": {
                    "desc": "å‰å¤æƒæ—¥è¡Œæƒ…",
                    "dir": "adj_daily",
                    "params": {"field": "secID,ticker,tradeDate,preClosePrice,openPrice,highestPrice,lowestPrice,closePrice,turnoverVol"},
                    "time_range": True
                },
                "MktAdjfGet": {
                    "desc": "å¤æƒå› å­",
                    "dir": "adj_factor",
                    "params": {"field": "secID,ticker,exDivDate,accumAdjFactor"},
                    "time_range": True
                }
            },
            
            # èµ„é‡‘æµå‘ (2ä¸ª) - æ”¯æŒèµ„é‡‘é¢ç­–ç•¥
            "flow_data": {
                "MktEquFlowGet": {
                    "desc": "ä¸ªè‚¡èµ„é‡‘æµå‘",
                    "dir": "stock_flow",
                    "params": {"field": "secID,ticker,tradeDate,buySmallAmount,buyMediumAmount,buyLargeAmount,buyExtraLargeAmount,sellSmallAmount,sellMediumAmount,sellLargeAmount,sellExtraLargeAmount"},
                    "time_range": True
                },
                "MktIndustryFlowGet": {
                    "desc": "è¡Œä¸šèµ„é‡‘æµå‘", 
                    "dir": "industry_flow",
                    "params": {"field": "industryID,industryName,tradeDate,buySmallAmount,buyMediumAmount,buyLargeAmount,buyExtraLargeAmount,sellSmallAmount,sellMediumAmount,sellLargeAmount,sellExtraLargeAmount"},
                    "time_range": True,
                    "no_ticker": True  # è¡Œä¸šæ•°æ®ä¸éœ€è¦tickerå‚æ•°
                }
            }
        }
        
        # é…ç½®æ—¥å¿—
        log_file = self.data_dir / "priority_download.log"
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )
        
    def get_stock_info(self):
        """è·å–è‚¡ç¥¨ä¿¡æ¯"""
        logging.info("ğŸ“‹ è·å–è‚¡ç¥¨åˆ—è¡¨...")
        
        try:
            stocks = uqer.DataAPI.EquGet(
                field='secID,ticker,secShortName,exchangeCD,listStatusCD,listDate'
            )
            
            if stocks is not None and not stocks.empty:
                a_stocks = stocks[stocks['listStatusCD'] == 'L'].copy()
                a_stocks['listDate'] = pd.to_datetime(a_stocks['listDate'])
                a_stocks['listYear'] = a_stocks['listDate'].dt.year
                
                logging.info(f"âœ… è·å–è‚¡ç¥¨ä¿¡æ¯: {len(a_stocks)} åªAè‚¡")
                return a_stocks
            
        except Exception as e:
            logging.error(f"âŒ è·å–è‚¡ç¥¨ä¿¡æ¯å¤±è´¥: {e}")
            return None
    
    def download_api_data(self, api_name, api_config, category, year=None, stocks=None):
        """ä¸‹è½½å•ä¸ªAPIæ•°æ®"""
        desc = api_config["desc"]
        data_dir = self.data_dir / category / api_config["dir"]
        data_dir.mkdir(parents=True, exist_ok=True)
        
        logging.info(f"ğŸ“¥ å¼€å§‹ä¸‹è½½ {desc}...")
        
        try:
            # è·å–APIå‡½æ•°
            api_func = getattr(uqer.DataAPI, api_name, None)
            if not api_func:
                logging.error(f"âŒ API {api_name} ä¸å­˜åœ¨")
                return False
            
            # æ„å»ºå‚æ•°
            params = api_config["params"].copy()
            
            if api_config.get("time_range") and year:
                # éœ€è¦æ—¶é—´èŒƒå›´çš„æ¥å£
                params["beginDate"] = f"{year}0101"
                params["endDate"] = f"{year}1231"
                
                if not api_config.get("no_ticker") and stocks is not None:
                    # éœ€è¦è‚¡ç¥¨ä»£ç çš„æ¥å£ï¼Œåˆ†æ‰¹ä¸‹è½½
                    return self._download_with_stocks(api_func, params, desc, data_dir, year, stocks)
                else:
                    # ä¸éœ€è¦è‚¡ç¥¨ä»£ç çš„æ¥å£ï¼ˆå¦‚è¡Œä¸šæ•°æ®ï¼‰
                    return self._download_without_stocks(api_func, params, desc, data_dir, year)
            else:
                # ä¸éœ€è¦æ—¶é—´èŒƒå›´çš„æ¥å£
                return self._download_static_data(api_func, params, desc, data_dir)
            
        except Exception as e:
            logging.error(f"âŒ {desc} ä¸‹è½½å¤±è´¥: {e}")
            return False
    
    def _download_with_stocks(self, api_func, params, desc, data_dir, year, stocks):
        """åˆ†æ‰¹ä¸‹è½½è‚¡ç¥¨æ•°æ®"""
        # ç­›é€‰è¯¥å¹´åº¦å·²ä¸Šå¸‚è‚¡ç¥¨
        year_stocks = stocks[stocks['listYear'] <= year].copy()
        
        if len(year_stocks) == 0:
            logging.warning(f"âš ï¸ {year}å¹´{desc}æ— å·²ä¸Šå¸‚è‚¡ç¥¨")
            return False
        
        # åˆ†æ‰¹ä¸‹è½½
        batch_size = 100
        batches = [year_stocks[i:i+batch_size] for i in range(0, len(year_stocks), batch_size)]
        
        total_records = 0
        success_count = 0
        
        for batch_idx, batch_stocks in enumerate(batches):
            batch_file = data_dir / f"{year}_batch_{batch_idx+1:03d}.csv"
            
            # è·³è¿‡å·²å­˜åœ¨æ–‡ä»¶
            if batch_file.exists():
                existing_data = pd.read_csv(batch_file)
                total_records += len(existing_data)
                success_count += 1
                continue
            
            try:
                # æ„å»ºtickeråˆ—è¡¨
                tickers = ','.join(batch_stocks['ticker'].tolist())
                params["ticker"] = tickers
                
                logging.info(f"ğŸ“¦ {desc} {year}å¹´ æ‰¹æ¬¡{batch_idx+1}/{len(batches)}: {len(batch_stocks)}åª")
                
                # è°ƒç”¨API
                data = api_func(**params)
                
                if data is not None and not data.empty:
                    data.to_csv(batch_file, index=False)
                    total_records += len(data)
                    success_count += 1
                    
                    logging.info(f"âœ… {desc} {year}å¹´ æ‰¹æ¬¡{batch_idx+1}: {len(data)}æ¡")
                else:
                    logging.warning(f"âš ï¸ {desc} {year}å¹´ æ‰¹æ¬¡{batch_idx+1}: æ•°æ®ä¸ºç©º")
                
                time.sleep(0.3)
                
            except Exception as e:
                logging.error(f"âŒ {desc} {year}å¹´ æ‰¹æ¬¡{batch_idx+1} å¤±è´¥: {e}")
                continue
        
        logging.info(f"ğŸ“Š {desc} {year}å¹´å®Œæˆ: {total_records}æ¡è®°å½•, {success_count}/{len(batches)}æ‰¹æ¬¡æˆåŠŸ")
        return total_records > 0
    
    def _download_without_stocks(self, api_func, params, desc, data_dir, year):
        """ä¸‹è½½ä¸éœ€è¦è‚¡ç¥¨ä»£ç çš„æ•°æ®ï¼ˆå¦‚è¡Œä¸šæ•°æ®ï¼‰"""
        file_path = data_dir / f"{year}_data.csv"
        
        # è·³è¿‡å·²å­˜åœ¨æ–‡ä»¶
        if file_path.exists():
            existing_data = pd.read_csv(file_path)
            logging.info(f"ğŸ“‚ {desc} {year}å¹´å·²å­˜åœ¨: {len(existing_data)}æ¡")
            return True
        
        try:
            logging.info(f"ğŸ“¥ {desc} {year}å¹´...")
            
            data = api_func(**params)
            
            if data is not None and not data.empty:
                data.to_csv(file_path, index=False)
                logging.info(f"âœ… {desc} {year}å¹´: {len(data)}æ¡è®°å½•")
                return True
            else:
                logging.warning(f"âš ï¸ {desc} {year}å¹´: æ•°æ®ä¸ºç©º")
                return False
                
        except Exception as e:
            logging.error(f"âŒ {desc} {year}å¹´å¤±è´¥: {e}")
            return False
    
    def _download_static_data(self, api_func, params, desc, data_dir):
        """ä¸‹è½½é™æ€æ•°æ®ï¼ˆä¸ä¾èµ–æ—¶é—´ï¼‰"""
        file_path = data_dir / "static_data.csv"
        
        # è·³è¿‡å·²å­˜åœ¨æ–‡ä»¶
        if file_path.exists():
            existing_data = pd.read_csv(file_path)
            logging.info(f"ğŸ“‚ {desc}å·²å­˜åœ¨: {len(existing_data)}æ¡")
            return True
        
        try:
            logging.info(f"ğŸ“¥ {desc}...")
            
            data = api_func(**params)
            
            if data is not None and not data.empty:
                data.to_csv(file_path, index=False)
                logging.info(f"âœ… {desc}: {len(data)}æ¡è®°å½•")
                return True
            else:
                logging.warning(f"âš ï¸ {desc}: æ•°æ®ä¸ºç©º")
                return False
                
        except Exception as e:
            logging.error(f"âŒ {desc}å¤±è´¥: {e}")
            return False
    
    def download_all_priority_data(self, start_year=2020, end_year=None):
        """ä¸‹è½½æ‰€æœ‰ä¼˜å…ˆçº§æ•°æ®"""
        if end_year is None:
            end_year = datetime.now().year
        
        logging.info("ğŸš€ å¼€å§‹ä¸‹è½½é«˜ä¼˜å…ˆçº§æ•°æ®")
        logging.info("ğŸ“‹ åŒ…å«: è¡Œæƒ…æ•°æ®(5ä¸ª) + èµ„é‡‘æµå‘(2ä¸ª)")
        
        # è·å–è‚¡ç¥¨ä¿¡æ¯
        stocks = self.get_stock_info()
        if stocks is None:
            return False
        
        total_success = 0
        total_apis = sum(len(category) for category in self.priority_apis.values())
        
        # æŒ‰å¹´ä»½ä¸‹è½½
        for year in range(start_year, end_year + 1):
            logging.info(f"\n{'='*60}")
            logging.info(f"ğŸ“… ä¸‹è½½ {year} å¹´æ•°æ®")
            logging.info(f"{'='*60}")
            
            year_success = 0
            
            # ä¸‹è½½å„ç±»æ•°æ®
            for category, apis in self.priority_apis.items():
                logging.info(f"\nğŸ“‚ {category} ç±»åˆ«æ•°æ®:")
                
                for api_name, api_config in apis.items():
                    success = self.download_api_data(api_name, api_config, category, year, stocks)
                    if success:
                        year_success += 1
                        total_success += 1
                    
                    time.sleep(0.2)
            
            logging.info(f"ğŸ“Š {year}å¹´å®Œæˆ: {year_success}/{len(self.priority_apis['market_data']) + len(self.priority_apis['flow_data'])}ä¸ªæ¥å£æˆåŠŸ")
            time.sleep(1)
        
        # æ€»ç»“
        logging.info(f"\nğŸ‰ ä¼˜å…ˆçº§æ•°æ®ä¸‹è½½å®Œæˆ!")
        logging.info(f"ğŸ“Š æ€»æˆåŠŸ: {total_success}/{total_apis * (end_year - start_year + 1)}ä¸ªæ¥å£")
        
        return True
    
    def show_download_plan(self):
        """æ˜¾ç¤ºä¸‹è½½è®¡åˆ’"""
        print("ğŸ¯ ä¼˜å…ˆçº§æ•°æ®ä¸‹è½½è®¡åˆ’")
        print("=" * 50)
        
        total_apis = 0
        for category, apis in self.priority_apis.items():
            print(f"\nğŸ“‚ {category}:")
            for api_name, config in apis.items():
                print(f"   âœ… {config['desc']}")
                total_apis += 1
        
        print(f"\nğŸ“Š æ€»æ¥å£æ•°: {total_apis} ä¸ª")
        print(f"ğŸ’¾ æ•°æ®ä¿å­˜: {self.data_dir}")

def main():
    """ä¸»å‡½æ•°"""
    downloader = PriorityMarketFlowDownloader()
    
    downloader.show_download_plan()
    
    print("\né€‰æ‹©æ“ä½œ:")
    print("1. ä¸‹è½½æœ€è¿‘5å¹´æ•°æ® (2020-2025)")
    print("2. ä¸‹è½½æœ€è¿‘3å¹´æ•°æ® (2022-2025)")
    print("3. ä¸‹è½½å…¨éƒ¨å†å²æ•°æ® (2000-2025)")
    
    choice = input("è¯·é€‰æ‹© (1-3): ").strip()
    
    if choice == "1":
        downloader.download_all_priority_data(2020)
    elif choice == "2":
        downloader.download_all_priority_data(2022)
    elif choice == "3":
        downloader.download_all_priority_data(2000)
    else:
        print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œé»˜è®¤ä¸‹è½½æœ€è¿‘5å¹´æ•°æ®")
        downloader.download_all_priority_data(2020)

if __name__ == "__main__":
    main()