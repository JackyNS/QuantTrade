#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¼˜çŸ¿æ ¸å¿ƒæ•°æ®ä¼˜å…ˆä¸‹è½½å™¨
====================

åŸºäºä¼˜çŸ¿api2025.txtå’Œcoreæ¨¡å—éœ€æ±‚ï¼Œä¼˜å…ˆä¸‹è½½æœ€é‡è¦çš„æ•°æ®ï¼š

1. ã€æ ¸å¿ƒè¡Œæƒ…æ•°æ®ã€‘- ç­–ç•¥å’Œå›æµ‹çš„åŸºç¡€
   - getMktEqud: è‚¡ç¥¨æ—¥è¡Œæƒ… [å…è´¹ç‰ˆ]
   - getMktEqudAdj: è‚¡ç¥¨å‰å¤æƒæ—¥è¡Œæƒ… [å…è´¹ç‰ˆ] 
   - getMktEquw: è‚¡ç¥¨å‘¨è¡Œæƒ… [å…è´¹ç‰ˆ]
   - getMktEqum: è‚¡ç¥¨æœˆè¡Œæƒ… [å…è´¹ç‰ˆ]

2. ã€è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯ã€‘- ç­›é€‰å’Œåˆ†æçš„åŸºç¡€
   - getEqu: è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯ [å…è´¹ç‰ˆ]
   - getEquIPO: è‚¡ç¥¨é¦–æ¬¡ä¸Šå¸‚ä¿¡æ¯ [å…è´¹ç‰ˆ]
   - getEquIndustry: è‚¡ç¥¨è¡Œä¸šåˆ†ç±» [å…è´¹ç‰ˆ]
   - getSecID: è¯åˆ¸ç¼–ç åŠåŸºæœ¬ä¸Šå¸‚ä¿¡æ¯ [å…è´¹ç‰ˆ]

3. ã€æŒ‡æ•°æ•°æ®ã€‘- åŸºå‡†å¯¹æ¯”
   - getMktIdxd: æŒ‡æ•°æ—¥è¡Œæƒ… [å…è´¹ç‰ˆ]
   - getIdx: æŒ‡æ•°åŸºæœ¬ä¿¡æ¯ [å…è´¹ç‰ˆ]

4. ã€äº¤æ˜“æ—¥å†ã€‘- æ—¶é—´ç®¡ç†
   - getTradeCal: äº¤æ˜“æ‰€äº¤æ˜“æ—¥å† [å…è´¹ç‰ˆ]

ç­–ç•¥ï¼š
- æŒ‰é‡è¦æ€§ä¼˜å…ˆçº§ä¸‹è½½
- é‡‡ç”¨å¢é‡ä¸‹è½½æ–¹å¼
- ç»Ÿä¸€å­˜å‚¨ç»“æ„
- è‡ªåŠ¨æ•°æ®éªŒè¯
"""

import uqer
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime, timedelta
import time
import logging
import json
import warnings
from typing import Dict, List, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

warnings.filterwarnings('ignore')

# ä¼˜çŸ¿Token
UQER_TOKEN = "68b9922817ae6273137bda7acba81e293582ba347281dfcc056dcb245b23faf3"

class PriorityUqerDataDownloader:
    """ä¼˜çŸ¿æ ¸å¿ƒæ•°æ®ä¼˜å…ˆä¸‹è½½å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–ä¸‹è½½å™¨"""
        self.client = uqer.Client(token=UQER_TOKEN)
        
        # æ•°æ®ç›®å½•
        self.base_path = Path("/Users/jackstudio/QuantTrade/data")
        self.base_path.mkdir(exist_ok=True)
        
        # è®¾ç½®æ—¥å¿—
        self.setup_logging()
        
        # ä¼˜å…ˆä¸‹è½½APIé…ç½®
        self.priority_apis = self._define_priority_apis()
        
        # ä¸‹è½½ç»Ÿè®¡
        self.download_stats = {
            'total_calls': 0,
            'successful_calls': 0,
            'failed_calls': 0,
            'total_records': 0,
            'start_time': None,
            'end_time': None
        }
        
        # çº¿ç¨‹é”
        self.stats_lock = threading.Lock()
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        log_file = self.base_path / "priority_download.log"
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def _define_priority_apis(self) -> Dict:
        """å®šä¹‰ä¼˜å…ˆä¸‹è½½çš„APIé…ç½®"""
        return {
            # ç¬¬1ä¼˜å…ˆçº§ï¼šæ ¸å¿ƒè¡Œæƒ…æ•°æ®
            "priority_1_market_data": {
                "description": "æ ¸å¿ƒè¡Œæƒ…æ•°æ® - ç­–ç•¥å›æµ‹åŸºç¡€",
                "apis": {
                    "getMktEqud": {
                        "name": "è‚¡ç¥¨æ—¥è¡Œæƒ…",
                        "package": "å…è´¹ç‰ˆ",
                        "dir": "daily",
                        "fields": "secID,ticker,tradeDate,preClosePrice,openPrice,highestPrice,lowestPrice,closePrice,turnoverVol,marketValue,dealAmount,turnoverRate",
                        "time_based": True,
                        "batch_size": 100
                    },
                    "getMktEqudAdj": {
                        "name": "è‚¡ç¥¨å‰å¤æƒæ—¥è¡Œæƒ…", 
                        "package": "å…è´¹ç‰ˆ",
                        "dir": "daily_adj",
                        "fields": "secID,ticker,tradeDate,preClosePrice,openPrice,highestPrice,lowestPrice,closePrice,turnoverVol,accumAdjFactor",
                        "time_based": True,
                        "batch_size": 100
                    }
                }
            },
            
            # ç¬¬2ä¼˜å…ˆçº§ï¼šåŸºæœ¬ä¿¡æ¯æ•°æ®
            "priority_2_basic_info": {
                "description": "è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯ - ç­›é€‰åˆ†æåŸºç¡€",
                "apis": {
                    "getEqu": {
                        "name": "è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯",
                        "package": "å…è´¹ç‰ˆ", 
                        "dir": "basic_info",
                        "fields": "secID,ticker,secShortName,exchangeCD,listStatusCD,listDate,delistDate,secFullName",
                        "time_based": False,
                        "batch_size": 1000
                    },
                    "getEquIPO": {
                        "name": "è‚¡ç¥¨é¦–æ¬¡ä¸Šå¸‚ä¿¡æ¯",
                        "package": "å…è´¹ç‰ˆ",
                        "dir": "basic_info", 
                        "fields": "secID,ticker,listDate,issuePrice,issueNum,parValue,floatSharesNum",
                        "time_based": False,
                        "batch_size": 1000
                    },
                    "getEquIndustry": {
                        "name": "è‚¡ç¥¨è¡Œä¸šåˆ†ç±»",
                        "package": "å…è´¹ç‰ˆ",
                        "dir": "basic_info",
                        "fields": "secID,ticker,industryID1,industryName1,industryID2,industryName2",
                        "time_based": False,
                        "batch_size": 1000
                    },
                    "getSecID": {
                        "name": "è¯åˆ¸ç¼–ç åŸºæœ¬ä¿¡æ¯",
                        "package": "å…è´¹ç‰ˆ", 
                        "dir": "basic_info",
                        "fields": "secID,ticker,secShortName,exchangeCD,listDate,delistDate",
                        "time_based": False,
                        "batch_size": 1000
                    }
                }
            },
            
            # ç¬¬3ä¼˜å…ˆçº§ï¼šå‘¨æœˆè¡Œæƒ…
            "priority_3_extended_market": {
                "description": "å‘¨æœˆè¡Œæƒ…æ•°æ®",
                "apis": {
                    "getMktEquw": {
                        "name": "è‚¡ç¥¨å‘¨è¡Œæƒ…",
                        "package": "å…è´¹ç‰ˆ",
                        "dir": "weekly",
                        "fields": "secID,ticker,tradeDate,preClosePrice,openPrice,highestPrice,lowestPrice,closePrice,turnoverVol,marketValue",
                        "time_based": True,
                        "batch_size": 100
                    },
                    "getMktEqum": {
                        "name": "è‚¡ç¥¨æœˆè¡Œæƒ…", 
                        "package": "å…è´¹ç‰ˆ",
                        "dir": "monthly",
                        "fields": "secID,ticker,tradeDate,preClosePrice,openPrice,highestPrice,lowestPrice,closePrice,turnoverVol,marketValue",
                        "time_based": True,
                        "batch_size": 100
                    }
                }
            },
            
            # ç¬¬4ä¼˜å…ˆçº§ï¼šæŒ‡æ•°æ•°æ®
            "priority_4_index_data": {
                "description": "æŒ‡æ•°æ•°æ® - åŸºå‡†å¯¹æ¯”",
                "apis": {
                    "getMktIdxd": {
                        "name": "æŒ‡æ•°æ—¥è¡Œæƒ…",
                        "package": "å…è´¹ç‰ˆ",
                        "dir": "index_daily",
                        "fields": "secID,ticker,tradeDate,preClosePrice,openPrice,highestPrice,lowestPrice,closePrice,turnoverVol,marketValue",
                        "time_based": True,
                        "batch_size": 50,
                        "no_stock_filter": True
                    },
                    "getIdx": {
                        "name": "æŒ‡æ•°åŸºæœ¬ä¿¡æ¯",
                        "package": "å…è´¹ç‰ˆ",
                        "dir": "basic_info",
                        "fields": "secID,ticker,secShortName,exchangeCD,listDate",
                        "time_based": False,
                        "batch_size": 500,
                        "no_stock_filter": True
                    }
                }
            },
            
            # ç¬¬5ä¼˜å…ˆçº§ï¼šäº¤æ˜“æ—¥å†
            "priority_5_calendar": {
                "description": "äº¤æ˜“æ—¥å† - æ—¶é—´ç®¡ç†", 
                "apis": {
                    "getTradeCal": {
                        "name": "äº¤æ˜“æ‰€äº¤æ˜“æ—¥å†",
                        "package": "å…è´¹ç‰ˆ",
                        "dir": "calendar",
                        "fields": "calendarDate,exchangeCD,prevTradeDate,isOpen",
                        "time_based": True,
                        "batch_size": 1,
                        "no_stock_filter": True,
                        "special_handling": True
                    }
                }
            }
        }
    
    def get_all_stock_codes(self) -> List[str]:
        """è·å–æ‰€æœ‰Aè‚¡ä»£ç åˆ—è¡¨"""
        try:
            self.logger.info("ğŸ“‹ è·å–Aè‚¡ä»£ç åˆ—è¡¨...")
            
            # è°ƒç”¨åŸºæœ¬ä¿¡æ¯APIè·å–è‚¡ç¥¨åˆ—è¡¨
            result = self.client.DataAPI.EquGet(
                listStatusCD='L,S,DE',  # ä¸Šå¸‚ã€åœç‰Œã€é€€å¸‚è‚¡ç¥¨
                field='secID,ticker,listStatusCD',
                pandas='1'
            )
            
            if isinstance(result, pd.DataFrame) and not result.empty:
                stock_codes = result['secID'].unique().tolist()
                self.logger.info(f"âœ… è·å–åˆ° {len(stock_codes)} åªAè‚¡ä»£ç ")
                return stock_codes
            else:
                self.logger.error("è·å–è‚¡ç¥¨ä»£ç å¤±è´¥")
                return []
                
        except Exception as e:
            self.logger.error(f"è·å–è‚¡ç¥¨ä»£ç å¼‚å¸¸: {str(e)}")
            return []\n    \n    def create_time_ranges(self, start_year: int = 2000, end_year: int = 2025) -> List[Tuple[str, str]]:\n        \"\"\"åˆ›å»ºæ—¶é—´èŒƒå›´åˆ—è¡¨\"\"\"\n        time_ranges = []\n        \n        for year in range(start_year, end_year + 1):\n            if year == 2025:\n                # 2025å¹´åªåˆ°8æœˆåº•\n                time_ranges.append((f\"{year}0101\", \"20250831\"))\n            else:\n                time_ranges.append((f\"{year}0101\", f\"{year}1231\"))\n        \n        return time_ranges\n    \n    def download_api_data(self, api_name: str, api_config: Dict, stock_codes: List[str], time_ranges: List[Tuple[str, str]]) -> bool:\n        \"\"\"ä¸‹è½½å•ä¸ªAPIçš„æ•°æ®\"\"\"\n        try:\n            self.logger.info(f\"ğŸš€ å¼€å§‹ä¸‹è½½ {api_config['name']} ({api_name})\")\n            self.logger.info(f\"ğŸ“¦ æ•°æ®åŒ…: {api_config['package']}\")\n            \n            # åˆ›å»ºè¾“å‡ºç›®å½•\n            output_dir = self.base_path / api_config['dir']\n            output_dir.mkdir(exist_ok=True)\n            \n            # è·å–APIå‡½æ•°\n            api_func = getattr(self.client.DataAPI, api_name, None)\n            if not api_func:\n                self.logger.error(f\"APIå‡½æ•° {api_name} ä¸å­˜åœ¨\")\n                return False\n            \n            total_records = 0\n            \n            # å¤„ç†ç‰¹æ®ŠAPI\n            if api_config.get('special_handling'):\n                return self._handle_special_api(api_name, api_config, api_func, output_dir)\n            \n            # å¤„ç†é™æ€æ•°æ®ï¼ˆä¸åŸºäºæ—¶é—´ï¼‰\n            if not api_config.get('time_based', True):\n                return self._download_static_data(api_name, api_config, api_func, output_dir, stock_codes)\n            \n            # å¤„ç†æ—¶é—´åºåˆ—æ•°æ®\n            return self._download_time_series_data(api_name, api_config, api_func, output_dir, stock_codes, time_ranges)\n        \n        except Exception as e:\n            self.logger.error(f\"ä¸‹è½½ {api_name} æ•°æ®å¼‚å¸¸: {str(e)}\")\n            return False\n    \n    def _handle_special_api(self, api_name: str, api_config: Dict, api_func, output_dir: Path) -> bool:\n        \"\"\"å¤„ç†ç‰¹æ®ŠAPIï¼ˆå¦‚äº¤æ˜“æ—¥å†ï¼‰\"\"\"\n        try:\n            if api_name == 'getTradeCal':\n                self.logger.info(\"ğŸ“… ä¸‹è½½äº¤æ˜“æ—¥å†...\")\n                \n                # ä¸‹è½½2000-2025å¹´çš„äº¤æ˜“æ—¥å†\n                result = api_func(\n                    exchangeCD='XSHG,XSHE',\n                    beginDate='20000101',\n                    endDate='20251231',\n                    field=api_config['fields'],\n                    pandas='1'\n                )\n                \n                if isinstance(result, pd.DataFrame) and not result.empty:\n                    output_file = output_dir / \"trading_calendar.csv\"\n                    result.to_csv(output_file, index=False)\n                    \n                    with self.stats_lock:\n                        self.download_stats['successful_calls'] += 1\n                        self.download_stats['total_records'] += len(result)\n                    \n                    self.logger.info(f\"âœ… äº¤æ˜“æ—¥å†ä¸‹è½½å®Œæˆ: {len(result)} æ¡è®°å½•\")\n                    return True\n                else:\n                    self.logger.error(\"äº¤æ˜“æ—¥å†æ•°æ®ä¸ºç©º\")\n                    return False\n            \n            return True\n            \n        except Exception as e:\n            self.logger.error(f\"ç‰¹æ®ŠAPI {api_name} å¤„ç†å¼‚å¸¸: {str(e)}\")\n            return False\n    \n    def _download_static_data(self, api_name: str, api_config: Dict, api_func, output_dir: Path, stock_codes: List[str]) -> bool:\n        \"\"\"ä¸‹è½½é™æ€æ•°æ®ï¼ˆä¸åŸºäºæ—¶é—´ï¼‰\"\"\"\n        try:\n            self.logger.info(f\"ğŸ“Š ä¸‹è½½é™æ€æ•°æ® {api_config['name']}...\")\n            \n            # æ— éœ€è‚¡ç¥¨ç­›é€‰çš„API\n            if api_config.get('no_stock_filter'):\n                result = api_func(\n                    field=api_config['fields'],\n                    pandas='1'\n                )\n                \n                if isinstance(result, pd.DataFrame) and not result.empty:\n                    output_file = output_dir / f\"{api_name.lower()}.csv\"\n                    result.to_csv(output_file, index=False)\n                    \n                    with self.stats_lock:\n                        self.download_stats['successful_calls'] += 1\n                        self.download_stats['total_records'] += len(result)\n                    \n                    self.logger.info(f\"âœ… {api_config['name']} ä¸‹è½½å®Œæˆ: {len(result)} æ¡è®°å½•\")\n                    return True\n            else:\n                # éœ€è¦è‚¡ç¥¨ç­›é€‰çš„APIï¼Œæ‰¹é‡ä¸‹è½½\n                batch_size = api_config.get('batch_size', 100)\n                total_records = 0\n                \n                for i in range(0, len(stock_codes), batch_size):\n                    batch_stocks = stock_codes[i:i+batch_size]\n                    batch_tickers = ','.join([code.split('.')[0] for code in batch_stocks])\n                    \n                    try:\n                        result = api_func(\n                            secID='',\n                            ticker=batch_tickers,\n                            field=api_config['fields'],\n                            pandas='1'\n                        )\n                        \n                        if isinstance(result, pd.DataFrame) and not result.empty:\n                            # è¿½åŠ ä¿å­˜\n                            output_file = output_dir / f\"{api_name.lower()}.csv\"\n                            if output_file.exists():\n                                result.to_csv(output_file, mode='a', header=False, index=False)\n                            else:\n                                result.to_csv(output_file, index=False)\n                            \n                            total_records += len(result)\n                            \n                            with self.stats_lock:\n                                self.download_stats['successful_calls'] += 1\n                                self.download_stats['total_records'] += len(result)\n                        \n                        # é¿å…é¢‘ç‡é™åˆ¶\n                        time.sleep(0.1)\n                        \n                        if (i // batch_size + 1) % 10 == 0:\n                            self.logger.info(f\"   è¿›åº¦: {i+batch_size}/{len(stock_codes)} è‚¡ç¥¨\")\n                            \n                    except Exception as e:\n                        self.logger.warning(f\"æ‰¹æ¬¡ {i//batch_size + 1} ä¸‹è½½å¤±è´¥: {str(e)}\")\n                        with self.stats_lock:\n                            self.download_stats['failed_calls'] += 1\n                        continue\n                \n                self.logger.info(f\"âœ… {api_config['name']} ä¸‹è½½å®Œæˆ: {total_records} æ¡è®°å½•\")\n                return True\n                \n        except Exception as e:\n            self.logger.error(f\"é™æ€æ•°æ® {api_name} ä¸‹è½½å¼‚å¸¸: {str(e)}\")\n            return False\n    \n    def _download_time_series_data(self, api_name: str, api_config: Dict, api_func, output_dir: Path, \n                                  stock_codes: List[str], time_ranges: List[Tuple[str, str]]) -> bool:\n        \"\"\"ä¸‹è½½æ—¶é—´åºåˆ—æ•°æ®\"\"\"\n        try:\n            self.logger.info(f\"ğŸ“ˆ ä¸‹è½½æ—¶é—´åºåˆ—æ•°æ® {api_config['name']}...\")\n            \n            batch_size = api_config.get('batch_size', 100)\n            total_records = 0\n            \n            # æŒ‰å¹´ä»½åˆ†åˆ«ä¸‹è½½\n            for start_date, end_date in time_ranges:\n                year = start_date[:4]\n                self.logger.info(f\"   ğŸ“… ä¸‹è½½ {year} å¹´æ•°æ®...\")\n                \n                year_records = 0\n                \n                # æ— éœ€è‚¡ç¥¨ç­›é€‰çš„APIï¼ˆå¦‚æŒ‡æ•°ï¼‰\n                if api_config.get('no_stock_filter'):\n                    try:\n                        result = api_func(\n                            beginDate=start_date,\n                            endDate=end_date,\n                            field=api_config['fields'],\n                            pandas='1'\n                        )\n                        \n                        if isinstance(result, pd.DataFrame) and not result.empty:\n                            output_file = output_dir / f\"{api_name.lower()}_{year}.csv\"\n                            result.to_csv(output_file, index=False)\n                            \n                            year_records = len(result)\n                            total_records += year_records\n                            \n                            with self.stats_lock:\n                                self.download_stats['successful_calls'] += 1\n                                self.download_stats['total_records'] += year_records\n                        \n                        self.logger.info(f\"   âœ… {year}å¹´: {year_records} æ¡è®°å½•\")\n                        \n                    except Exception as e:\n                        self.logger.warning(f\"{year}å¹´æ•°æ®ä¸‹è½½å¤±è´¥: {str(e)}\")\n                        with self.stats_lock:\n                            self.download_stats['failed_calls'] += 1\n                \n                else:\n                    # éœ€è¦è‚¡ç¥¨ç­›é€‰çš„APIï¼Œåˆ†æ‰¹ä¸‹è½½\n                    year_output_file = output_dir / f\"{api_name.lower()}_{year}.csv\"\n                    \n                    for i in range(0, len(stock_codes), batch_size):\n                        batch_stocks = stock_codes[i:i+batch_size]\n                        batch_tickers = ','.join([code.split('.')[0] for code in batch_stocks])\n                        \n                        try:\n                            result = api_func(\n                                secID='',\n                                ticker=batch_tickers,\n                                beginDate=start_date,\n                                endDate=end_date,\n                                field=api_config['fields'],\n                                pandas='1'\n                            )\n                            \n                            if isinstance(result, pd.DataFrame) and not result.empty:\n                                # è¿½åŠ ä¿å­˜åˆ°å¹´åº¦æ–‡ä»¶\n                                if year_output_file.exists():\n                                    result.to_csv(year_output_file, mode='a', header=False, index=False)\n                                else:\n                                    result.to_csv(year_output_file, index=False)\n                                \n                                batch_records = len(result)\n                                year_records += batch_records\n                                \n                                with self.stats_lock:\n                                    self.download_stats['successful_calls'] += 1\n                                    self.download_stats['total_records'] += batch_records\n                            \n                            # é¿å…é¢‘ç‡é™åˆ¶\n                            time.sleep(0.1)\n                            \n                        except Exception as e:\n                            self.logger.warning(f\"{year}å¹´æ‰¹æ¬¡ {i//batch_size + 1} ä¸‹è½½å¤±è´¥: {str(e)}\")\n                            with self.stats_lock:\n                                self.download_stats['failed_calls'] += 1\n                            continue\n                    \n                    self.logger.info(f\"   âœ… {year}å¹´: {year_records} æ¡è®°å½•\")\n                \n                total_records += year_records\n                \n                # å¹´ä»½ä¹‹é—´ç¨ä½œåœé¡¿\n                time.sleep(0.5)\n            \n            self.logger.info(f\"âœ… {api_config['name']} å…¨éƒ¨ä¸‹è½½å®Œæˆ: {total_records} æ¡è®°å½•\")\n            return True\n            \n        except Exception as e:\n            self.logger.error(f\"æ—¶é—´åºåˆ—æ•°æ® {api_name} ä¸‹è½½å¼‚å¸¸: {str(e)}\")\n            return False\n    \n    def run_priority_download(self):\n        \"\"\"è¿è¡Œä¼˜å…ˆä¸‹è½½æµç¨‹\"\"\"\n        start_time = datetime.now()\n        self.download_stats['start_time'] = start_time\n        \n        print(\"ğŸš€ ä¼˜çŸ¿æ ¸å¿ƒæ•°æ®ä¼˜å…ˆä¸‹è½½å™¨\")\n        print(\"ğŸ¯ ç›®æ ‡: ä¸‹è½½coreæ¨¡å—å¿…éœ€çš„å…³é”®æ•°æ®\")\n        print(\"=\" * 80)\n        \n        try:\n            # 1. è·å–æ‰€æœ‰è‚¡ç¥¨ä»£ç \n            stock_codes = self.get_all_stock_codes()\n            if not stock_codes:\n                print(\"âŒ æ— æ³•è·å–è‚¡ç¥¨ä»£ç åˆ—è¡¨\")\n                return\n            \n            print(f\"ğŸ“Š Aè‚¡æ€»æ•°: {len(stock_codes)}\")\n            \n            # 2. åˆ›å»ºæ—¶é—´èŒƒå›´\n            time_ranges = self.create_time_ranges()\n            print(f\"ğŸ“… æ—¶é—´èŒƒå›´: {len(time_ranges)} å¹´ (2000-2025)\")\n            \n            # 3. æŒ‰ä¼˜å…ˆçº§ä¸‹è½½æ•°æ®\n            for priority_group, group_config in self.priority_apis.items():\n                print(f\"\\n{priority_group.upper()}: {group_config['description']}\")\n                print(\"=\" * 60)\n                \n                for api_name, api_config in group_config['apis'].items():\n                    success = self.download_api_data(api_name, api_config, stock_codes, time_ranges)\n                    \n                    if success:\n                        print(f\"âœ… {api_config['name']} ä¸‹è½½æˆåŠŸ\")\n                    else:\n                        print(f\"âŒ {api_config['name']} ä¸‹è½½å¤±è´¥\")\n                    \n                    # APIä¹‹é—´ç¨ä½œåœé¡¿\n                    time.sleep(1)\n            \n            # 4. ç”Ÿæˆä¸‹è½½æŠ¥å‘Š\n            self.generate_download_report()\n            \n        except Exception as e:\n            self.logger.error(f\"ä¸‹è½½æµç¨‹å¼‚å¸¸: {str(e)}\")\n            raise\n        \n        finally:\n            end_time = datetime.now()\n            self.download_stats['end_time'] = end_time\n            duration = end_time - start_time\n            \n            print(f\"\\nğŸŠ ä¸‹è½½å®Œæˆ!\")\n            print(f\"â±ï¸ æ€»è€—æ—¶: {duration}\")\n            print(f\"ğŸ“Š APIè°ƒç”¨: {self.download_stats['successful_calls']} æˆåŠŸ, {self.download_stats['failed_calls']} å¤±è´¥\")\n            print(f\"ğŸ“‹ æ€»è®°å½•æ•°: {self.download_stats['total_records']:,}\")\n    \n    def generate_download_report(self):\n        \"\"\"ç”Ÿæˆä¸‹è½½æŠ¥å‘Š\"\"\"\n        report = {\n            'download_time': datetime.now().isoformat(),\n            'statistics': self.download_stats.copy(),\n            'api_summary': {},\n            'data_structure': {}\n        }\n        \n        # ç»Ÿè®¡å„ç›®å½•çš„æ–‡ä»¶\n        for priority_group, group_config in self.priority_apis.items():\n            report['api_summary'][priority_group] = {\n                'description': group_config['description'],\n                'apis': list(group_config['apis'].keys())\n            }\n        \n        # ç»Ÿè®¡æ•°æ®ç›®å½•\n        for item in self.base_path.iterdir():\n            if item.is_dir():\n                files = list(item.glob(\"*.csv\"))\n                if files:\n                    total_size = sum(f.stat().st_size for f in files)\n                    report['data_structure'][item.name] = {\n                        'file_count': len(files),\n                        'size_mb': round(total_size / (1024 * 1024), 2)\n                    }\n        \n        # ä¿å­˜æŠ¥å‘Š\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        report_file = self.base_path / f\"priority_download_report_{timestamp}.json\"\n        \n        with open(report_file, 'w', encoding='utf-8') as f:\n            json.dump(report, f, indent=2, ensure_ascii=False, default=str)\n        \n        print(f\"\\nğŸ“„ ä¸‹è½½æŠ¥å‘Šå·²ä¿å­˜: {report_file}\")\n        \n        # æ˜¾ç¤ºæ•°æ®ç»“æ„æ€»ç»“\n        print(f\"\\nğŸ“ æ•°æ®ç›®å½•ç»“æ„:\")\n        for dir_name, dir_info in report['data_structure'].items():\n            print(f\"   ğŸ“‚ {dir_name}: {dir_info['file_count']} æ–‡ä»¶, {dir_info['size_mb']} MB\")\n\ndef main():\n    \"\"\"ä¸»å‡½æ•°\"\"\"\n    downloader = PriorityUqerDataDownloader()\n    downloader.run_priority_download()\n\nif __name__ == \"__main__\":\n    main()