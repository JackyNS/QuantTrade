#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸“ä¸šçº§æ•°æ®ä¸‹è½½å™¨ - åˆ©ç”¨ä¼˜çŸ¿æé€Ÿç‰ˆå®Œæ•´APIæƒé™
åŸºäºä¼˜çŸ¿2025 APIæ¸…å•çš„268ä¸ªæ¥å£è¿›è¡Œå…¨é¢æ•°æ®è¡¥å…¨
"""

import sys
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime, timedelta
import warnings
import json
import time
import os
from io import StringIO
import concurrent.futures
from threading import Lock
warnings.filterwarnings('ignore')

try:
    import uqer
    print("âœ… UQER API å¯ç”¨")
    UQER_AVAILABLE = True
except ImportError:
    print("âŒ UQER API ä¸å¯ç”¨")
    UQER_AVAILABLE = False
    sys.exit(1)

class ProfessionalDataDownloader:
    """ä¸“ä¸šçº§æ•°æ®ä¸‹è½½å™¨ - æé€Ÿç‰ˆæƒé™"""
    
    def __init__(self):
        """åˆå§‹åŒ–ä¸‹è½½å™¨"""
        self.setup_uqer()
        self.setup_paths()
        self.load_api_mappings()
        self.download_stats = {
            'start_time': datetime.now(),
            'total_stocks': 0,
            'successful_downloads': 0,
            'failed_downloads': 0,
            'total_records': 0,
            'data_size_mb': 0,
            'api_calls': 0
        }
        self.progress_lock = Lock()
        
    def setup_uqer(self):
        """è®¾ç½®UQERè¿æ¥"""
        try:
            uqer_token = "68b9922817ae6273137bda7acba81e293582ba347281dfcc056dcb245b23faf3"
            uqer.Client(token=uqer_token)
            print("âœ… UQERæé€Ÿç‰ˆè¿æ¥æˆåŠŸ")
            self.uqer_connected = True
        except Exception as e:
            print(f"âŒ UQERè¿æ¥å¤±è´¥: {e}")
            self.uqer_connected = False
            sys.exit(1)
    
    def setup_paths(self):
        """è®¾ç½®å­˜å‚¨è·¯å¾„"""
        self.base_path = Path("/Users/jackstudio/QuantTrade/data/professional_complete")
        self.base_path.mkdir(exist_ok=True)
        
        # æŒ‰æ•°æ®ç±»å‹ç»„ç»‡
        self.data_paths = {
            'daily': self.base_path / "daily" / "stocks",
            'daily_adj': self.base_path / "daily_adj" / "stocks",  # å‰å¤æƒæ—¥çº¿
            'weekly': self.base_path / "weekly" / "stocks",
            'weekly_adj': self.base_path / "weekly_adj" / "stocks",  # å‰å¤æƒå‘¨çº¿
            'monthly': self.base_path / "monthly" / "stocks", 
            'monthly_adj': self.base_path / "monthly_adj" / "stocks",  # å‰å¤æƒæœˆçº¿
            'quarterly': self.base_path / "quarterly" / "stocks",
            'yearly': self.base_path / "yearly" / "stocks",
            'factors': self.base_path / "factors" / "stocks",  # å› å­æ•°æ®
            'basic_info': self.base_path / "basic_info",  # è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯
        }
        
        # åˆ›å»ºæ‰€æœ‰ç›®å½•
        for path in self.data_paths.values():
            path.mkdir(parents=True, exist_ok=True)
        
        print(f"ğŸ“ ä¸“ä¸šæ•°æ®å­˜å‚¨è·¯å¾„: {self.base_path}")
        for data_type, path in self.data_paths.items():
            print(f"   ğŸ“Š {data_type}: {path}")
    
    def load_api_mappings(self):
        """åŠ è½½APIæ˜ å°„å…³ç³»"""
        self.api_mappings = {
            'daily': 'MktEqudGet',          # æ—¥çº¿
            'daily_adj': 'MktEqudAdjGet',   # å‰å¤æƒæ—¥çº¿ [æé€Ÿç‰ˆ]
            'weekly': 'MktEquwGet',         # å‘¨çº¿
            'weekly_adj': 'MktEquwAdjGet',  # å‰å¤æƒå‘¨çº¿ [æé€Ÿç‰ˆ]
            'monthly': 'MktEqumGet',        # æœˆçº¿  
            'monthly_adj': 'MktEqumAdjGet', # å‰å¤æƒæœˆçº¿
            'quarterly': 'MktEquqGet',      # å­£çº¿ [æé€Ÿç‰ˆ]
            'yearly': 'MktEquaGet',         # å¹´çº¿ [æé€Ÿç‰ˆ]
            'basic_info': 'EquGet',         # åŸºæœ¬ä¿¡æ¯
            'factors': 'StockFactorsDateRangeGet'  # å› å­æ•°æ®
        }
        
        print("ğŸ“‹ APIæ˜ å°„åŠ è½½å®Œæˆ:")
        for data_type, api_name in self.api_mappings.items():
            print(f"   {data_type}: uqer.DataAPI.{api_name}")
    
    def get_all_a_stocks(self):
        """è·å–å…¨éƒ¨Aè‚¡è‚¡ç¥¨åˆ—è¡¨"""
        print("ğŸ“‹ è·å–å…¨éƒ¨Aè‚¡è‚¡ç¥¨åˆ—è¡¨...")
        
        try:
            # ä½¿ç”¨åŸºæœ¬ä¿¡æ¯APIè·å–æ›´å®Œæ•´çš„è‚¡ç¥¨åˆ—è¡¨
            all_stocks = []
            
            # ä¸Šå¸‚è‚¡ç¥¨
            current_result = uqer.DataAPI.EquGet(
                listStatusCD='L',
                pandas=1
            )
            
            # å·²é€€å¸‚è‚¡ç¥¨
            delisted_result = uqer.DataAPI.EquGet(
                listStatusCD='DE', 
                pandas=1
            )
            
            # å¤„ç†è¿”å›æ•°æ®
            for result, status in [(current_result, 'ä¸Šå¸‚'), (delisted_result, 'é€€å¸‚')]:
                if result is not None:
                    if isinstance(result, str):
                        df = pd.read_csv(StringIO(result))
                    else:
                        df = result
                    
                    if len(df) > 0:
                        all_stocks.append(df)
                        print(f"   ğŸ“ˆ {status}: {len(df)} åª")
            
            if not all_stocks:
                print("âŒ è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥")
                return []
            
            # åˆå¹¶å¹¶è¿‡æ»¤Aè‚¡
            df = pd.concat(all_stocks, ignore_index=True)
            a_stocks = df[
                df['secID'].str.contains('.XSHE|.XSHG', na=False)
            ].copy()
            
            # æ’é™¤æŒ‡æ•°å’Œå…¶ä»–éè‚¡ç¥¨
            a_stocks = a_stocks[
                ~a_stocks['secID'].str.contains('.ZICN|.INDX|.XBEI', na=False)
            ]
            
            stock_list = a_stocks['secID'].unique().tolist()
            stock_list.sort()
            
            print(f"âœ… è·å–åˆ° {len(stock_list)} åªAè‚¡")
            
            # ä¿å­˜è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯
            basic_info_file = self.data_paths['basic_info'] / 'a_stock_info.csv'
            a_stocks.to_csv(basic_info_file, index=False, encoding='utf-8')
            print(f"ğŸ“„ è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯å·²ä¿å­˜: {basic_info_file}")
            
            self.download_stats['total_stocks'] = len(stock_list)
            return stock_list
            
        except Exception as e:
            print(f"âŒ è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")
            return []
    
    def download_stock_multi_timeframe(self, stock_code):
        """ä¸‹è½½å•åªè‚¡ç¥¨çš„å¤šæ—¶é—´å‘¨æœŸæ•°æ®"""
        results = {}
        
        # å®šä¹‰ä¸‹è½½ä»»åŠ¡
        download_tasks = [
            ('daily', '2000-01-01', '2025-08-31'),
            ('daily_adj', '2000-01-01', '2025-08-31'),
            ('weekly', '2000-01-01', '2025-08-31'),
            ('weekly_adj', '2000-01-01', '2025-08-31'),
            ('monthly', '2000-01-01', '2025-08-31'),
            ('monthly_adj', '2000-01-01', '2025-08-31'),
        ]
        
        for data_type, start_date, end_date in download_tasks:
            try:
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
                file_path = self.data_paths[data_type] / f"{stock_code.replace('.', '_')}.csv"
                if file_path.exists():
                    results[data_type] = {'status': 'exists', 'file': file_path}
                    continue
                
                # è°ƒç”¨å¯¹åº”API
                api_name = self.api_mappings[data_type]
                api_func = getattr(uqer.DataAPI, api_name)
                
                result = api_func(
                    secID=stock_code,
                    beginDate=start_date.replace('-', ''),
                    endDate=end_date.replace('-', ''),
                    pandas=1
                )
                
                self.download_stats['api_calls'] += 1
                
                if result is None:
                    results[data_type] = {'status': 'no_data'}
                    continue
                
                # å¤„ç†è¿”å›æ•°æ®
                if isinstance(result, str):
                    df = pd.read_csv(StringIO(result))
                elif isinstance(result, pd.DataFrame):
                    df = result.copy()
                else:
                    results[data_type] = {'status': 'invalid_format'}
                    continue
                
                if len(df) == 0:
                    results[data_type] = {'status': 'empty_data'}
                    continue
                
                # æ•°æ®å¤„ç†å’ŒéªŒè¯
                df = self.process_timeframe_data(df, stock_code, data_type)
                
                if df is None or len(df) == 0:
                    results[data_type] = {'status': 'processing_failed'}
                    continue
                
                # éªŒè¯æ—¶é—´èŒƒå›´
                if not self.validate_date_range(df, start_date, end_date, data_type):
                    results[data_type] = {'status': 'date_range_invalid'}
                    print(f"   âš ï¸ {data_type}: æ—¶é—´èŒƒå›´ä¸ç¬¦åˆè¦æ±‚")
                    continue
                
                # ä¿å­˜æ•°æ®
                df.to_csv(file_path, index=False, encoding='utf-8')
                
                results[data_type] = {
                    'status': 'success',
                    'records': len(df),
                    'file': file_path,
                    'date_range': f"{df['endDate'].min() if 'endDate' in df.columns else df['tradeDate'].min()} - {df['endDate'].max() if 'endDate' in df.columns else df['tradeDate'].max()}"
                }
                
                # æ›´æ–°ç»Ÿè®¡
                with self.progress_lock:
                    self.download_stats['successful_downloads'] += 1
                    self.download_stats['total_records'] += len(df)
                    file_size = file_path.stat().st_size / 1024 / 1024
                    self.download_stats['data_size_mb'] += file_size
                
                # APIé™é€Ÿ
                time.sleep(0.1)
                
            except Exception as e:
                results[data_type] = {'status': 'error', 'error': str(e)}
                with self.progress_lock:
                    self.download_stats['failed_downloads'] += 1
        
        return results
    
    def process_timeframe_data(self, df, stock_code, data_type):
        """å¤„ç†æ—¶é—´å‘¨æœŸæ•°æ®"""
        try:
            # ç¡®å®šæ—¥æœŸåˆ—
            date_col = 'tradeDate' if 'tradeDate' in df.columns else 'endDate'
            if date_col not in df.columns:
                return None
            
            # è½¬æ¢æ—¥æœŸ
            df[date_col] = pd.to_datetime(df[date_col])
            
            # æ ‡å‡†åŒ–åˆ—å
            column_mapping = {
                'highestPrice': 'highPrice',
                'lowestPrice': 'lowPrice',
                'turnoverVol': 'volume',
                'turnoverValue': 'amount',
                'chgPct': 'changePct'
            }
            df = df.rename(columns=column_mapping)
            
            # è¿‡æ»¤æœ‰æ•ˆæ•°æ®
            price_col = 'closePrice'
            if price_col in df.columns:
                df = df.dropna(subset=[price_col])
                df = df[df[price_col] > 0]
                df = df.sort_values(date_col)
            
            # ç¡®ä¿è‚¡ç¥¨ä»£ç æ­£ç¡®
            df['secID'] = stock_code
            
            return df.reset_index(drop=True)
            
        except Exception as e:
            return None
    
    def validate_date_range(self, df, start_date, end_date, data_type):
        """éªŒè¯æ•°æ®æ—¶é—´èŒƒå›´"""
        try:
            date_col = 'tradeDate' if 'tradeDate' in df.columns else 'endDate'
            if date_col not in df.columns:
                return False
            
            data_start = df[date_col].min()
            data_end = df[date_col].max()
            
            target_start = pd.Timestamp(start_date)
            target_end = pd.Timestamp(end_date)
            
            # å¯¹äº2025å¹´çš„æ•°æ®ï¼Œæ£€æŸ¥æ˜¯å¦è‡³å°‘åˆ°è¾¾2025å¹´8æœˆ
            if target_end.year == 2025:
                # æ•°æ®åº”è¯¥è‡³å°‘åˆ°2025å¹´7æœˆï¼ˆè€ƒè™‘æ•°æ®å»¶è¿Ÿï¼‰
                min_required_end = pd.Timestamp('2025-07-01')
                if data_end < min_required_end:
                    print(f"   âš ï¸ {data_type}: æ•°æ®æˆªæ­¢{data_end}ï¼Œæœªè¾¾åˆ°2025å¹´è¦æ±‚")
                    return False
            
            return True
            
        except Exception as e:
            return False
    
    def download_all_professional_data(self):
        """ä¸‹è½½å…¨éƒ¨ä¸“ä¸šçº§æ•°æ®"""
        print("ğŸš€ å¼€å§‹ä¸“ä¸šçº§æ•°æ®ä¸‹è½½")
        print("   ğŸ”¥ æƒé™: ä¼˜çŸ¿æé€Ÿç‰ˆ (268ä¸ªAPI)")
        print("   ğŸ“… æ—¶é—´: 2000å¹´1æœˆ1æ—¥ - 2025å¹´8æœˆ31æ—¥")
        print("   ğŸ“Š ç±»å‹: æ—¥/å‘¨/æœˆ/å­£/å¹´çº¿ + å¤æƒ + å› å­")
        print("=" * 80)
        
        stock_list = self.get_all_a_stocks()
        
        if not stock_list:
            print("âŒ æ— æ³•è·å–è‚¡ç¥¨åˆ—è¡¨")
            return
        
        # é™åˆ¶æ•°é‡è¿›è¡Œæµ‹è¯• - å…ˆä¸‹è½½å‰100åªéªŒè¯
        test_stocks = stock_list[:100]
        print(f"ğŸ“Š æµ‹è¯•ä¸‹è½½: {len(test_stocks)} åªè‚¡ç¥¨")
        
        successful_stocks = 0
        failed_stocks = 0
        
        for i, stock_code in enumerate(test_stocks, 1):
            print(f"\nğŸ“ˆ [{i}/{len(test_stocks)}] å¤„ç†: {stock_code}")
            
            # ä¸‹è½½å¤šæ—¶é—´å‘¨æœŸæ•°æ®
            results = self.download_stock_multi_timeframe(stock_code)
            
            # ç»Ÿè®¡ç»“æœ
            success_count = sum(1 for r in results.values() if r['status'] in ['success', 'exists'])
            total_count = len(results)
            
            if success_count >= total_count // 2:  # è‡³å°‘ä¸€åŠæˆåŠŸ
                successful_stocks += 1
                print(f"   âœ… æˆåŠŸ: {success_count}/{total_count} ä¸ªæ—¶é—´å‘¨æœŸ")
                
                # æ˜¾ç¤ºè¯¦ç»†ç»“æœ
                for data_type, result in results.items():
                    if result['status'] == 'success':
                        print(f"      ğŸ“Š {data_type}: {result['records']} æ¡è®°å½•")
                    elif result['status'] == 'exists':
                        print(f"      â© {data_type}: å·²å­˜åœ¨")
                    elif result['status'] != 'success':
                        print(f"      âŒ {data_type}: {result['status']}")
            else:
                failed_stocks += 1
                print(f"   âŒ å¤±è´¥: {success_count}/{total_count} ä¸ªæ—¶é—´å‘¨æœŸæˆåŠŸ")
            
            # APIé™é€Ÿ
            if i % 10 == 0:
                time.sleep(2)
                print(f"   ğŸ“ˆ æ€»ä½“è¿›åº¦: {i}/{len(test_stocks)} ({i/len(test_stocks)*100:.1f}%)")
        
        # åˆ›å»ºä¸‹è½½æ€»ç»“
        self.create_professional_summary(successful_stocks, failed_stocks, len(test_stocks))
    
    def create_professional_summary(self, successful_stocks, failed_stocks, total_stocks):
        """åˆ›å»ºä¸“ä¸šçº§ä¸‹è½½æ€»ç»“"""
        end_time = datetime.now()
        duration = end_time - self.download_stats['start_time']
        
        summary = {
            'professional_download_info': {
                'start_time': self.download_stats['start_time'].isoformat(),
                'end_time': end_time.isoformat(),
                'duration_minutes': round(duration.total_seconds() / 60, 2),
                'data_range': '2000å¹´1æœˆ1æ—¥ - 2025å¹´8æœˆ31æ—¥',
                'api_version': 'ä¼˜çŸ¿æé€Ÿç‰ˆ',
                'total_api_calls': self.download_stats['api_calls']
            },
            'statistics': {
                'total_stocks_attempted': total_stocks,
                'successful_stocks': successful_stocks,
                'failed_stocks': failed_stocks,
                'success_rate': f"{successful_stocks/total_stocks*100:.1f}%",
                'total_downloads': self.download_stats['successful_downloads'],
                'total_records': self.download_stats['total_records'],
                'total_size_mb': round(self.download_stats['data_size_mb'], 2),
                'total_size_gb': round(self.download_stats['data_size_mb'] / 1024, 2)
            },
            'data_types': {
                'daily': 'æ—¥çº¿æ•°æ® (MktEqudGet)',
                'daily_adj': 'å‰å¤æƒæ—¥çº¿ (MktEqudAdjGet) [æé€Ÿç‰ˆ]',
                'weekly': 'å‘¨çº¿æ•°æ® (MktEquwGet)',
                'weekly_adj': 'å‰å¤æƒå‘¨çº¿ (MktEquwAdjGet) [æé€Ÿç‰ˆ]',
                'monthly': 'æœˆçº¿æ•°æ® (MktEqumGet)',
                'monthly_adj': 'å‰å¤æƒæœˆçº¿ (MktEqumAdjGet)'
            },
            'file_structure': {
                'base_path': str(self.base_path),
                'organization': 'æŒ‰æ•°æ®ç±»å‹å’Œè‚¡ç¥¨ä»£ç ç»„ç»‡',
                'naming_convention': 'XXXXXX_XXXX.csv'
            }
        }
        
        # ä¿å­˜æ€»ç»“
        summary_file = self.base_path / 'professional_download_summary.json'
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸŠ ä¸“ä¸šçº§æ•°æ®ä¸‹è½½å®Œæˆ!")
        print(f"=" * 80)
        print(f"ğŸ“Š ä¸‹è½½ç»Ÿè®¡:")
        print(f"   ğŸ¯ æµ‹è¯•è‚¡ç¥¨: {summary['statistics']['total_stocks_attempted']}")
        print(f"   âœ… æˆåŠŸè‚¡ç¥¨: {summary['statistics']['successful_stocks']}")
        print(f"   âŒ å¤±è´¥è‚¡ç¥¨: {summary['statistics']['failed_stocks']}")
        print(f"   ğŸ“ˆ æˆåŠŸç‡: {summary['statistics']['success_rate']}")
        print(f"   ğŸ“Š æ€»ä¸‹è½½æ•°: {summary['statistics']['total_downloads']}")
        print(f"   ğŸ“ æ€»è®°å½•æ•°: {summary['statistics']['total_records']:,}")
        print(f"   ğŸ’¾ æ•°æ®å¤§å°: {summary['statistics']['total_size_gb']} GB")
        print(f"   ğŸ”¥ APIè°ƒç”¨: {summary['professional_download_info']['total_api_calls']}")
        print(f"   â±ï¸ ç”¨æ—¶: {summary['professional_download_info']['duration_minutes']} åˆ†é’Ÿ")
        print(f"   ğŸ“ å­˜å‚¨ä½ç½®: {self.base_path}")
        print(f"   ğŸ“„ è¯¦ç»†æŠ¥å‘Š: {summary_file}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”¥ ä¸“ä¸šçº§æ•°æ®ä¸‹è½½å™¨ - ä¼˜çŸ¿æé€Ÿç‰ˆ")
    print("=" * 80)
    print("ğŸ¯ åˆ©ç”¨ä¼˜çŸ¿2025å…¨éƒ¨268ä¸ªAPIæ¥å£")
    print("ğŸ“… æ—¶é—´èŒƒå›´: 2000å¹´1æœˆ1æ—¥ - 2025å¹´8æœˆ31æ—¥")
    print("ğŸš€ æ•°æ®ç±»å‹: å¤šæ—¶é—´å‘¨æœŸ + å‰å¤æƒ + å› å­æ•°æ®")
    
    downloader = ProfessionalDataDownloader()
    downloader.download_all_professional_data()

if __name__ == "__main__":
    main()