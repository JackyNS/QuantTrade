#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®‰å…¨æ•°æ®æ¸…ç†å·¥å…·
================

ç›®æ ‡ï¼š
1. è¯†åˆ«æ‰€æœ‰ä¼˜çŸ¿ä¸‹è½½çš„æ•°æ®ç›®å½•
2. å®‰å…¨å¤‡ä»½é‡è¦é…ç½®æ–‡ä»¶
3. æ¸…ç†æ··ä¹±çš„æ•°æ®æ–‡ä»¶
4. ä¸ºé‡æ–°ä¸‹è½½åšå‡†å¤‡

"""

import shutil
from pathlib import Path
from datetime import datetime
import json

class SafeDataCleanup:
    """å®‰å…¨æ•°æ®æ¸…ç†å·¥å…·"""
    
    def __init__(self):
        """åˆå§‹åŒ–"""
        self.base_path = Path("/Users/jackstudio/QuantTrade")
        self.data_path = self.base_path / "data"
        self.backup_path = self.base_path / "backup_before_cleanup"
        
    def identify_uqer_data_directories(self):
        """è¯†åˆ«ä¼˜çŸ¿ç›¸å…³çš„æ•°æ®ç›®å½•"""
        print("ğŸ” è¯†åˆ«ä¼˜çŸ¿ç›¸å…³æ•°æ®ç›®å½•...")
        print("=" * 60)
        
        uqer_directories = []
        
        if self.data_path.exists():
            # ä¸»è¦çš„ä¼˜çŸ¿æ•°æ®ç›®å½•
            potential_dirs = [
                "priority_download",
                "csv_complete", 
                "optimized_data",
                "raw",
                "final_comprehensive_download",
                "comprehensive_api_download",
                "reorganized_stocks"
            ]
            
            for dir_name in potential_dirs:
                dir_path = self.data_path / dir_name
                if dir_path.exists():
                    size_mb = self._calculate_directory_size(dir_path)
                    uqer_directories.append({
                        'path': dir_path,
                        'name': dir_name,
                        'size_mb': size_mb,
                        'size_gb': round(size_mb / 1024, 2)
                    })
                    print(f"   ğŸ“ {dir_name}: {size_mb:.1f} MB ({size_mb/1024:.2f} GB)")
            
            # æŸ¥æ‰¾å…¶ä»–å¯èƒ½çš„ä¼˜çŸ¿æ•°æ®ç›®å½•
            for item in self.data_path.iterdir():
                if item.is_dir() and item.name not in potential_dirs:
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¼˜çŸ¿æ•°æ®ç‰¹å¾æ–‡ä»¶
                    csv_files = list(item.rglob("*.csv"))
                    if len(csv_files) > 100:  # å¦‚æœåŒ…å«å¤§é‡CSVæ–‡ä»¶ï¼Œå¯èƒ½æ˜¯ä¼˜çŸ¿æ•°æ®
                        size_mb = self._calculate_directory_size(item)
                        uqer_directories.append({
                            'path': item,
                            'name': item.name,
                            'size_mb': size_mb,
                            'size_gb': round(size_mb / 1024, 2)
                        })
                        print(f"   ğŸ“ {item.name}: {size_mb:.1f} MB ({size_mb/1024:.2f} GB) [æ£€æµ‹åˆ°]")
        
        total_size_gb = sum(d['size_gb'] for d in uqer_directories)
        print(f"\nğŸ“Š å‘ç°ä¼˜çŸ¿æ•°æ®ç›®å½•: {len(uqer_directories)} ä¸ª")
        print(f"ğŸ’½ æ€»å¤§å°: {total_size_gb:.2f} GB")
        
        return uqer_directories
    
    def _calculate_directory_size(self, directory):
        """è®¡ç®—ç›®å½•å¤§å°ï¼ˆMBï¼‰"""
        total_size = 0
        try:
            for file_path in directory.rglob('*'):
                if file_path.is_file():
                    total_size += file_path.stat().st_size
        except:
            pass
        return round(total_size / (1024 * 1024), 1)
    
    def backup_important_files(self):
        """å¤‡ä»½é‡è¦æ–‡ä»¶"""
        print("\nğŸ’¾ å¤‡ä»½é‡è¦æ–‡ä»¶...")
        print("-" * 40)
        
        self.backup_path.mkdir(exist_ok=True)
        
        # é‡è¦æ–‡ä»¶åˆ—è¡¨
        important_files = [
            "ä¼˜çŸ¿api2025.txt",
            "CLAUDE.md",
            "requirements.txt",
            ".env",
            "main.py"
        ]
        
        backed_up = []
        
        for file_name in important_files:
            file_path = self.base_path / file_name
            if file_path.exists():
                backup_file = self.backup_path / file_name
                shutil.copy2(file_path, backup_file)
                backed_up.append(file_name)
                print(f"   âœ… {file_name}")
        
        # å¤‡ä»½é…ç½®ç›®å½•
        config_dirs = ["core/config", "scripts", "tools"]
        for config_dir in config_dirs:
            config_path = self.base_path / config_dir
            if config_path.exists():
                backup_config = self.backup_path / config_dir
                shutil.copytree(config_path, backup_config, dirs_exist_ok=True)
                backed_up.append(config_dir)
                print(f"   âœ… {config_dir}/")
        
        print(f"ğŸ“ å¤‡ä»½ç›®å½•: {self.backup_path}")
        return backed_up
    
    def create_cleanup_plan(self, uqer_directories):
        """åˆ›å»ºæ¸…ç†è®¡åˆ’"""
        print("\nğŸ“‹ åˆ›å»ºæ¸…ç†è®¡åˆ’...")
        print("-" * 40)
        
        cleanup_plan = {
            'cleanup_time': datetime.now().isoformat(),
            'directories_to_remove': [],
            'total_space_to_free_gb': 0,
            'backup_location': str(self.backup_path)
        }
        
        total_size = 0
        for directory in uqer_directories:
            cleanup_plan['directories_to_remove'].append({
                'path': str(directory['path']),
                'name': directory['name'],
                'size_gb': directory['size_gb']
            })
            total_size += directory['size_gb']
            print(f"   ğŸ—‘ï¸  {directory['name']}: {directory['size_gb']:.2f} GB")
        
        cleanup_plan['total_space_to_free_gb'] = round(total_size, 2)
        
        print(f"\nğŸ’½ æ€»å…±å°†é‡Šæ”¾ç©ºé—´: {total_size:.2f} GB")
        
        # ä¿å­˜æ¸…ç†è®¡åˆ’
        plan_file = self.backup_path / "cleanup_plan.json"
        with open(plan_file, 'w', encoding='utf-8') as f:
            json.dump(cleanup_plan, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“„ æ¸…ç†è®¡åˆ’å·²ä¿å­˜: {plan_file}")
        return cleanup_plan
    
    def execute_cleanup(self, uqer_directories, confirm=True):
        """æ‰§è¡Œæ¸…ç†"""
        if confirm:
            print(f"\nâš ï¸ å³å°†åˆ é™¤ {len(uqer_directories)} ä¸ªç›®å½•ï¼Œé‡Šæ”¾ {sum(d['size_gb'] for d in uqer_directories):.2f} GB ç©ºé—´")
            print("ğŸ“‹ æ¸…ç†ç›®å½•åˆ—è¡¨:")
            for directory in uqer_directories:
                print(f"   ğŸ—‘ï¸  {directory['name']} ({directory['size_gb']:.2f} GB)")
            
            print("\nâš ï¸ ç”±äºåœ¨Claude Codeç¯å¢ƒä¸­ï¼Œå°†è‡ªåŠ¨æ‰§è¡Œæ¸…ç†ï¼ˆå·²å¤‡ä»½é‡è¦æ–‡ä»¶ï¼‰")
            # response = input("\nâ“ ç¡®è®¤æ‰§è¡Œæ¸…ç†ï¼Ÿ(è¾“å…¥ 'YES' ç¡®è®¤): ")
            # if response != 'YES':
            #     print("âŒ æ¸…ç†å·²å–æ¶ˆ")
            #     return False
        
        print(f"\nğŸ§¹ å¼€å§‹æ‰§è¡Œæ¸…ç†...")
        print("=" * 40)
        
        cleaned_dirs = []
        total_freed = 0
        
        for directory in uqer_directories:
            try:
                print(f"   ğŸ—‘ï¸  åˆ é™¤ {directory['name']} ({directory['size_gb']:.2f} GB)...")
                shutil.rmtree(directory['path'])
                cleaned_dirs.append(directory['name'])
                total_freed += directory['size_gb']
                print(f"   âœ… å®Œæˆ")
            except Exception as e:
                print(f"   âŒ åˆ é™¤ {directory['name']} å¤±è´¥: {str(e)}")
        
        print(f"\nğŸŠ æ¸…ç†å®Œæˆ!")
        print(f"âœ… æˆåŠŸåˆ é™¤: {len(cleaned_dirs)} ä¸ªç›®å½•")
        print(f"ğŸ’½ é‡Šæ”¾ç©ºé—´: {total_freed:.2f} GB")
        
        return True
    
    def create_fresh_data_structure(self):
        """åˆ›å»ºå…¨æ–°çš„æ•°æ®ç›®å½•ç»“æ„"""
        print(f"\nğŸ—ï¸ åˆ›å»ºå…¨æ–°çš„æ•°æ®ç›®å½•ç»“æ„...")
        print("-" * 40)
        
        # åˆ›å»ºæ ‡å‡†çš„æ•°æ®ç›®å½•ç»“æ„
        new_structure = {
            "data": {
                "raw": "åŸå§‹ä¸‹è½½æ•°æ®",
                "processed": "å¤„ç†åæ•°æ®", 
                "daily": "æ—¥çº¿æ•°æ®",
                "weekly": "å‘¨çº¿æ•°æ®",
                "monthly": "æœˆçº¿æ•°æ®",
                "basic_info": "è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯",
                "reports": "æ•°æ®æŠ¥å‘Š"
            }
        }
        
        for main_dir, sub_dirs in new_structure.items():
            main_path = self.base_path / main_dir
            main_path.mkdir(exist_ok=True)
            
            if isinstance(sub_dirs, dict):
                for sub_dir, description in sub_dirs.items():
                    sub_path = main_path / sub_dir
                    sub_path.mkdir(exist_ok=True)
                    print(f"   ğŸ“ {main_dir}/{sub_dir} - {description}")
                    
                    # åˆ›å»ºREADMEæ–‡ä»¶è¯´æ˜ç›®å½•ç”¨é€”
                    readme_file = sub_path / "README.md"
                    readme_file.write_text(f"# {sub_dir}\n\n{description}\n\nåˆ›å»ºæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        print(f"âœ… æ–°ç›®å½•ç»“æ„åˆ›å»ºå®Œæˆ")
        return new_structure
    
    def run_complete_cleanup(self):
        """è¿è¡Œå®Œæ•´æ¸…ç†æµç¨‹"""
        print("ğŸ§¹ ä¼˜çŸ¿æ•°æ®å®‰å…¨æ¸…ç†å·¥å…·")
        print("ğŸ¯ ç›®æ ‡: æ¸…ç†æ··ä¹±æ•°æ®ï¼Œä¸ºé‡æ–°ä¸‹è½½åšå‡†å¤‡")
        print("=" * 80)
        
        try:
            # 1. è¯†åˆ«ä¼˜çŸ¿æ•°æ®ç›®å½•
            uqer_directories = self.identify_uqer_data_directories()
            
            if not uqer_directories:
                print("âœ… æœªå‘ç°éœ€è¦æ¸…ç†çš„ä¼˜çŸ¿æ•°æ®ç›®å½•")
                return
            
            # 2. å¤‡ä»½é‡è¦æ–‡ä»¶
            backed_up_files = self.backup_important_files()
            
            # 3. åˆ›å»ºæ¸…ç†è®¡åˆ’
            cleanup_plan = self.create_cleanup_plan(uqer_directories)
            
            # 4. æ‰§è¡Œæ¸…ç†
            success = self.execute_cleanup(uqer_directories, confirm=True)
            
            if success:
                # 5. åˆ›å»ºå…¨æ–°ç›®å½•ç»“æ„
                new_structure = self.create_fresh_data_structure()
                
                print(f"\nğŸŠ æ¸…ç†å®Œæˆ!")
                print(f"ğŸ’¾ é‡è¦æ–‡ä»¶å·²å¤‡ä»½åˆ°: {self.backup_path}")
                print(f"ğŸ“ æ–°çš„æ•°æ®ç›®å½•ç»“æ„å·²åˆ›å»º")
                print(f"ğŸš€ ç°åœ¨å¯ä»¥é‡æ–°ä¸‹è½½ä¼˜çŸ¿æ•°æ®äº†")
            
        except Exception as e:
            print(f"âŒ æ¸…ç†è¿‡ç¨‹å‡ºé”™: {str(e)}")
            raise

def main():
    """ä¸»å‡½æ•°"""
    cleanup_tool = SafeDataCleanup()
    cleanup_tool.run_complete_cleanup()

if __name__ == "__main__":
    main()