#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®è´¨é‡æ·±åº¦æ£€æŸ¥å·¥å…·
"""

import pandas as pd
import numpy as np
from pathlib import Path
import json
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

class DataQualityChecker:
    """æ•°æ®è´¨é‡æ£€æŸ¥å™¨"""
    
    def __init__(self):
        self.data_root = Path("data")
        self.quality_report = {}
        
    def check_sample_data_quality(self):
        """æ£€æŸ¥æ ·æœ¬æ•°æ®è´¨é‡"""
        print("ğŸ¯ æ·±åº¦æ•°æ®è´¨é‡æ£€æŸ¥...")
        
        quality_issues = {
            'priority_download': {},
            'smart_download': {},
            'historical_download': {}
        }
        
        # 1. æ£€æŸ¥ä¼˜å…ˆçº§ä¸‹è½½å™¨æ•°æ®
        priority_daily = self.data_root / "priority_download/market_data/daily"
        if priority_daily.exists():
            sample_files = list(priority_daily.glob("**/2024_batch_001.csv"))[:3]
            for file_path in sample_files:
                issues = self._analyze_file_quality(file_path, "daily_market")
                if issues:
                    quality_issues['priority_download'][file_path.name] = issues
        
        # 2. æ£€æŸ¥æ™ºèƒ½ä¸‹è½½å™¨æ•°æ®  
        smart_2024 = self.data_root / "smart_download/year_2024"
        if smart_2024.exists():
            sample_files = list(smart_2024.glob("batch_001.csv"))
            for file_path in sample_files:
                issues = self._analyze_file_quality(file_path, "daily_market")
                if issues:
                    quality_issues['smart_download'][file_path.name] = issues
        
        # 3. æ£€æŸ¥å†å²ä¸‹è½½å™¨æ•°æ®
        hist_2024 = self.data_root / "historical_download/market_data/year_2024"
        if hist_2024.exists():
            sample_files = list(hist_2024.glob("batch_001.csv"))
            for file_path in sample_files:
                issues = self._analyze_file_quality(file_path, "daily_market")
                if issues:
                    quality_issues['historical_download'][file_path.name] = issues
        
        return quality_issues
    
    def _analyze_file_quality(self, file_path, data_type):
        """åˆ†æå•ä¸ªæ–‡ä»¶è´¨é‡"""
        issues = []
        
        try:
            df = pd.read_csv(file_path)
            
            # åŸºç¡€æ£€æŸ¥
            if df.empty:
                issues.append("æ–‡ä»¶ä¸ºç©º")
                return issues
            
            # 1. ç©ºå€¼æ£€æŸ¥
            null_counts = df.isnull().sum()
            critical_nulls = null_counts[null_counts > 0]
            if len(critical_nulls) > 0:
                null_info = {}
                for col, count in critical_nulls.items():
                    null_info[col] = f"{count}/{len(df)} ({count/len(df)*100:.1f}%)"
                issues.append(f"ç©ºå€¼: {null_info}")
            
            # 2. é‡å¤è®°å½•æ£€æŸ¥
            if data_type == "daily_market":
                # æ£€æŸ¥åŒä¸€è‚¡ç¥¨åŒä¸€æ—¥æœŸçš„é‡å¤
                if 'ticker' in df.columns and 'tradeDate' in df.columns:
                    duplicates = df.duplicated(['ticker', 'tradeDate']).sum()
                    if duplicates > 0:
                        issues.append(f"é‡å¤è®°å½•: {duplicates} æ¡")
            
            # 3. ä»·æ ¼å¼‚å¸¸æ£€æŸ¥
            price_cols = ['openPrice', 'highestPrice', 'lowestPrice', 'closePrice']
            for col in price_cols:
                if col in df.columns:
                    prices = df[col].dropna()
                    if len(prices) > 0:
                        # è´Ÿä»·æ ¼æˆ–é›¶ä»·æ ¼
                        invalid_prices = (prices <= 0).sum()
                        if invalid_prices > 0:
                            issues.append(f"{col}å¼‚å¸¸ä»·æ ¼: {invalid_prices} æ¡ <= 0")
                        
                        # æç«¯ä»·æ ¼ï¼ˆè¶…è¿‡1000å…ƒæˆ–ä½äº0.01å…ƒï¼‰
                        extreme_high = (prices > 1000).sum()
                        extreme_low = ((prices > 0) & (prices < 0.01)).sum()
                        if extreme_high > 0:
                            issues.append(f"{col}æé«˜ä»·æ ¼: {extreme_high} æ¡ > 1000")
                        if extreme_low > 0:
                            issues.append(f"{col}æä½ä»·æ ¼: {extreme_low} æ¡ < 0.01")
            
            # 4. é€»è¾‘å…³ç³»æ£€æŸ¥
            if all(col in df.columns for col in ['openPrice', 'highestPrice', 'lowestPrice', 'closePrice']):
                # æ£€æŸ¥ high >= open, close, low å’Œ low <= open, close, high
                high_issues = ((df['highestPrice'] < df['openPrice']) | 
                              (df['highestPrice'] < df['closePrice']) |
                              (df['highestPrice'] < df['lowestPrice'])).sum()
                
                low_issues = ((df['lowestPrice'] > df['openPrice']) |
                             (df['lowestPrice'] > df['closePrice']) |
                             (df['lowestPrice'] > df['highestPrice'])).sum()
                
                if high_issues > 0:
                    issues.append(f"æœ€é«˜ä»·é€»è¾‘é”™è¯¯: {high_issues} æ¡")
                if low_issues > 0:
                    issues.append(f"æœ€ä½ä»·é€»è¾‘é”™è¯¯: {low_issues} æ¡")
            
            # 5. äº¤æ˜“é‡æ£€æŸ¥
            if 'turnoverVol' in df.columns:
                volumes = df['turnoverVol'].dropna()
                if len(volumes) > 0:
                    negative_vol = (volumes < 0).sum()
                    if negative_vol > 0:
                        issues.append(f"è´Ÿäº¤æ˜“é‡: {negative_vol} æ¡")
            
            # 6. æ—¥æœŸæ ¼å¼æ£€æŸ¥
            if 'tradeDate' in df.columns:
                try:
                    pd.to_datetime(df['tradeDate'])
                except:
                    issues.append("æ—¥æœŸæ ¼å¼é”™è¯¯")
            
            # 7. æ•°æ®å®Œæ•´æ€§æ£€æŸ¥ï¼ˆè®°å½•æ•°æ˜¯å¦åˆç†ï¼‰
            if data_type == "daily_market":
                expected_records = self._estimate_expected_records(file_path, df)
                actual_records = len(df)
                if actual_records < expected_records * 0.8:  # å°‘äºé¢„æœŸ80%
                    issues.append(f"æ•°æ®ä¸å®Œæ•´: {actual_records}/{expected_records} æ¡")
                    
        except Exception as e:
            issues.append(f"è¯»å–å¤±è´¥: {str(e)}")
        
        return issues
    
    def _estimate_expected_records(self, file_path, df):
        """ä¼°ç®—é¢„æœŸè®°å½•æ•°"""
        # æ ¹æ®æ–‡ä»¶è·¯å¾„åˆ¤æ–­å¹´ä»½
        path_str = str(file_path)
        year = None
        for y in range(2000, 2026):
            if str(y) in path_str:
                year = y
                break
        
        if year is None:
            return len(df)  # æ— æ³•ä¼°ç®—åˆ™è¿”å›å®é™…æ•°é‡
        
        # ä¼°ç®—è¯¥å¹´ä»½çš„äº¤æ˜“æ—¥æ•°é‡
        trading_days = 250  # å¹³å‡æ¯å¹´äº¤æ˜“æ—¥
        if year == 2025:
            # 2025å¹´æŒ‰å½“å‰æ—¥æœŸè®¡ç®—
            days_passed = (datetime.now() - datetime(2025, 1, 1)).days
            trading_days = min(days_passed * 250 / 365, 250)
        
        # ä¼°ç®—è‚¡ç¥¨æ•°é‡ï¼ˆæ‰¹æ¬¡æ–‡ä»¶é€šå¸¸åŒ…å«100åªå·¦å³ï¼‰
        unique_stocks = df['ticker'].nunique() if 'ticker' in df.columns else 100
        
        return int(trading_days * unique_stocks)
    
    def check_data_consistency(self):
        """æ£€æŸ¥æ•°æ®ä¸€è‡´æ€§"""
        print("\nğŸ” æ£€æŸ¥æ•°æ®ä¸€è‡´æ€§...")
        
        consistency_issues = {}
        
        # æ£€æŸ¥åŒä¸€å¹´ä»½ä¸åŒä¸‹è½½å™¨çš„æ•°æ®ä¸€è‡´æ€§
        test_years = [2020, 2023, 2024]
        
        for year in test_years:
            year_issues = []
            
            # è·å–å„ä¸‹è½½å™¨è¯¥å¹´ä»½æ•°æ®
            files = {
                'historical': list((self.data_root / f"historical_download/market_data/year_{year}").glob("batch_001.csv")),
                'smart': list((self.data_root / f"smart_download/year_{year}").glob("batch_001.csv")),
                'priority': list((self.data_root / f"priority_download/market_data/daily").glob(f"{year}_batch_001.csv"))
            }
            
            # æ¯”è¾ƒæ•°æ®
            dfs = {}
            for downloader, file_list in files.items():
                if file_list:
                    try:
                        df = pd.read_csv(file_list[0])
                        if not df.empty and 'ticker' in df.columns and 'closePrice' in df.columns:
                            dfs[downloader] = df
                    except:
                        continue
            
            # æ£€æŸ¥ç›¸åŒè‚¡ç¥¨ç›¸åŒæ—¥æœŸçš„ä»·æ ¼æ˜¯å¦ä¸€è‡´
            if len(dfs) >= 2:
                downloader_names = list(dfs.keys())
                for i, d1 in enumerate(downloader_names[:-1]):
                    for d2 in downloader_names[i+1:]:
                        df1, df2 = dfs[d1], dfs[d2]
                        
                        # æ‰¾å‡ºå…±åŒçš„è‚¡ç¥¨å’Œæ—¥æœŸ
                        if all(col in df1.columns and col in df2.columns for col in ['ticker', 'tradeDate', 'closePrice']):
                            common = pd.merge(df1[['ticker', 'tradeDate', 'closePrice']], 
                                            df2[['ticker', 'tradeDate', 'closePrice']], 
                                            on=['ticker', 'tradeDate'], 
                                            suffixes=('_1', '_2'))
                            
                            if len(common) > 0:
                                # æ£€æŸ¥ä»·æ ¼å·®å¼‚
                                price_diff = abs(common['closePrice_1'] - common['closePrice_2'])
                                inconsistent = (price_diff > 0.01).sum()  # å·®å¼‚è¶…è¿‡1åˆ†é’±
                                
                                if inconsistent > 0:
                                    total_common = len(common)
                                    year_issues.append(f"{d1} vs {d2}: {inconsistent}/{total_common} æ¡ä»·æ ¼ä¸ä¸€è‡´")
            
            if year_issues:
                consistency_issues[year] = year_issues
        
        return consistency_issues
    
    def check_temporal_continuity(self):
        """æ£€æŸ¥æ—¶é—´è¿ç»­æ€§"""
        print("\nğŸ“… æ£€æŸ¥æ—¶é—´è¿ç»­æ€§...")
        
        continuity_issues = {}
        
        # æ£€æŸ¥ä¼˜å…ˆçº§æ•°æ®çš„æ—¶é—´è¿ç»­æ€§
        daily_dir = self.data_root / "priority_download/market_data/daily"
        if daily_dir.exists():
            # éšæœºé€‰æ‹©å‡ ä¸ªè‚¡ç¥¨æ£€æŸ¥
            sample_files = list(daily_dir.glob("2024_batch_001.csv"))[:1]
            
            for file_path in sample_files:
                try:
                    df = pd.read_csv(file_path)
                    if 'ticker' in df.columns and 'tradeDate' in df.columns:
                        # é€‰æ‹©æ•°æ®è¾ƒå¤šçš„è‚¡ç¥¨
                        stock_counts = df['ticker'].value_counts()
                        top_stocks = stock_counts.head(3).index
                        
                        for stock in top_stocks:
                            stock_data = df[df['ticker'] == stock].copy()
                            stock_data['tradeDate'] = pd.to_datetime(stock_data['tradeDate'])
                            stock_data = stock_data.sort_values('tradeDate')
                            
                            # æ£€æŸ¥æ—¥æœŸé—´éš”
                            date_diffs = stock_data['tradeDate'].diff().dt.days
                            
                            # æ­£å¸¸äº¤æ˜“æ—¥é—´éš”åº”è¯¥æ˜¯1-3å¤©ï¼ˆè€ƒè™‘å‘¨æœ«ï¼‰
                            large_gaps = date_diffs[date_diffs > 7]  # è¶…è¿‡ä¸€å‘¨çš„é—´éš”
                            
                            if len(large_gaps) > 0:
                                continuity_issues[f"{file_path.name}_{stock}"] = f"{len(large_gaps)} ä¸ªå¤§é—´éš”"
                
                except Exception as e:
                    continuity_issues[file_path.name] = f"æ£€æŸ¥å¤±è´¥: {e}"
        
        return continuity_issues
    
    def generate_quality_report(self):
        """ç”Ÿæˆè´¨é‡æŠ¥å‘Š"""
        print("ğŸš€ å¼€å§‹æ•°æ®è´¨é‡æ·±åº¦æ£€æŸ¥...\n")
        
        # æ‰§è¡Œå„é¡¹æ£€æŸ¥
        sample_quality = self.check_sample_data_quality()
        consistency = self.check_data_consistency()
        continuity = self.check_temporal_continuity()
        
        # æ±‡æ€»æŠ¥å‘Š
        report = {
            'check_time': datetime.now().isoformat(),
            'sample_quality_issues': sample_quality,
            'consistency_issues': consistency,
            'continuity_issues': continuity,
            'summary': self._generate_quality_summary(sample_quality, consistency, continuity)
        }
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = self.data_root / 'data_quality_report.json'
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"\nğŸ“‹ è´¨é‡æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        return report
    
    def _generate_quality_summary(self, sample_quality, consistency, continuity):
        """ç”Ÿæˆè´¨é‡æ‘˜è¦"""
        summary = {
            'overall_quality': 'good',
            'critical_issues': 0,
            'warning_issues': 0,
            'recommendations': []
        }
        
        # ç»Ÿè®¡é—®é¢˜
        for downloader, files in sample_quality.items():
            for filename, issues in files.items():
                for issue in issues:
                    if any(keyword in issue for keyword in ['å¼‚å¸¸ä»·æ ¼', 'é€»è¾‘é”™è¯¯', 'é‡å¤è®°å½•']):
                        summary['critical_issues'] += 1
                    else:
                        summary['warning_issues'] += 1
        
        summary['critical_issues'] += len(consistency)
        summary['warning_issues'] += len(continuity)
        
        # è¯„ä¼°æ•´ä½“è´¨é‡
        if summary['critical_issues'] > 5:
            summary['overall_quality'] = 'poor'
            summary['recommendations'].append('ç«‹å³ä¿®å¤ä¸¥é‡æ•°æ®è´¨é‡é—®é¢˜')
        elif summary['critical_issues'] > 0:
            summary['overall_quality'] = 'fair'
            summary['recommendations'].append('ä¿®å¤å‘ç°çš„æ•°æ®è´¨é‡é—®é¢˜')
        
        if summary['warning_issues'] > 10:
            summary['recommendations'].append('å¤„ç†æ•°æ®è­¦å‘Šé—®é¢˜ä»¥æå‡è´¨é‡')
        
        return summary
    
    def print_quality_summary(self, report):
        """æ‰“å°è´¨é‡æ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸ“Š æ•°æ®è´¨é‡æ£€æŸ¥æ‘˜è¦")
        print("="*60)
        
        summary = report['summary']
        print(f"ğŸ¯ æ•´ä½“è´¨é‡: {summary['overall_quality'].upper()}")
        print(f"âŒ ä¸¥é‡é—®é¢˜: {summary['critical_issues']} ä¸ª")
        print(f"âš ï¸ è­¦å‘Šé—®é¢˜: {summary['warning_issues']} ä¸ª")
        
        if summary['critical_issues'] > 0:
            print(f"\nğŸ” ä¸¥é‡é—®é¢˜è¯¦æƒ…:")
            for downloader, files in report['sample_quality_issues'].items():
                for filename, issues in files.items():
                    critical = [i for i in issues if any(k in i for k in ['å¼‚å¸¸ä»·æ ¼', 'é€»è¾‘é”™è¯¯', 'é‡å¤è®°å½•'])]
                    if critical:
                        print(f"   ğŸ“ {downloader}/{filename}:")
                        for issue in critical[:3]:  # æ˜¾ç¤ºå‰3ä¸ª
                            print(f"      â€¢ {issue}")
        
        if report['consistency_issues']:
            print(f"\nğŸ“Š æ•°æ®ä¸€è‡´æ€§é—®é¢˜:")
            for year, issues in report['consistency_issues'].items():
                print(f"   {year}å¹´: {len(issues)} ä¸ªé—®é¢˜")
                for issue in issues[:2]:
                    print(f"      â€¢ {issue}")
        
        if summary['recommendations']:
            print(f"\nğŸ’¡ å»ºè®®:")
            for rec in summary['recommendations']:
                print(f"   â€¢ {rec}")

def main():
    """ä¸»å‡½æ•°"""
    checker = DataQualityChecker()
    report = checker.generate_quality_report()
    checker.print_quality_summary(report)

if __name__ == "__main__":
    main()