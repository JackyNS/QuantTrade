#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ ¹ç›®å½•æ·±åº¦åˆ†æå™¨ - è¯†åˆ«è¿›ä¸€æ­¥ä¼˜åŒ–æœºä¼š
"""

import os
from pathlib import Path
from datetime import datetime
import json

class RootDirectoryAnalyzer:
    """æ ¹ç›®å½•åˆ†æå™¨"""
    
    def __init__(self):
        self.root = Path(".")
        
    def analyze_python_files(self):
        """åˆ†ææ ¹ç›®å½•Pythonæ–‡ä»¶"""
        print("ğŸ åˆ†æPythonæ–‡ä»¶...")
        
        py_files = list(self.root.glob("*.py"))
        categories = {
            'core_business': [],      # æ ¸å¿ƒä¸šåŠ¡
            'utility_tools': [],      # å·¥å…·è„šæœ¬
            'analysis_tools': [],     # åˆ†æå·¥å…·
            'maintenance': [],        # ç»´æŠ¤è„šæœ¬
            'temporary': []           # ä¸´æ—¶æ–‡ä»¶
        }
        
        for py_file in py_files:
            name = py_file.name
            
            # æ ¸å¿ƒä¸šåŠ¡æ–‡ä»¶
            if name in ['main.py', 'setup.py', 'data_usage_guide.py', 
                       'daily_update_uqer.py', 'monitor_download_progress.py',
                       'priority_market_flow_downloader.py', 
                       'start_historical_download.py', 'start_smart_download.py']:
                categories['core_business'].append(name)
                
            # å·¥å…·è„šæœ¬  
            elif name in ['auto_backup.py', 'setup_daily_backup.py', 'setup_scheduler.py']:
                categories['utility_tools'].append(name)
                
            # åˆ†æå·¥å…·
            elif 'git' in name or 'redundancy' in name or 'verification' in name:
                categories['analysis_tools'].append(name)
                
            # ç»´æŠ¤è„šæœ¬
            elif 'optimize' in name or 'cleanup' in name or 'execute' in name:
                categories['maintenance'].append(name)
                
            else:
                categories['temporary'].append(name)
        
        return categories
    
    def analyze_markdown_files(self):
        """åˆ†æMarkdownæ–‡ä»¶"""
        print("ğŸ“‹ åˆ†æMarkdownæ–‡ä»¶...")
        
        md_files = list(self.root.glob("*.md"))
        categories = {
            'core_docs': [],          # æ ¸å¿ƒæ–‡æ¡£
            'process_reports': [],    # è¿‡ç¨‹æŠ¥å‘Š
            'project_summaries': [],  # é¡¹ç›®æ€»ç»“
            'redundant': []           # å†—ä½™æ–‡æ¡£
        }
        
        for md_file in md_files:
            name = md_file.name
            
            if name in ['README.md', 'CLAUDE.md']:
                categories['core_docs'].append(name)
            elif 'REPORT' in name or 'SUMMARY' in name:
                categories['process_reports'].append(name)
            elif 'OVERVIEW' in name or 'STRUCTURE' in name:
                categories['project_summaries'].append(name)
            else:
                categories['redundant'].append(name)
        
        return categories
    
    def analyze_directories(self):
        """åˆ†æç›®å½•ç»“æ„"""
        print("ğŸ“ åˆ†æç›®å½•ç»“æ„...")
        
        directories = [d for d in self.root.iterdir() 
                      if d.is_dir() and not d.name.startswith('.')]
        
        analysis = {}
        
        for directory in directories:
            if directory.name in ['core', 'data']:
                continue  # è·³è¿‡æ ¸å¿ƒæ¨¡å—
                
            file_count = len(list(directory.rglob("*")))
            recent_files = sum(1 for f in directory.rglob("*") 
                             if f.is_file() and 
                             (datetime.now().timestamp() - f.stat().st_mtime) < 86400*7)
            
            analysis[directory.name] = {
                'total_files': file_count,
                'recent_files': recent_files,
                'purpose': self._classify_directory(directory.name),
                'optimization_potential': self._assess_optimization_potential(directory)
            }
        
        return analysis
    
    def _classify_directory(self, name):
        """åˆ†ç±»ç›®å½•ç”¨é€”"""
        classifications = {
            'tools': 'development_tools',
            'archive': 'historical_storage',
            'docs': 'documentation', 
            'logs': 'system_logs',
            'reports': 'analysis_output',
            'results': 'computation_output',
            'scripts': 'execution_scripts',
            'notebooks': 'development_environment',
            'tests': 'testing_suite'
        }
        return classifications.get(name, 'unknown')
    
    def _assess_optimization_potential(self, directory):
        """è¯„ä¼°ä¼˜åŒ–æ½œåŠ›"""
        name = directory.name
        file_count = len(list(directory.rglob("*")))
        
        # æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤åŠŸèƒ½çš„ç›®å½•
        if name in ['logs', 'reports', 'results']:
            return 'merge_potential'  # å¯èƒ½å¯ä»¥åˆå¹¶
        elif name == 'scripts' and file_count < 10:
            return 'integration_potential'  # å¯èƒ½å¯ä»¥é›†æˆåˆ°å…¶ä»–åœ°æ–¹
        elif name == 'notebooks' and file_count > 50:
            return 'cleanup_needed'  # éœ€è¦æ¸…ç†
        else:
            return 'well_organized'
    
    def identify_optimization_opportunities(self, py_analysis, md_analysis, dir_analysis):
        """è¯†åˆ«ä¼˜åŒ–æœºä¼š"""
        print("\nğŸ¯ è¯†åˆ«ä¼˜åŒ–æœºä¼š...")
        
        opportunities = {
            'file_consolidation': [],
            'directory_merging': [],
            'cleanup_candidates': [],
            'restructuring': []
        }
        
        # æ–‡ä»¶æ•´åˆæœºä¼š
        if py_analysis['analysis_tools']:
            opportunities['file_consolidation'].append({
                'category': 'analysis_tools',
                'files': py_analysis['analysis_tools'],
                'suggestion': 'ç§»åŠ¨åˆ° tools/analysis/ ç›®å½•'
            })
            
        if py_analysis['maintenance']:
            opportunities['file_consolidation'].append({
                'category': 'maintenance_scripts', 
                'files': py_analysis['maintenance'],
                'suggestion': 'ç§»åŠ¨åˆ° tools/maintenance/ ç›®å½•'
            })
        
        # æ–‡æ¡£æ•´åˆ
        if len(md_analysis['process_reports']) > 3:
            opportunities['file_consolidation'].append({
                'category': 'process_reports',
                'files': md_analysis['process_reports'],
                'suggestion': 'åˆå¹¶ä¸ºå•ä¸€æ€»ç»“æ–‡æ¡£æˆ–ç§»åŠ¨åˆ° archive/docs/'
            })
        
        # ç›®å½•åˆå¹¶æœºä¼š
        merge_candidates = []
        for dir_name, info in dir_analysis.items():
            if info['optimization_potential'] == 'merge_potential':
                merge_candidates.append(dir_name)
        
        if 'reports' in merge_candidates and 'results' in merge_candidates:
            opportunities['directory_merging'].append({
                'directories': ['reports', 'results'],
                'suggestion': 'åˆå¹¶ä¸ºå•ä¸€è¾“å‡ºç›®å½• outputs/'
            })
        
        # æ¸…ç†å€™é€‰
        for dir_name, info in dir_analysis.items():
            if info['recent_files'] == 0 and info['total_files'] > 5:
                opportunities['cleanup_candidates'].append({
                    'directory': dir_name,
                    'reason': f'æ— æœ€è¿‘æ´»åŠ¨æ–‡ä»¶ï¼ŒåŒ…å«{info["total_files"]}ä¸ªæ—§æ–‡ä»¶'
                })
        
        return opportunities
    
    def generate_optimization_plan(self, opportunities):
        """ç”Ÿæˆä¼˜åŒ–è®¡åˆ’"""
        plan = {
            'immediate_actions': [],
            'optional_improvements': [],
            'long_term_considerations': []
        }
        
        # ç«‹å³è¡ŒåŠ¨
        for consolidation in opportunities['file_consolidation']:
            if consolidation['category'] in ['analysis_tools', 'maintenance_scripts']:
                plan['immediate_actions'].append({
                    'action': f"mkdir -p tools/{consolidation['category'].split('_')[0]}",
                    'description': f"ç§»åŠ¨ {len(consolidation['files'])} ä¸ª{consolidation['category']}æ–‡ä»¶"
                })
        
        # å¯é€‰æ”¹è¿›
        for merge in opportunities['directory_merging']:
            plan['optional_improvements'].append({
                'action': f"åˆå¹¶ç›®å½•: {' + '.join(merge['directories'])}",
                'description': merge['suggestion']
            })
        
        # é•¿æœŸè€ƒè™‘
        for cleanup in opportunities['cleanup_candidates']:
            plan['long_term_considerations'].append({
                'action': f"review_{cleanup['directory']}",
                'description': f"å®¡æŸ¥ {cleanup['directory']} ç›®å½•: {cleanup['reason']}"
            })
        
        return plan
    
    def run_analysis(self):
        """è¿è¡Œå®Œæ•´åˆ†æ"""
        print("ğŸ” å¼€å§‹æ ¹ç›®å½•æ·±åº¦åˆ†æ...")
        print("=" * 50)
        
        # åˆ†ææ–‡ä»¶
        py_analysis = self.analyze_python_files()
        md_analysis = self.analyze_markdown_files()
        dir_analysis = self.analyze_directories()
        
        # è¯†åˆ«ä¼˜åŒ–æœºä¼š
        opportunities = self.identify_optimization_opportunities(py_analysis, md_analysis, dir_analysis)
        
        # ç”Ÿæˆè®¡åˆ’
        plan = self.generate_optimization_plan(opportunities)
        
        # è¾“å‡ºç»“æœ
        print(f"\nğŸ“Š åˆ†æç»“æœ:")
        print(f"ğŸ“ Pythonæ–‡ä»¶: {sum(len(files) for files in py_analysis.values())} ä¸ª")
        print(f"ğŸ“‹ Markdownæ–‡ä»¶: {sum(len(files) for files in md_analysis.values())} ä¸ª")
        print(f"ğŸ“ ç›®å½•: {len(dir_analysis)} ä¸ª")
        
        print(f"\nğŸ¯ ä¼˜åŒ–å»ºè®®:")
        
        if opportunities['file_consolidation']:
            print(f"\nğŸ“¦ æ–‡ä»¶æ•´åˆæœºä¼š ({len(opportunities['file_consolidation'])} é¡¹):")
            for item in opportunities['file_consolidation']:
                print(f"  â€¢ {item['category']}: {len(item['files'])} ä¸ªæ–‡ä»¶")
                print(f"    å»ºè®®: {item['suggestion']}")
        
        if opportunities['directory_merging']:
            print(f"\nğŸ”„ ç›®å½•åˆå¹¶æœºä¼š ({len(opportunities['directory_merging'])} é¡¹):")
            for item in opportunities['directory_merging']:
                print(f"  â€¢ åˆå¹¶: {' + '.join(item['directories'])}")
                print(f"    å»ºè®®: {item['suggestion']}")
        
        if opportunities['cleanup_candidates']:
            print(f"\nğŸ§¹ æ¸…ç†å€™é€‰ ({len(opportunities['cleanup_candidates'])} é¡¹):")
            for item in opportunities['cleanup_candidates']:
                print(f"  â€¢ {item['directory']}: {item['reason']}")
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        report = {
            'analysis_time': datetime.now().isoformat(),
            'python_analysis': py_analysis,
            'markdown_analysis': md_analysis,
            'directory_analysis': dir_analysis,
            'optimization_opportunities': opportunities,
            'optimization_plan': plan
        }
        
        with open('root_analysis_report.json', 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“Š è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: root_analysis_report.json")
        
        return report

def main():
    """ä¸»å‡½æ•°"""
    analyzer = RootDirectoryAnalyzer()
    analyzer.run_analysis()

if __name__ == "__main__":
    main()