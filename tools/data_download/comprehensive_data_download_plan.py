#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å…¨é¢æ•°æ®ä¸‹è½½è®¡åˆ’
=============

åŸºäº263ä¸ªä¼˜çŸ¿æ¥å£ï¼Œåˆ¶å®šä»2000å¹´è‡³ä»Šçš„ç³»ç»Ÿæ€§æ•°æ®ä¸‹è½½æ–¹æ¡ˆ
"""

import uqer
import pandas as pd
from datetime import datetime, timedelta
from pathlib import Path
import time
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/comprehensive_download.log'),
        logging.StreamHandler()
    ]
)

# ä¼˜çŸ¿Token
UQER_TOKEN = "68b9922817ae6273137bda7acba81e293582ba347281dfcc056dcb245b23faf3"

class ComprehensiveDataDownloader:
    """å…¨é¢æ•°æ®ä¸‹è½½å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–ä¸‹è½½å™¨"""
        self.client = uqer.Client(token=UQER_TOKEN)
        self.data_dir = Path("data/comprehensive")
        self.data_dir.mkdir(parents=True, exist_ok=True)
        self.start_date = "20000101"
        self.end_date = datetime.now().strftime('%Y%m%d')
        
        # æ ¸å¿ƒæ•°æ®æ¥å£åˆ†ç±»
        self.core_apis = self._define_core_apis()
        
    def _define_core_apis(self):
        """å®šä¹‰æ ¸å¿ƒæ•°æ®æ¥å£"""
        return {
            # 1. åŸºç¡€è¡Œæƒ…æ•°æ® (æœ€é«˜ä¼˜å…ˆçº§)
            "market_data": {
                "priority": 1,
                "apis": {
                    "getMktEqud": "æ²ªæ·±è‚¡ç¥¨æ—¥è¡Œæƒ…",
                    "getMktEquw": "è‚¡ç¥¨å‘¨è¡Œæƒ…", 
                    "getMktEqum": "è‚¡ç¥¨æœˆè¡Œæƒ…",
                    "getMktIdxd": "æŒ‡æ•°æ—¥è¡Œæƒ…",
                    "getMktEqudAdj": "æ²ªæ·±è‚¡ç¥¨å‰å¤æƒè¡Œæƒ…",
                    "getMktEquFlow": "ä¸ªè‚¡æ—¥èµ„é‡‘æµå‘"
                }
            },
            
            # 2. è‚¡ç¥¨åŸºç¡€ä¿¡æ¯ (é«˜ä¼˜å…ˆçº§)
            "stock_basics": {
                "priority": 2,
                "apis": {
                    "getEqu": "è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯",
                    "getEquIPO": "è‚¡ç¥¨é¦–æ¬¡ä¸Šå¸‚ä¿¡æ¯",
                    "getEquIndustry": "è‚¡ç¥¨è¡Œä¸šåˆ†ç±»",
                    "getSecID": "è¯åˆ¸ç¼–ç åŠåŸºæœ¬ä¸Šå¸‚ä¿¡æ¯",
                    "getEquDiv": "è‚¡ç¥¨åˆ†çº¢ä¿¡æ¯",
                    "getEquSplits": "è‚¡ç¥¨æ‹†è‚¡ä¿¡æ¯"
                }
            },
            
            # 3. è´¢åŠ¡æ•°æ® (é«˜ä¼˜å…ˆçº§)
            "financial_data": {
                "priority": 3,
                "apis": {
                    # èµ„äº§è´Ÿå€ºè¡¨
                    "getFdmtBSAllLatest": "åˆå¹¶èµ„äº§è´Ÿå€ºè¡¨(æœ€æ–°æŠ«éœ²)",
                    "getFdmtBSBankAllLatest": "é“¶è¡Œä¸šèµ„äº§è´Ÿå€ºè¡¨(æœ€æ–°æŠ«éœ²)",
                    "getFdmtBSInduAllLatest": "ä¸€èˆ¬å·¥å•†ä¸šèµ„äº§è´Ÿå€ºè¡¨(æœ€æ–°æŠ«éœ²)",
                    
                    # åˆ©æ¶¦è¡¨
                    "getFdmtISAllLatest": "åˆå¹¶åˆ©æ¶¦è¡¨(æœ€æ–°æŠ«éœ²)",
                    "getFdmtISBankAllLatest": "é“¶è¡Œä¸šåˆ©æ¶¦è¡¨(æœ€æ–°æŠ«éœ²)", 
                    "getFdmtISInduAllLatest": "ä¸€èˆ¬å·¥å•†ä¸šåˆ©æ¶¦è¡¨(æœ€æ–°æŠ«éœ²)",
                    
                    # ç°é‡‘æµé‡è¡¨
                    "getFdmtCFAllLatest": "åˆå¹¶ç°é‡‘æµé‡è¡¨(æœ€æ–°æŠ«éœ²)",
                    "getFdmtCFBankAllLatest": "é“¶è¡Œä¸šç°é‡‘æµé‡è¡¨(æœ€æ–°æŠ«éœ²)",
                    "getFdmtCFInduAllLatest": "ä¸€èˆ¬å·¥å•†ä¸šç°é‡‘æµé‡è¡¨(æœ€æ–°æŠ«éœ²)",
                    
                    # è´¢åŠ¡æŒ‡æ ‡
                    "getFdmtDer": "è´¢åŠ¡è¡ç”Ÿæ•°æ®",
                    "getFdmtIndiPS": "è´¢åŠ¡æŒ‡æ ‡â€”æ¯è‚¡æŒ‡æ ‡",
                    "getFdmtIndiGrowth": "è´¢åŠ¡æŒ‡æ ‡â€”æˆé•¿èƒ½åŠ›",
                    "getFdmtIndiRtn": "è´¢åŠ¡æŒ‡æ ‡â€”ç›ˆåˆ©èƒ½åŠ›",
                    "getFdmtIndiLqd": "è´¢åŠ¡æŒ‡æ ‡â€”å¿å€ºèƒ½åŠ›"
                }
            },
            
            # 4. æŠ€æœ¯å› å­æ•°æ® (ä¸­ä¼˜å…ˆçº§)
            "factor_data": {
                "priority": 4,
                "apis": {
                    "getStockFactorsDateRange": "è‚¡ç¥¨å› å­æ•°æ®æ—¶é—´åºåˆ—",
                    "getStockFactorsOneDay": "å•æ—¥è‚¡ç¥¨å› å­æ•°æ®"
                }
            },
            
            # 5. å¸‚åœºå¾®è§‚ç»“æ„ (ä¸­ä¼˜å…ˆçº§)  
            "market_structure": {
                "priority": 5,
                "apis": {
                    "getMktBlockd": "æ²ªæ·±å¤§å®—äº¤æ˜“",
                    "getFstTotal": "æ²ªæ·±èèµ„èåˆ¸æ¯æ—¥æ±‡æ€»",
                    "getFstDetail": "æ²ªæ·±èèµ„èåˆ¸æ¯æ—¥æ˜ç»†",
                    "getMktLimit": "æ²ªæ·±æ¶¨è·Œåœé™åˆ¶",
                    "getSecHalt": "æ²ªæ·±è¯åˆ¸åœå¤ç‰Œ"
                }
            },
            
            # 6. åŸºé‡‘æ•°æ® (ä¸­ä¼˜å…ˆçº§)
            "fund_data": {
                "priority": 6,
                "apis": {
                    "getFund": "åŸºé‡‘åŸºæœ¬ä¿¡æ¯",
                    "getFundNav": "åŸºé‡‘å†å²å‡€å€¼",
                    "getFundHoldings": "åŸºé‡‘æŒä»“æ˜ç»†",
                    "getMktFundd": "åŸºé‡‘æ—¥è¡Œæƒ…"
                }
            },
            
            # 7. æŒ‡æ•°ä¸è¡Œä¸šæ•°æ® (ä¸­ä¼˜å…ˆçº§)
            "index_industry": {
                "priority": 7,
                "apis": {
                    "getIdx": "æŒ‡æ•°åŸºæœ¬ä¿¡æ¯", 
                    "getIdxCons": "æŒ‡æ•°æˆåˆ†æ„æˆ",
                    "getIdxCloseWeight": "æŒ‡æ•°æˆåˆ†è‚¡æƒé‡",
                    "getMktIndustryFlow": "è¡Œä¸šæ—¥èµ„é‡‘æµå‘",
                    "getMktIndustryEval": "è¡Œä¸šä¼°å€¼ä¿¡æ¯"
                }
            },
            
            # 8. å…¬å¸æ²»ç†ä¸äº‹ä»¶ (ä½ä¼˜å…ˆçº§)
            "corporate_events": {
                "priority": 8,
                "apis": {
                    "getEquShTen": "å…¬å¸åå¤§è‚¡ä¸œ",
                    "getEquFloatShTen": "å…¬å¸åå¤§æµé€šè‚¡ä¸œ",
                    "getEquManagers": "ä¸Šå¸‚å…¬å¸ç®¡ç†å±‚",
                    "getEquSharesFloat": "é™å”®è‚¡è§£ç¦",
                    "getEquSpo": "å¢å‘ä¿¡æ¯"
                }
            }
        }
    
    def get_stock_list(self):
        """è·å–è‚¡ç¥¨åˆ—è¡¨"""
        logging.info("ğŸ“‹ è·å–Aè‚¡è‚¡ç¥¨åˆ—è¡¨...")
        
        try:
            stocks = uqer.DataAPI.EquGet(
                field='secID,ticker,secShortName,exchangeCD,listStatusCD,listDate,delistDate'
            )
            
            if stocks is not None and not stocks.empty:
                # è¿‡æ»¤Aè‚¡ä¸Šå¸‚è‚¡ç¥¨
                a_stocks = stocks[stocks['listStatusCD'] == 'L'].copy()
                
                logging.info(f"âœ… è·å–åˆ° {len(a_stocks)} åªAè‚¡")
                
                # ä¿å­˜è‚¡ç¥¨åˆ—è¡¨
                stock_list_file = self.data_dir / "stock_list.csv"
                a_stocks.to_csv(stock_list_file, index=False)
                logging.info(f"ğŸ“ è‚¡ç¥¨åˆ—è¡¨ä¿å­˜è‡³: {stock_list_file}")
                
                return a_stocks
            else:
                logging.error("âŒ è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥")
                return None
                
        except Exception as e:
            logging.error(f"âŒ è·å–è‚¡ç¥¨åˆ—è¡¨å¼‚å¸¸: {e}")
            return None
    
    def download_market_data(self):
        """ä¸‹è½½æ ¸å¿ƒè¡Œæƒ…æ•°æ®"""
        logging.info("ğŸ“Š å¼€å§‹ä¸‹è½½æ ¸å¿ƒè¡Œæƒ…æ•°æ®...")
        
        # è·å–è‚¡ç¥¨åˆ—è¡¨
        stocks = self.get_stock_list()
        if stocks is None:
            return False
        
        # åˆ›å»ºè¡Œæƒ…æ•°æ®ç›®å½•
        market_dir = self.data_dir / "market_data"
        market_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¸‹è½½æ—¥è¡Œæƒ…æ•°æ®
        return self._download_daily_market_data(stocks, market_dir)
    
    def _download_daily_market_data(self, stocks, market_dir):
        """ä¸‹è½½æ—¥è¡Œæƒ…æ•°æ®"""
        logging.info("ğŸ“ˆ ä¸‹è½½è‚¡ç¥¨æ—¥è¡Œæƒ…æ•°æ®...")
        
        try:
            # åˆ†æ‰¹ä¸‹è½½ï¼Œæ¯æ¬¡100åªè‚¡ç¥¨
            batch_size = 100
            stock_batches = [stocks[i:i+batch_size] for i in range(0, len(stocks), batch_size)]
            
            success_count = 0
            failed_count = 0
            
            for batch_idx, batch_stocks in enumerate(stock_batches):
                logging.info(f"ğŸ“¦ å¤„ç†ç¬¬ {batch_idx+1}/{len(stock_batches)} æ‰¹è‚¡ç¥¨...")
                
                # æ„å»ºtickeråˆ—è¡¨
                tickers = ','.join(batch_stocks['ticker'].tolist())
                
                try:
                    # è°ƒç”¨ä¼˜çŸ¿API
                    data = uqer.DataAPI.MktEqudGet(
                        secID='',
                        ticker=tickers,
                        beginDate=self.start_date,
                        endDate=self.end_date,
                        field='secID,ticker,tradeDate,preClosePrice,openPrice,highestPrice,lowestPrice,closePrice,turnoverVol,negMarketValue,dealAmount,turnoverRate,marketValue'
                    )
                    
                    if data is not None and not data.empty:
                        # ä¿å­˜æ•°æ®
                        batch_file = market_dir / f"daily_batch_{batch_idx+1}.csv"
                        data.to_csv(batch_file, index=False)
                        
                        logging.info(f"âœ… æ‰¹æ¬¡ {batch_idx+1} æˆåŠŸ: {len(data)} æ¡è®°å½•")
                        success_count += len(batch_stocks)
                        
                    else:
                        logging.warning(f"âš ï¸ æ‰¹æ¬¡ {batch_idx+1} æ•°æ®ä¸ºç©º")
                        failed_count += len(batch_stocks)
                    
                    # é˜²æ­¢APIé™åˆ¶ï¼Œæ·»åŠ å»¶è¿Ÿ
                    time.sleep(0.2)
                    
                except Exception as e:
                    logging.error(f"âŒ æ‰¹æ¬¡ {batch_idx+1} ä¸‹è½½å¤±è´¥: {e}")
                    failed_count += len(batch_stocks)
                    continue
            
            logging.info(f"ğŸ“Š æ—¥è¡Œæƒ…ä¸‹è½½å®Œæˆ: æˆåŠŸ {success_count} åª, å¤±è´¥ {failed_count} åª")
            return success_count > 0
            
        except Exception as e:
            logging.error(f"âŒ æ—¥è¡Œæƒ…ä¸‹è½½å¼‚å¸¸: {e}")
            return False
    
    def download_financial_data(self):
        """ä¸‹è½½è´¢åŠ¡æ•°æ®"""
        logging.info("ğŸ’° å¼€å§‹ä¸‹è½½è´¢åŠ¡æ•°æ®...")
        
        # åˆ›å»ºè´¢åŠ¡æ•°æ®ç›®å½•
        financial_dir = self.data_dir / "financial_data"
        financial_dir.mkdir(parents=True, exist_ok=True)
        
        financial_apis = self.core_apis["financial_data"]["apis"]
        
        for api_name, description in financial_apis.items():
            logging.info(f"ğŸ“‹ ä¸‹è½½ {description}...")
            
            try:
                # è°ƒç”¨API
                api_func = getattr(uqer.DataAPI, api_name)
                data = api_func(
                    publishDateBegin=self.start_date,
                    publishDateEnd=self.end_date
                )
                
                if data is not None and not data.empty:
                    # ä¿å­˜æ•°æ®
                    file_path = financial_dir / f"{api_name}.csv"
                    data.to_csv(file_path, index=False)
                    
                    logging.info(f"âœ… {description} æˆåŠŸ: {len(data)} æ¡è®°å½•")
                else:
                    logging.warning(f"âš ï¸ {description} æ•°æ®ä¸ºç©º")
                
                # APIé™åˆ¶å»¶è¿Ÿ
                time.sleep(0.5)
                
            except Exception as e:
                logging.error(f"âŒ {description} ä¸‹è½½å¤±è´¥: {e}")
                continue
    
    def create_download_plan(self):
        """åˆ›å»ºè¯¦ç»†ä¸‹è½½è®¡åˆ’"""
        plan = {
            "download_plan": {
                "total_apis": sum(len(category["apis"]) for category in self.core_apis.values()),
                "estimated_time_hours": 24,  # é¢„ä¼°24å°æ—¶
                "data_size_gb": 50,  # é¢„ä¼°50GB
                "date_range": f"{self.start_date} åˆ° {self.end_date}",
                "categories": {}
            }
        }
        
        for category, info in self.core_apis.items():
            plan["download_plan"]["categories"][category] = {
                "priority": info["priority"],
                "api_count": len(info["apis"]),
                "estimated_time_hours": len(info["apis"]) * 0.5,
                "apis": info["apis"]
            }
        
        return plan
    
    def execute_comprehensive_download(self):
        """æ‰§è¡Œå…¨é¢æ•°æ®ä¸‹è½½"""
        logging.info("ğŸš€ å¼€å§‹æ‰§è¡Œå…¨é¢æ•°æ®ä¸‹è½½è®¡åˆ’...")
        logging.info(f"ğŸ“… æ—¶é—´èŒƒå›´: {self.start_date} - {self.end_date}")
        
        # åˆ›å»ºä¸‹è½½è®¡åˆ’
        plan = self.create_download_plan()
        logging.info(f"ğŸ“Š ä¸‹è½½è®¡åˆ’: {plan['download_plan']['total_apis']} ä¸ªAPIæ¥å£")
        
        results = {}
        
        # æŒ‰ä¼˜å…ˆçº§æ’åºæ‰§è¡Œ
        sorted_categories = sorted(
            self.core_apis.items(),
            key=lambda x: x[1]["priority"]
        )
        
        for category_name, category_info in sorted_categories:
            logging.info(f"ğŸ¯ æ‰§è¡Œåˆ†ç±»: {category_name} (ä¼˜å…ˆçº§: {category_info['priority']})")
            
            if category_name == "market_data":
                results[category_name] = self.download_market_data()
            elif category_name == "financial_data":
                results[category_name] = self.download_financial_data()
            else:
                # å…¶ä»–åˆ†ç±»çš„ä¸‹è½½é€»è¾‘
                results[category_name] = self._download_category_data(category_name, category_info)
        
        # ç”Ÿæˆä¸‹è½½æŠ¥å‘Š
        self._generate_download_report(results)
        
        return results
    
    def _download_category_data(self, category_name, category_info):
        """ä¸‹è½½ç‰¹å®šåˆ†ç±»æ•°æ®"""
        logging.info(f"ğŸ“ å¼€å§‹ä¸‹è½½ {category_name} åˆ†ç±»æ•°æ®...")
        
        # åˆ›å»ºåˆ†ç±»ç›®å½•
        category_dir = self.data_dir / category_name
        category_dir.mkdir(parents=True, exist_ok=True)
        
        success_count = 0
        total_count = len(category_info["apis"])
        
        for api_name, description in category_info["apis"].items():
            try:
                logging.info(f"ğŸ“‹ ä¸‹è½½ {description}...")
                
                # æ ¹æ®APIç±»å‹è°ƒç”¨ä¸åŒçš„ä¸‹è½½æ–¹æ³•
                success = self._download_single_api(api_name, description, category_dir)
                if success:
                    success_count += 1
                
                # APIé™åˆ¶å»¶è¿Ÿ
                time.sleep(0.3)
                
            except Exception as e:
                logging.error(f"âŒ {description} ä¸‹è½½å¼‚å¸¸: {e}")
                continue
        
        logging.info(f"ğŸ“Š {category_name} å®Œæˆ: {success_count}/{total_count}")
        return success_count > 0
    
    def _download_single_api(self, api_name, description, save_dir):
        """ä¸‹è½½å•ä¸ªAPIæ•°æ®"""
        try:
            # è·å–APIå‡½æ•°
            api_func = getattr(uqer.DataAPI, api_name, None)
            if not api_func:
                logging.warning(f"âš ï¸ API {api_name} ä¸å­˜åœ¨")
                return False
            
            # æ ¹æ®APIç±»å‹è®¾ç½®å‚æ•°
            if "beginDate" in api_func.__code__.co_varnames:
                data = api_func(
                    beginDate=self.start_date,
                    endDate=self.end_date
                )
            else:
                data = api_func()
            
            if data is not None and not data.empty:
                # ä¿å­˜æ•°æ®
                file_path = save_dir / f"{api_name}.csv"
                data.to_csv(file_path, index=False)
                
                logging.info(f"âœ… {description}: {len(data)} æ¡è®°å½•")
                return True
            else:
                logging.warning(f"âš ï¸ {description}: æ•°æ®ä¸ºç©º")
                return False
                
        except Exception as e:
            logging.error(f"âŒ {description} ä¸‹è½½å¤±è´¥: {e}")
            return False
    
    def _generate_download_report(self, results):
        """ç”Ÿæˆä¸‹è½½æŠ¥å‘Š"""
        logging.info("ğŸ“„ ç”Ÿæˆä¸‹è½½æŠ¥å‘Š...")
        
        report = {
            "download_summary": {
                "start_time": datetime.now().isoformat(),
                "date_range": f"{self.start_date} - {self.end_date}",
                "categories": results,
                "success_categories": sum(1 for success in results.values() if success),
                "total_categories": len(results)
            }
        }
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = self.data_dir / "download_report.json"
        import json
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        logging.info(f"ğŸ“Š ä¸‹è½½æŠ¥å‘Šä¿å­˜è‡³: {report_file}")

if __name__ == "__main__":
    downloader = ComprehensiveDataDownloader()
    
    # åˆ›å»ºæ—¥å¿—ç›®å½•
    Path("logs").mkdir(exist_ok=True)
    
    # æ‰§è¡Œä¸‹è½½
    results = downloader.execute_comprehensive_download()
    
    print("\nğŸ‰ å…¨é¢æ•°æ®ä¸‹è½½è®¡åˆ’æ‰§è¡Œå®Œæˆ!")
    print(f"ğŸ“Š ç»“æœ: {results}")