#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»Ÿä¸€æ•°æ®è®¿é—®æ¥å£ - æ•´åˆæ‰€æœ‰CSVæ•°æ®æºçš„ç»Ÿä¸€è®¿é—®å…¥å£
"""

import pandas as pd
from pathlib import Path
import json
import logging
from datetime import datetime
from typing import Optional, List, Dict, Union

class UnifiedDataAccess:
    """ç»Ÿä¸€æ•°æ®è®¿é—®æ¥å£"""
    
    def __init__(self):
        # æ•°æ®æºä¼˜å…ˆçº§ï¼ˆæ•°å­—è¶Šå°ä¼˜å…ˆçº§è¶Šé«˜ï¼‰
        self.data_sources = {
            "final_comprehensive_download": {
                "path": Path("data/final_comprehensive_download"),
                "description": "å®Œæ•´ä¸‹è½½æ•°æ® (ä¸»æ•°æ®æº)",
                "priority": 1
            },
            "optimized_data": {
                "path": Path("data/optimized_data"),
                "description": "ä¼˜åŒ–CSVæ•°æ®",
                "priority": 2
            },
            "priority_download": {
                "path": Path("data/priority_download"),
                "description": "ä¼˜å…ˆä¸‹è½½æ•°æ®",
                "priority": 3
            },
            "historical_download": {
                "path": Path("data/historical_download"),
                "description": "å†å²åŸºç¡€æ•°æ®",
                "priority": 4
            }
        }
        
        # ä»åˆ†ææŠ¥å‘ŠåŠ è½½æ•°æ®æ˜ å°„
        self.load_data_mapping()
        self.setup_logging()
    
    def load_data_mapping(self):
        """åŠ è½½æ•°æ®æ˜ å°„é…ç½®"""
        report_file = Path("csv_data_comprehensive_report.json")
        if report_file.exists():
            with open(report_file, 'r', encoding='utf-8') as f:
                report = json.load(f)
            
            # ä»æŠ¥å‘Šä¸­æå–æœ€ä½³æ•°æ®æºæ˜ å°„
            self.api_mapping = {}
            consolidation_plan = report.get("consolidation_plan", {})
            target_structure = consolidation_plan.get("target_structure", {})
            
            for category, apis in target_structure.items():
                if category not in self.api_mapping:
                    self.api_mapping[category] = {}
                
                for api_name, info in apis.items():
                    self.api_mapping[category][api_name] = {
                        "source_name": info["source_name"],
                        "source_path": Path(info["source_path"]),
                        "files": info["files"],
                        "size_mb": info["size_mb"]
                    }
        else:
            # å¦‚æœæ²¡æœ‰æŠ¥å‘Šï¼Œé»˜è®¤ä½¿ç”¨final_comprehensive_download
            logging.warning("æœªæ‰¾åˆ°æ•°æ®åˆ†ææŠ¥å‘Šï¼Œä½¿ç”¨é»˜è®¤æ˜ å°„")
            self.api_mapping = {}
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
    
    def get_available_categories(self) -> List[str]:
        """è·å–æ‰€æœ‰å¯ç”¨çš„æ•°æ®ç±»åˆ«"""
        categories = set()
        
        for source_info in self.data_sources.values():
            source_path = source_info["path"]
            if source_path.exists():
                for category_dir in source_path.iterdir():
                    if category_dir.is_dir():
                        categories.add(category_dir.name)
        
        return sorted(list(categories))
    
    def get_available_apis(self, category: str) -> List[str]:
        """è·å–æŒ‡å®šç±»åˆ«ä¸‹çš„æ‰€æœ‰API"""
        apis = set()
        
        for source_info in self.data_sources.values():
            category_path = source_info["path"] / category
            if category_path.exists():
                for api_dir in category_path.iterdir():
                    if api_dir.is_dir() and list(api_dir.glob("*.csv")):
                        apis.add(api_dir.name)
        
        return sorted(list(apis))
    
    def get_available_files(self, category: str, api: str) -> List[Dict]:
        """è·å–æŒ‡å®šAPIä¸‹çš„æ‰€æœ‰æ–‡ä»¶åŠå…¶æ¥æºä¿¡æ¯"""
        files_info = []
        
        for source_name, source_info in self.data_sources.items():
            api_path = source_info["path"] / category / api
            if api_path.exists():
                csv_files = list(api_path.glob("*.csv"))
                for csv_file in csv_files:
                    files_info.append({
                        "filename": csv_file.name,
                        "source": source_name,
                        "path": str(csv_file),
                        "size_mb": csv_file.stat().st_size / 1024 / 1024,
                        "priority": source_info["priority"]
                    })
        
        # æŒ‰ä¼˜å…ˆçº§å’Œæ–‡ä»¶åæ’åº
        files_info.sort(key=lambda x: (x["priority"], x["filename"]))
        
        return files_info
    
    def find_best_data_source(self, category: str, api: str, filename: Optional[str] = None) -> Optional[Path]:
        """æ‰¾åˆ°æœ€ä½³çš„æ•°æ®æºè·¯å¾„"""
        
        # 1. å¦‚æœæœ‰æ˜ å°„é…ç½®ï¼Œä¼˜å…ˆä½¿ç”¨
        if category in self.api_mapping and api in self.api_mapping[category]:
            mapped_source = self.api_mapping[category][api]
            source_path = mapped_source["source_path"]
            
            if filename:
                file_path = source_path / filename
                if not file_path.suffix:
                    file_path = file_path.with_suffix('.csv')
                if file_path.exists():
                    return file_path
            else:
                # è¿”å›ç›®å½•ä¸‹çš„ç¬¬ä¸€ä¸ªCSVæ–‡ä»¶
                csv_files = list(source_path.glob("*.csv"))
                if csv_files:
                    return csv_files[0]
        
        # 2. æŒ‰ä¼˜å…ˆçº§æŸ¥æ‰¾
        for source_name, source_info in sorted(self.data_sources.items(), 
                                               key=lambda x: x[1]["priority"]):
            api_path = source_info["path"] / category / api
            
            if not api_path.exists():
                continue
            
            if filename:
                file_path = api_path / filename
                if not file_path.suffix:
                    file_path = file_path.with_suffix('.csv')
                if file_path.exists():
                    return file_path
            else:
                # è¿”å›ç›®å½•ä¸‹çš„ç¬¬ä¸€ä¸ªCSVæ–‡ä»¶
                csv_files = list(api_path.glob("*.csv"))
                if csv_files:
                    return csv_files[0]
        
        return None
    
    def read_data(self, 
                  category: str, 
                  api: str, 
                  filename: Optional[str] = None,
                  max_rows: Optional[int] = None,
                  columns: Optional[List[str]] = None,
                  date_range: Optional[tuple] = None,
                  use_chunks: bool = False,
                  chunk_size: int = 10000) -> Optional[pd.DataFrame]:
        """è¯»å–æ•°æ®çš„ä¸»è¦æ¥å£"""
        
        logging.info(f"ğŸ“Š è¯»å–æ•°æ®: {category}/{api}")
        if filename:
            logging.info(f"   æŒ‡å®šæ–‡ä»¶: {filename}")
        
        # æ‰¾åˆ°æœ€ä½³æ•°æ®æº
        file_path = self.find_best_data_source(category, api, filename)
        
        if not file_path:
            logging.error(f"âŒ æœªæ‰¾åˆ°æ•°æ®: {category}/{api}")
            if filename:
                logging.error(f"   æ–‡ä»¶: {filename}")
            return None
        
        logging.info(f"ğŸ“– æ•°æ®æº: {file_path}")
        
        try:
            # è·å–æ–‡ä»¶ä¿¡æ¯
            file_size_mb = file_path.stat().st_size / 1024 / 1024
            logging.info(f"ğŸ“Š æ–‡ä»¶å¤§å°: {file_size_mb:.2f} MB")
            
            # é€‰æ‹©è¯»å–ç­–ç•¥
            if use_chunks or file_size_mb > 100:  # å¤§äº100MBä½¿ç”¨åˆ†å—
                logging.info("ğŸ“‹ ä½¿ç”¨åˆ†å—è¯»å–ç­–ç•¥")
                df = self._read_large_file(file_path, max_rows, columns, chunk_size)
            else:
                logging.info("ğŸ“‹ ä½¿ç”¨æ ‡å‡†è¯»å–ç­–ç•¥")
                df = pd.read_csv(file_path, low_memory=False)
                
                if columns:
                    available_cols = [col for col in columns if col in df.columns]
                    if available_cols:
                        df = df[available_cols]
                
                if max_rows and len(df) > max_rows:
                    df = df.head(max_rows)
            
            if df is None or df.empty:
                logging.error("âŒ è¯»å–çš„æ•°æ®ä¸ºç©º")
                return None
            
            # æ—¥æœŸè¿‡æ»¤
            if date_range:
                df = self._filter_by_date(df, date_range)
            
            logging.info(f"âœ… æˆåŠŸè¯»å–: {df.shape[0]:,} è¡Œ, {df.shape[1]} åˆ—")
            return df
            
        except Exception as e:
            logging.error(f"âŒ è¯»å–æ•°æ®å¤±è´¥: {e}")
            return None
    
    def _read_large_file(self, file_path: Path, max_rows: Optional[int], 
                        columns: Optional[List[str]], chunk_size: int) -> Optional[pd.DataFrame]:
        """è¯»å–å¤§æ–‡ä»¶çš„åˆ†å—ç­–ç•¥"""
        chunk_list = []
        total_rows = 0
        
        try:
            for chunk in pd.read_csv(file_path, chunksize=chunk_size, low_memory=False):
                if columns:
                    available_cols = [col for col in columns if col in chunk.columns]
                    if available_cols:
                        chunk = chunk[available_cols]
                
                chunk_list.append(chunk)
                total_rows += len(chunk)
                
                if max_rows and total_rows >= max_rows:
                    break
            
            if not chunk_list:
                return None
            
            df = pd.concat(chunk_list, ignore_index=True)
            
            if max_rows and len(df) > max_rows:
                df = df.head(max_rows)
            
            return df
            
        except Exception as e:
            logging.error(f"âŒ åˆ†å—è¯»å–å¤±è´¥: {e}")
            return None
    
    def _filter_by_date(self, df: pd.DataFrame, date_range: tuple) -> pd.DataFrame:
        """æŒ‰æ—¥æœŸèŒƒå›´è¿‡æ»¤æ•°æ®"""
        try:
            start_date, end_date = date_range
            
            # å¯»æ‰¾æ—¥æœŸåˆ—
            date_columns = [col for col in df.columns if 'date' in col.lower()]
            
            if not date_columns:
                logging.warning("âš ï¸ æœªæ‰¾åˆ°æ—¥æœŸåˆ—ï¼Œè·³è¿‡æ—¥æœŸè¿‡æ»¤")
                return df
            
            # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ—¥æœŸåˆ—è¿›è¡Œè¿‡æ»¤
            date_col = date_columns[0]
            df[date_col] = pd.to_datetime(df[date_col], errors='coerce')
            
            # è¿‡æ»¤
            mask = (df[date_col] >= start_date) & (df[date_col] <= end_date)
            filtered_df = df[mask]
            
            logging.info(f"ğŸ“… æ—¥æœŸè¿‡æ»¤: {len(df):,} â†’ {len(filtered_df):,} è¡Œ")
            return filtered_df
            
        except Exception as e:
            logging.warning(f"âš ï¸ æ—¥æœŸè¿‡æ»¤å¤±è´¥: {e}ï¼Œè¿”å›åŸå§‹æ•°æ®")
            return df
    
    def get_data_info(self, category: str, api: str) -> Dict:
        """è·å–æ•°æ®ä¿¡æ¯"""
        info = {
            "category": category,
            "api": api,
            "available_files": [],
            "total_files": 0,
            "total_size_mb": 0,
            "data_sources": [],
            "recommended_source": None
        }
        
        files_info = self.get_available_files(category, api)
        info["available_files"] = files_info
        info["total_files"] = len(files_info)
        info["total_size_mb"] = sum(f["size_mb"] for f in files_info)
        
        # è·å–æ‰€æœ‰æ•°æ®æº
        sources = list(set(f["source"] for f in files_info))
        info["data_sources"] = sources
        
        # æ¨èæœ€é«˜ä¼˜å…ˆçº§çš„æ•°æ®æº
        if files_info:
            info["recommended_source"] = files_info[0]["source"]
        
        return info
    
    def show_data_catalog(self):
        """æ˜¾ç¤ºæ•°æ®ç›®å½•"""
        print("ğŸ“š **ç»Ÿä¸€æ•°æ®è®¿é—®ç›®å½•**")
        print("=" * 60)
        
        categories = self.get_available_categories()
        
        total_apis = 0
        total_sources = 0
        
        for category in categories:
            apis = self.get_available_apis(category)
            print(f"\nğŸ“‚ **{category}** ({len(apis)} ä¸ªAPI)")
            
            for api in apis[:5]:  # æ˜¾ç¤ºå‰5ä¸ªAPI
                info = self.get_data_info(category, api)
                sources_str = ", ".join(info["data_sources"])
                print(f"  ğŸ”Œ {api}: {info['total_files']} æ–‡ä»¶, {info['total_size_mb']:.1f}MB")
                print(f"     ğŸ“ æ¥æº: {sources_str}")
                print(f"     â­ æ¨è: {info['recommended_source']}")
            
            if len(apis) > 5:
                print(f"  ... è¿˜æœ‰ {len(apis) - 5} ä¸ªAPI")
            
            total_apis += len(apis)
            total_sources += len(set(f["source"] for api in apis for f in self.get_available_files(category, api)))
        
        print(f"\nğŸ¯ **æ€»è®¡**: {len(categories)} ä¸ªç±»åˆ«, {total_apis} ä¸ªAPI")
        print(f"ğŸ“¦ **æ•°æ®æº**: {len(self.data_sources)} ä¸ªç›®å½•")

def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºç»Ÿä¸€æ•°æ®è®¿é—®åŠŸèƒ½"""
    accessor = UnifiedDataAccess()
    
    print("ğŸš€ **ç»Ÿä¸€æ•°æ®è®¿é—®ç³»ç»Ÿæ¼”ç¤º**\n")
    
    # 1. æ˜¾ç¤ºæ•°æ®ç›®å½•
    accessor.show_data_catalog()
    
    # 2. è¯»å–æ ·æœ¬æ•°æ®
    print("\n" + "="*60)
    print("ğŸ§ª **æ ·æœ¬æ•°æ®è¯»å–æ¼”ç¤º**:")
    
    # è¯»å–è´¢åŠ¡æ•°æ®
    df = accessor.read_data(
        category="financial",
        api="fdmtindipsget",
        max_rows=5
    )
    
    if df is not None:
        print("\nğŸ“„ **æ•°æ®é¢„è§ˆ**:")
        print(df.to_string())
    
    # 3. è·å–æ•°æ®ä¿¡æ¯
    print("\n" + "="*60)
    print("â„¹ï¸ **æ•°æ®ä¿¡æ¯æŸ¥è¯¢æ¼”ç¤º**:")
    
    info = accessor.get_data_info("financial", "fdmtindipsget")
    print(f"ğŸ“Š APIä¿¡æ¯: {info['category']}/{info['api']}")
    print(f"ğŸ“ æ–‡ä»¶æ•°é‡: {info['total_files']}")
    print(f"ğŸ’¾ æ€»å¤§å°: {info['total_size_mb']:.1f} MB")
    print(f"ğŸ“¦ æ•°æ®æº: {info['data_sources']}")
    print(f"â­ æ¨èæº: {info['recommended_source']}")
    
    print("\nğŸŠ ç»Ÿä¸€æ•°æ®è®¿é—®ç³»ç»Ÿæ¼”ç¤ºå®Œæˆï¼")
    print("ğŸ’¡ æç¤º: ç³»ç»Ÿä¼šè‡ªåŠ¨é€‰æ‹©æœ€ä½³æ•°æ®æºï¼Œç¡®ä¿æ•°æ®çš„å®Œæ•´æ€§å’Œæ—¶æ•ˆæ€§")

if __name__ == "__main__":
    main()