#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å‘¨çº¿ã€æœˆçº¿æ•°æ®æ•´ç†å’Œè¡¥å…¨å™¨
ç»Ÿä¸€ç»„ç»‡ç°æœ‰æ•°æ®å¹¶è¡¥å…¨ç¼ºå¤±éƒ¨åˆ†
"""

import sys
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime, timedelta
import warnings
import json
import time
import os
from io import StringIO
import concurrent.futures
from threading import Lock
warnings.filterwarnings('ignore')

try:
    import uqer
    print("âœ… UQER API å¯ç”¨")
    UQER_AVAILABLE = True
except ImportError:
    print("âŒ UQER API ä¸å¯ç”¨")
    UQER_AVAILABLE = False
    sys.exit(1)

class WeeklyMonthlyDataOrganizer:
    """å‘¨çº¿ã€æœˆçº¿æ•°æ®æ•´ç†å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ•´ç†å™¨"""
        self.setup_uqer()
        self.setup_paths()
        self.stats = {
            'start_time': datetime.now(),
            'weekly_files_found': 0,
            'monthly_files_found': 0,
            'stocks_organized': 0,
            'weekly_gaps_filled': 0,
            'monthly_gaps_filled': 0
        }
        
    def setup_uqer(self):
        """è®¾ç½®UQERè¿æ¥"""
        try:
            uqer_token = "68b9922817ae6273137bda7acba81e293582ba347281dfcc056dcb245b23faf3"
            uqer.Client(token=uqer_token)
            print("âœ… UQERè¿æ¥æˆåŠŸ")
            self.uqer_connected = True
        except Exception as e:
            print(f"âŒ UQERè¿æ¥å¤±è´¥: {e}")
            self.uqer_connected = False
            sys.exit(1)
    
    def setup_paths(self):
        """è®¾ç½®è·¯å¾„"""
        # ç°æœ‰æ•°æ®è·¯å¾„
        self.existing_weekly_path = Path("/Users/jackstudio/QuantTrade/data/priority_download/market_data/weekly")
        self.existing_monthly_path = Path("/Users/jackstudio/QuantTrade/data/priority_download/market_data/monthly")
        
        # æ–°çš„ç»Ÿä¸€æ•°æ®è·¯å¾„
        self.base_path = Path("/Users/jackstudio/QuantTrade/data/csv_complete")
        self.base_path.mkdir(exist_ok=True)
        
        # å‘¨çº¿æ•°æ®ç›®å½•
        self.weekly_path = self.base_path / "weekly"
        self.weekly_path.mkdir(exist_ok=True)
        self.weekly_stocks_path = self.weekly_path / "stocks"
        self.weekly_stocks_path.mkdir(exist_ok=True)
        
        # æœˆçº¿æ•°æ®ç›®å½•
        self.monthly_path = self.base_path / "monthly"
        self.monthly_path.mkdir(exist_ok=True)
        self.monthly_stocks_path = self.monthly_path / "stocks"
        self.monthly_stocks_path.mkdir(exist_ok=True)
        
        print(f"ğŸ“ æ•°æ®æ•´ç†è·¯å¾„:")
        print(f"   ğŸ“Š å‘¨çº¿: {self.weekly_path}")
        print(f"   ğŸ“… æœˆçº¿: {self.monthly_path}")
    
    def scan_existing_data(self):
        """æ‰«æç°æœ‰æ•°æ®"""
        print("ğŸ” æ‰«æç°æœ‰å‘¨çº¿ã€æœˆçº¿æ•°æ®...")
        
        # æ‰«æå‘¨çº¿æ–‡ä»¶
        weekly_files = list(self.existing_weekly_path.glob("*.csv"))
        monthly_files = list(self.existing_monthly_path.glob("*.csv"))
        
        self.stats['weekly_files_found'] = len(weekly_files)
        self.stats['monthly_files_found'] = len(monthly_files)
        
        print(f"   ğŸ“Š å‘¨çº¿æ–‡ä»¶: {len(weekly_files)} ä¸ª")
        print(f"   ğŸ“… æœˆçº¿æ–‡ä»¶: {len(monthly_files)} ä¸ª")
        
        return weekly_files, monthly_files
    
    def extract_stock_data(self, files, data_type="weekly"):
        """ä»æ‰¹æ¬¡æ–‡ä»¶ä¸­æå–æŒ‰è‚¡ç¥¨ç»„ç»‡çš„æ•°æ®"""
        print(f"ğŸ“¦ æ•´ç†{data_type}æ•°æ®...")
        
        all_stocks_data = {}
        
        for file_path in files:
            try:
                df = pd.read_csv(file_path)
                
                if 'secID' not in df.columns or 'endDate' not in df.columns:
                    continue
                
                # æŒ‰è‚¡ç¥¨åˆ†ç»„
                for stock_id, stock_data in df.groupby('secID'):
                    if stock_id not in all_stocks_data:
                        all_stocks_data[stock_id] = []
                    
                    all_stocks_data[stock_id].append(stock_data)
                    
            except Exception as e:
                print(f"   âŒ å¤„ç†æ–‡ä»¶å¤±è´¥: {file_path.name}")
                continue
        
        # åˆå¹¶æ¯åªè‚¡ç¥¨çš„æ•°æ®
        organized_data = {}
        for stock_id, data_list in all_stocks_data.items():
            try:
                combined_df = pd.concat(data_list, ignore_index=True)
                combined_df['endDate'] = pd.to_datetime(combined_df['endDate'])
                combined_df = combined_df.drop_duplicates(subset=['endDate']).sort_values('endDate')
                organized_data[stock_id] = combined_df
            except Exception as e:
                continue
        
        print(f"   âœ… æ•´ç†å®Œæˆ: {len(organized_data)} åªè‚¡ç¥¨")
        return organized_data
    
    def save_organized_data(self, organized_data, data_type="weekly"):
        """ä¿å­˜æ•´ç†åçš„æ•°æ®"""
        save_path = self.weekly_stocks_path if data_type == "weekly" else self.monthly_stocks_path
        
        print(f"ğŸ’¾ ä¿å­˜{data_type}æ•°æ®...")
        
        saved_count = 0
        for stock_id, df in organized_data.items():
            try:
                file_name = f"{stock_id.replace('.', '_')}.csv"
                file_path = save_path / file_name
                
                df.to_csv(file_path, index=False, encoding='utf-8')
                saved_count += 1
                
                if saved_count % 100 == 0:
                    print(f"   ğŸ“Š å·²ä¿å­˜: {saved_count}/{len(organized_data)}")
                    
            except Exception as e:
                continue
        
        print(f"   âœ… ä¿å­˜å®Œæˆ: {saved_count} ä¸ªæ–‡ä»¶")
        return saved_count
    
    def check_data_gaps(self, organized_data, data_type="weekly"):
        """æ£€æŸ¥æ•°æ®ç¼ºå£"""
        print(f"ğŸ” æ£€æŸ¥{data_type}æ•°æ®ç¼ºå£...")
        
        gaps_info = []
        target_end_date = pd.Timestamp('2025-08-31')
        
        for stock_id, df in organized_data.items():
            latest_date = df['endDate'].max()
            
            if latest_date < target_end_date:
                gap_days = (target_end_date - latest_date).days
                gaps_info.append({
                    'stock_id': stock_id,
                    'latest_date': latest_date,
                    'gap_days': gap_days
                })
        
        print(f"   ğŸ“Š éœ€è¦è¡¥å…¨çš„è‚¡ç¥¨: {len(gaps_info)}")
        return gaps_info
    
    def fill_data_gaps(self, gaps_info, data_type="weekly"):
        """è¡¥å…¨æ•°æ®ç¼ºå£"""
        if not gaps_info:
            print(f"   âœ… {data_type}æ•°æ®æ— éœ€è¡¥å…¨")
            return 0
        
        print(f"ğŸ”„ è¡¥å…¨{data_type}æ•°æ®ç¼ºå£...")
        
        # æ ¹æ®ç±»å‹é€‰æ‹©API
        if data_type == "weekly":
            api_func = uqer.DataAPI.MktEqudGet
            freq = 'W'
        else:  # monthly
            api_func = uqer.DataAPI.MktEqudGet
            freq = 'M'
        
        filled_count = 0
        save_path = self.weekly_stocks_path if data_type == "weekly" else self.monthly_stocks_path
        
        for i, gap_info in enumerate(gaps_info[:50], 1):  # é™åˆ¶æ•°é‡é¿å…APIè¶…é™
            stock_id = gap_info['stock_id']
            start_date = gap_info['latest_date'] + pd.Timedelta(days=1)
            
            try:
                print(f"   ğŸ“ˆ [{i}/50] è¡¥å…¨: {stock_id}")
                
                # è·å–æ—¥çº¿æ•°æ®ç„¶åè½¬æ¢
                result = api_func(
                    secID=stock_id,
                    beginDate=start_date.strftime('%Y%m%d'),
                    endDate='20250831',
                    pandas=1
                )
                
                if result is None:
                    continue
                
                # å¤„ç†APIè¿”å›çš„æ•°æ®
                if isinstance(result, str):
                    daily_df = pd.read_csv(StringIO(result))
                elif isinstance(result, pd.DataFrame):
                    daily_df = result.copy()
                else:
                    continue
                
                if len(daily_df) == 0:
                    continue
                
                # è½¬æ¢ä¸ºå‘¨çº¿æˆ–æœˆçº¿
                converted_df = self.convert_to_period(daily_df, freq)
                
                if converted_df is not None and len(converted_df) > 0:
                    # è¯»å–ç°æœ‰æ•°æ®å¹¶åˆå¹¶
                    file_path = save_path / f"{stock_id.replace('.', '_')}.csv"
                    
                    if file_path.exists():
                        existing_df = pd.read_csv(file_path)
                        existing_df['endDate'] = pd.to_datetime(existing_df['endDate'])
                        
                        # åˆå¹¶æ•°æ®
                        combined_df = pd.concat([existing_df, converted_df], ignore_index=True)
                        combined_df = combined_df.drop_duplicates(subset=['endDate']).sort_values('endDate')
                        
                        combined_df.to_csv(file_path, index=False, encoding='utf-8')
                        filled_count += 1
                
                time.sleep(0.2)  # APIé™é€Ÿ
                
            except Exception as e:
                print(f"   âŒ è¡¥å…¨å¤±è´¥: {stock_id}")
                continue
        
        print(f"   âœ… è¡¥å…¨å®Œæˆ: {filled_count} åªè‚¡ç¥¨")
        return filled_count
    
    def convert_to_period(self, daily_df, freq='W'):
        """å°†æ—¥çº¿æ•°æ®è½¬æ¢ä¸ºå‘¨çº¿æˆ–æœˆçº¿"""
        try:
            if 'tradeDate' not in daily_df.columns:
                return None
            
            daily_df['tradeDate'] = pd.to_datetime(daily_df['tradeDate'])
            daily_df = daily_df.set_index('tradeDate')
            
            # é‡é‡‡æ ·è§„åˆ™
            agg_rules = {
                'openPrice': 'first',
                'highestPrice': 'max',
                'lowestPrice': 'min', 
                'closePrice': 'last',
                'turnoverVol': 'sum',
                'turnoverValue': 'sum'
            }
            
            # å¤„ç†åˆ—åå·®å¼‚
            if 'highPrice' in daily_df.columns:
                agg_rules['highPrice'] = 'max'
                agg_rules.pop('highestPrice')
            if 'lowPrice' in daily_df.columns:
                agg_rules['lowPrice'] = 'min'
                agg_rules.pop('lowestPrice')
            if 'volume' in daily_df.columns:
                agg_rules['volume'] = 'sum'
                agg_rules.pop('turnoverVol')
            if 'amount' in daily_df.columns:
                agg_rules['amount'] = 'sum'
                agg_rules.pop('turnoverValue')
            
            # é‡é‡‡æ ·
            resampled = daily_df.resample(freq).agg(agg_rules).dropna()
            
            # é‡ç½®ç´¢å¼•å¹¶é‡å‘½å
            resampled = resampled.reset_index()
            resampled = resampled.rename(columns={'tradeDate': 'endDate'})
            
            # æ·»åŠ åŸºç¡€å­—æ®µ
            if len(daily_df) > 0:
                resampled['secID'] = daily_df['secID'].iloc[0] if 'secID' in daily_df.columns else ""
                resampled['ticker'] = daily_df['ticker'].iloc[0] if 'ticker' in daily_df.columns else ""
            
            return resampled
            
        except Exception as e:
            return None
    
    def organize_all_data(self):
        """æ•´ç†æ‰€æœ‰å‘¨çº¿ã€æœˆçº¿æ•°æ®"""
        print("ğŸš€ å¼€å§‹æ•´ç†å‘¨çº¿ã€æœˆçº¿æ•°æ®")
        print("=" * 80)
        
        # æ‰«æç°æœ‰æ•°æ®
        weekly_files, monthly_files = self.scan_existing_data()
        
        if not weekly_files and not monthly_files:
            print("âŒ æœªæ‰¾åˆ°ç°æœ‰å‘¨çº¿ã€æœˆçº¿æ•°æ®")
            return
        
        # æ•´ç†å‘¨çº¿æ•°æ®
        if weekly_files:
            print("\nğŸ“Š å¤„ç†å‘¨çº¿æ•°æ®:")
            weekly_data = self.extract_stock_data(weekly_files, "weekly")
            weekly_saved = self.save_organized_data(weekly_data, "weekly")
            weekly_gaps = self.check_data_gaps(weekly_data, "weekly")
            weekly_filled = self.fill_data_gaps(weekly_gaps, "weekly")
            
            self.stats['stocks_organized'] = max(self.stats['stocks_organized'], len(weekly_data))
            self.stats['weekly_gaps_filled'] = weekly_filled
        
        # æ•´ç†æœˆçº¿æ•°æ®
        if monthly_files:
            print("\nğŸ“… å¤„ç†æœˆçº¿æ•°æ®:")
            monthly_data = self.extract_stock_data(monthly_files, "monthly") 
            monthly_saved = self.save_organized_data(monthly_data, "monthly")
            monthly_gaps = self.check_data_gaps(monthly_data, "monthly")
            monthly_filled = self.fill_data_gaps(monthly_gaps, "monthly")
            
            self.stats['stocks_organized'] = max(self.stats['stocks_organized'], len(monthly_data))
            self.stats['monthly_gaps_filled'] = monthly_filled
        
        # åˆ›å»ºæ€»ç»“æŠ¥å‘Š
        self.create_summary()
    
    def create_summary(self):
        """åˆ›å»ºæ•´ç†æ€»ç»“"""
        end_time = datetime.now()
        duration = end_time - self.stats['start_time']
        
        summary = {
            'organization_info': {
                'start_time': self.stats['start_time'].isoformat(),
                'end_time': end_time.isoformat(),
                'duration_minutes': round(duration.total_seconds() / 60, 2),
                'data_range': '2000å¹´1æœˆ1æ—¥-2025å¹´8æœˆ31æ—¥',
                'organization': 'æŒ‰è‚¡ç¥¨ç»„ç»‡çš„CSVæ–‡ä»¶'
            },
            'statistics': {
                'weekly_files_found': self.stats['weekly_files_found'],
                'monthly_files_found': self.stats['monthly_files_found'],
                'stocks_organized': self.stats['stocks_organized'],
                'weekly_gaps_filled': self.stats['weekly_gaps_filled'],
                'monthly_gaps_filled': self.stats['monthly_gaps_filled']
            },
            'file_structure': {
                'weekly_path': str(self.weekly_path),
                'monthly_path': str(self.monthly_path),
                'file_naming': 'XXXXXX_XXXX.csv (å¦‚ 000001_XSHE.csv)'
            }
        }
        
        # ä¿å­˜æ€»ç»“
        summary_file = self.base_path / 'weekly_monthly_summary.json'
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸŠ å‘¨çº¿ã€æœˆçº¿æ•°æ®æ•´ç†å®Œæˆ!")
        print(f"=" * 80)
        print(f"ğŸ“Š æ•´ç†ç»Ÿè®¡:")
        print(f"   ğŸ“Š åŸå‘¨çº¿æ–‡ä»¶: {summary['statistics']['weekly_files_found']}")
        print(f"   ğŸ“… åŸæœˆçº¿æ–‡ä»¶: {summary['statistics']['monthly_files_found']}")
        print(f"   ğŸ¯ æ•´ç†è‚¡ç¥¨æ•°: {summary['statistics']['stocks_organized']}")
        print(f"   ğŸ“ˆ å‘¨çº¿è¡¥å…¨: {summary['statistics']['weekly_gaps_filled']}")
        print(f"   ğŸ“… æœˆçº¿è¡¥å…¨: {summary['statistics']['monthly_gaps_filled']}")
        print(f"   â±ï¸ ç”¨æ—¶: {summary['organization_info']['duration_minutes']} åˆ†é’Ÿ")
        print(f"   ğŸ“ å­˜å‚¨ä½ç½®: {self.base_path}")
        print(f"   ğŸ“„ è¯¦ç»†æŠ¥å‘Š: {summary_file}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ“Š å‘¨çº¿ã€æœˆçº¿æ•°æ®æ•´ç†å™¨")
    print("=" * 80)
    print("ğŸ¯ ç›®æ ‡: æ•´ç†ç°æœ‰æ•°æ®å¹¶è¡¥å…¨ç¼ºå£")
    print("ğŸ“… æ—¶é—´: ç»Ÿä¸€åˆ°2025å¹´8æœˆ31æ—¥")
    print("ğŸ“¡ è¡¥å…¨æ•°æ®æº: UQER MktEqudGet API")
    
    organizer = WeeklyMonthlyDataOrganizer()
    organizer.organize_all_data()

if __name__ == "__main__":
    main()